var documenterSearchIndex = {"docs":
[{"location":"publications/#Publications","page":"Related Publications","title":"Publications","text":"","category":"section"},{"location":"publications/","page":"Related Publications","title":"Related Publications","text":"Dispensing with unnecessary assumptions in population genetics analysis. With accompanying code repository tag for PheWAS' and pairwise interactions and this tag for 3 points interactions.","category":"page"},{"location":"parameter_specification/#Describing-the-causal-parameters-of-interest","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"","category":"section"},{"location":"parameter_specification/#Parameter-Files","page":"Describing the causal parameters of interest","title":"Parameter Files","text":"","category":"section"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"In this section, by parameter, we mean a statistical estimand that represents a scientific quantity of interest and will be estimated via TarGene. The complete specification of a parameter requires the description of a causal model which can be represented by the following graph.","category":"page"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"<div style=\"text-align:center\">\n<img src=\"../assets/causal_graph.png\" alt=\"Causal Model\" style=\"width:600px;\"/>\n</div>","category":"page"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"Parameters are specified by a YAML file containing the list of parameters to be estimated. Each parameter is fully determined by four fields:","category":"page"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"type: The parameter type, one of:\nATE: Average Treatment Effect\nIATE: Interaction Average Treatment Effect\nCM: Conditional Mean\ntarget: The trait of interest, as specified in the TRAITS_CONFIG file in the phenotypes/name field. You can also use the wildcard \"*\" to signify that you want to estimate this parameter accross all traits in the dataset.\ntreatment: The treatment variables and associated control/case settings (see example below).\nconfounders: A list of confounding variables. Note that principal components will be added to that list by default and must not be provided here. You can provide an empty list.\ncovariates: This is optional and correspond to a list of additional covariates for the prediction of the trait.","category":"page"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"Here is an example parameter file:","category":"page"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"\nParameters:\n  - type: IATE\n    target: \"*\"\n    treatment: (RSID_10 = (control = \"AA\", case = \"AC\"), Sun_Exposure = (control = 1, case = 0))\n    confounders: [Economic_background]\n    covariates: [Age]\n  - type: ATE\n    target: PHENOTYPE_2\n    treatment: (RSID_10 = (control = \"AA\", case = \"CC\"), RSID_100 = (control = \"GC\", case = \"CC\"))\n    confounders: []\n  - type: CM\n    target: PHENOTYPE_2\n    treatment: (RSID_10 = \"AA\",)\n    confounders: []","category":"page"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"Note that variants must be encoded via an explicit genotype string representation (e.g. \"AC\"), the order of the alleles in the genotype is not important.","category":"page"},{"location":"parameter_specification/#Parameter-Plans","page":"Describing the causal parameters of interest","title":"Parameter Plans","text":"","category":"section"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"At the moment, there are two main ways one can specify parameters that need to be estimated during a targene-pipeline run. This is done via the MODE parameter.","category":"page"},{"location":"parameter_specification/#PARAMETER_PLAN-FROM_PARAM_FILE","page":"Describing the causal parameters of interest","title":"PARAMETER_PLAN = FROM_PARAM_FILE","text":"","category":"section"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"This is the most general setting and should match the needs of any project, however it requires some preliminary work. In this setting, one typically provides a set of parameter files as described above. If you are interested in only a few parameters it may be acceptable to write them by hand. Otherwise it is best to generate them using a programming language (for instance using TMLE.jl). The path to those parameters is then provided with the PARAMETER_FILE nextflow parameter. See the previous section on Parameter Files.","category":"page"},{"location":"parameter_specification/#PARAMETER_PLAN-FROM_ACTORS","page":"Describing the causal parameters of interest","title":"PARAMETER_PLAN = FROM_ACTORS","text":"","category":"section"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"In this setting the goal is to infer the interaction effect between multiple variants and potential external factors, interacting together via a specific biological mechanism. Typically, multiple sets of variants are of interest and each set is identified with a specific molecule, contributing to the mechanism. In particular, it is assumed that a set of variants, usually binding quantitative trait loci (bQTLs) play a pivotal role. All interactions of interest are thus defined with respect to that set of genetic variations. Let's Consider the following scenario: we know that a transcription factor binds to molecules x and y and then differentially binds to specific regions in the genome (bQTLs) to regulate downstream genes. We suspect that an alteration of this mechanism is responsible for some diseases. A set of xQTLs, associated with the expression of x and a set of yQTLs associated with the expression of y have been identified. Together xQTLs and yQTLs variants are termed \"trans actors\". We further suspect that some environmental factors may influence this process. From that scenario, there are many questions that can be asked, for instance : \"What is the interaction effects of a bQTL with an environmental factor?\". This is a simple pairwise interaction setting and more complex scenarios can be envisaged as described in the following graph.","category":"page"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"<div style=\"text-align:center\">\n<img src=\"../assets/from_actors.png\" alt=\"FromActors\" style=\"width:600px;\"/>\n</div>","category":"page"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"Let us now turn to the pipeline specification for this parameter plan:","category":"page"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"BQTLS: A path to a .csv file containing at least an ID column for each rsID and an optional CHR column for the chromosome on which the SNP is located.\nTRANS_ACTORS: A path prefix to a set of .csv files identifying different trans-acting variants. Each file has the same format as for the bQTLs.\nENVIRONMENTALS: A path to a .txt file containing a list of environmental exposures with no header and one exposure per line. Each exposure should be available from the trait dataset.\nEXTRA_COVARIATES: A path to a .txt file containing a list of extra covariate variables with no header and one variable per line. Each variable should be available from the trait dataset.\nEXTRA_CONFOUNDERS: A path to a .txt file containing a list of extra confounding variables with no header and one variable per line. Each variable should be available from the trait dataset.\nORDERS: A comma separated string that specifies the various interaction orders of interest. All combinations satisfying the positivity constraint will be generated. The order 1 corresponds to the Average Treatment Effect (ATE) for bQTLs, any higher order corresponds to the Interaction Average Treatment Effect (IATE) for the various actors. For example, in the previous scenario, assume we provided ORDERS=1,2. This would generate parameter files for the estimation of all:\nATEs parameters for all bQTLs\nIATEs parameters for all (bQTLs, xQTLs), (bQTLs, yQTLs), (bQTLs, Envs) pairs.","category":"page"},{"location":"parameter_specification/#Parallelization","page":"Describing the causal parameters of interest","title":"Parallelization","text":"","category":"section"},{"location":"parameter_specification/","page":"Describing the causal parameters of interest","title":"Describing the causal parameters of interest","text":"Depending on the available resources, you can achieve further speed by batching the parameters to be estimated. The batch size can be controled via BATCH_SIZE(default: 400).","category":"page"},{"location":"confounding_adjustment/#Adjusting-for-confounders","page":"Adjusting for confounders","title":"Adjusting for confounders","text":"","category":"section"},{"location":"confounding_adjustment/","page":"Adjusting for confounders","title":"Adjusting for confounders","text":"To account for potential confounding effect due to population stratification, we extract principal components from the genetic data using flashpca. We follow the recommended procedure for this tool which implies some preprocessing and filtering. The associated arguments are as follows:","category":"page"},{"location":"confounding_adjustment/","page":"Adjusting for confounders","title":"Adjusting for confounders","text":"LD_BLOCKS (required): A path to pre-identified linkage desequlibrium blocks around the variants that will be queried for causal effect estimation. Those LD blocks will be removed from the data.\nFLASHPCA_EXCLUSION_REGIONS (required): A path to the flashpca special exclusion regions which is provided in their repository.\nMAF_THRESHOLD (optional): Only variants with that minor allele frequency are considered\nNB_PCS (optional, default: 6): The number of PCA components to extract.","category":"page"},{"location":"nextflow_params/#Index-of-the-pipeline-parameters","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"Here is a list of all the pipeline parameters:","category":"page"},{"location":"nextflow_params/#[Setting-a-data-source](@ref)","page":"Index of the pipeline parameters","title":"Setting a data source","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"ENCRYPTED_DATASET (required unless a DECRYPTED_DATASET is given): Path to a UK-Biobank encrypted main dataset.\nENCODING_FILE (required unless a DECRYPTED_DATASET is given): If an encrypted dataset is provided, an encoding file must accompany it.\nDECRYPTED_DATASET (optional): Path to a UK-Biobank decrypted main dataset. If set, ENCRYPTED_DATASET is ignored.\nTRAITS_CONFIG (required): Configuration file describing which traits should be extracted from the main dataset.\nWITHDRAWAL_LIST (required): List of participants withdrawn from the study.\nQC_FILE (required): Genotyping quality control file from the UK-Biobank study.\nUKBB_BED_FILES (required): Path expression to PLINK BED files.\nUKBB_BGEN_FILES (required): Path expression to iputed BGEN files.","category":"page"},{"location":"nextflow_params/#[Adjusting-for-confounders](@ref)","page":"Index of the pipeline parameters","title":"Adjusting for confounders","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"LD_BLOCKS (required): A path to pre-identified linkage desequlibrium blocks around the variants that will be queried for causal effect estimation. Those will be removed from the data.\nFLASHPCA_EXCLUSION_REGIONS (required): A path to the flashpca special exclusion regions which is provided in their repository.\nMAF_THRESHOLD (optional): Only variants with that minor allele frequency are considered\nNB_PCS (optional, default: 6): The number of PCA components to extract.","category":"page"},{"location":"nextflow_params/#[Describing-the-causal-parameters-of-interest](@ref)","page":"Index of the pipeline parameters","title":"Describing the causal parameters of interest","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"PARAMETER_PLAN (required, default: \"FROM_PARAM_FILE\"): One of \"FROM_PARAM_FILE\", \"FROM_ACTORS\".","category":"page"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"If PARAMETER_PLAN=\"FROM_PARAM_FILE\":","category":"page"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"PARAMETER_FILE (required): Path expression to the parameter files.","category":"page"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"If PARAMETER_PLAN=\"FROM_ACTORS\":","category":"page"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"BQTLS (required): A CSV file containing binding quantitative trait loci.\nTRANS_ACTORS (required): A prefix to CSV files containing quantitative trait loci potentially interacting with the previous bqtls.\nEXTRA_CONFOUNDERS (optional, default: nothing): Path to additional confounders file, one per line, no header.\nEXTRA_COVARIATES (optional, default: nothing): Path to additional covariates file, one per line, no header.\nENVIRONMENTALS (optional, default: nothing): Path to additional environmental treatments file, one per line, no header.\nORDERS (optional, default: \"1,2\"): Comma separated list describing the order of desired interaction parameters, 1 for the ATE (no interaction), 2 for pairwise interactions etc... e.g. \"1,2\"","category":"page"},{"location":"nextflow_params/#[Specifying-a-Targeted-Estimator](@ref)","page":"Index of the pipeline parameters","title":"Specifying a Targeted Estimator","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"ESTIMATORFILE (required): YAML configuration file describing the nuisance parameters learners.\nPOSITIVITY_CONSTRAINT (optional, default: 0.01): Treatment variables rarest configuration should have at least that frequency.\nSAVE_IC (optional, default: true): For all parameters with an p-value below PVAL_THRESHOLD, the influence curve is saved. Make sure to keep to true if you want to use sieve variance correction, i.e. if NB_VAR_ESTIMATORS != 0.","category":"page"},{"location":"nextflow_params/#[Correcting-for-population-relatedness](@ref)","page":"Index of the pipeline parameters","title":"Correcting for population relatedness","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"GRM_NSPLITS (optional, default: 100): To fasten GRM computation, it is typically split in batches.\nNB_VAR_ESTIMATORS (optional, default: 0): Number of sieve variance estimates per curve. Setting this value to 0 results in skipping sieve variance correction.\nMAX_TAU (optional, default: 0.9): Variance estimates are computed for tau ranging from 0 to MAX_TAU\nPVAL_THRESHOLD (optional, default: 0.05): To save computation time and disk, only parameters with a p-value below this threshold are considered for sieve variance correction.","category":"page"},{"location":"nextflow_params/#[Tweaking-additional-behaviour](@ref)","page":"Index of the pipeline parameters","title":"Tweaking additional behaviour","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"CALL_THRESHOLD (optional, default: 0.9): For putative causal variants (listed in the parameter files described in the Describing the causal parameters of interest section). If a individual's allele's probability is greater than the threshold, then it is called, otherwise it is considered missing.\nBATCH_SIZE (optional, default: 400): The set of parameters to be estimated is batched and the TMLE processes will run in parallel across batches on your platform.\nOUTDIR (optional, default: \"results\"): Output directory\nRNG (optional, default: 123): General random seed used where appropriate.","category":"page"},{"location":"nextflow_params/#[Running-negative-control-checks](@ref)","page":"Index of the pipeline parameters","title":"Running negative control checks","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"MAX_PERMUTATION_TESTS (optional, default: null): Arbitrarily limits the number of permutation tests performed.\nPVAL_COL (optional, default: TMLE_PVALUE): In the output summary.csv, the p-value column used to define significant hits.\nPERMUTATION_ORDERS (optional, default: \"1\"): A comma separating string defining the permutation test orders to be performed. (see Permutation tests)\nMAF_MATCHING_RELTOL(optional, default:0.05): Random variants are chosen with a similar MAF matched with the given relative tolerance.\nN_RANDOM_VARIANTS(optional, default: 10): For each hit, that many variants are randomly picked.","category":"page"},{"location":"contribution_guide/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"Contributions, whether bug fixes, new features or documentation improvements are very welcome!","category":"page"},{"location":"contribution_guide/#Raise-an-issue","page":"Contributing","title":"Raise an issue","text":"","category":"section"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"In order to discuss and track the evolution of the project, please first raise an issue on the targene-pipeline repository. If a change is agreed upon, the discussion should identify the relevant repositories that are concerned by the change and open an issue on each of the repository. For instance, if one wishes to improve the extraction of traits from the UK-Biobank, the UKBMain.jl would surely be impacted and a new release for that package necessary.","category":"page"},{"location":"contribution_guide/#Workflow","page":"Contributing","title":"Workflow","text":"","category":"section"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"Following our previous UKBMain.jl example, there are two repositories tha need to be updated, the current workflow is as follows:","category":"page"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"Develop\nUKBMain.jl\nCreate a new git branch for your change\nDevelop and test\nRelease an image for your branch by selecting it after clicking the Run workflow button. If the tests pass, a new docker image will be generated and hosted on Docker hub with your branch's name (see Note on Docker images).\ntargene-pipeline\nCreate a new git branch for your change\nFor each Nextflow process using the UKBMain.jl's docker image, update to the branch's image name.\nDevelop further required changes and run/add the tests (see Note on the pipeline's tests).\nReview: When everything is working, ask for a review\nRelease UKBMain.jl:\nMerge your branch into main\nCreate a new Github release following semantic versioning, this will create a new docker image with your release name.\nRelease TarGene\nFor each Nextflow process using the UKBMain.jl's docker image, update to the released image name (as before).\nCreate a new Github release following semantic versioning","category":"page"},{"location":"contribution_guide/#Note-on-Docker-images","page":"Contributing","title":"Note on Docker images","text":"","category":"section"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"Currently, all TarGene building blocks (executables) are provided as docker images. The following table provides a map linking each TarGene repository to the associated Docker image tags.","category":"page"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"Repository Docker tag\nTargeneCore.jl tl-core\nUKBMain.jl ukbmain\nTargetedEstimation.jl targeted-estimation\nNegativeControl.jl negative-controls","category":"page"},{"location":"contribution_guide/#Note-on-the-pipeline's-tests","page":"Contributing","title":"Note on the pipeline's tests","text":"","category":"section"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"The pipeline is automatically tested for every push/pull-request made to the github repository. The tests will require that Julia and the container engine of your choice be installed (see below). We currently have a suite of two tests corresponding to the two main usages of the pipeline. They can be run locally as follows:","category":"page"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"julia --project=test --startup-file=no test/from_param_files.jl -profile PROFILE -resume","category":"page"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"and","category":"page"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"julia --project=test --startup-file=no test/from_actors.jl -profile PROFILE -resume","category":"page"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"where PROFILE is one of: 'local' (local with singularity engine), 'ci' (local with singularity engine), 'eddie' (SGE with singularity engine.).","category":"page"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"The tests can also be run interactively via the Julia REPL.","category":"page"},{"location":"sieve_variance/#Correcting-for-population-relatedness","page":"Correcting for population relatedness","title":"Correcting for population relatedness","text":"","category":"section"},{"location":"sieve_variance/","page":"Correcting for population relatedness","title":"Correcting for population relatedness","text":"If the i.i.d. (independent and identically distributed) hypothesis is not satisfied, most of the traditional statistical inference theory falls apart. This is typically possible in population genetics where a study may contain related individuals. Here we leverage a non-parametric method called Sieve Variance Plateau (SVP) estimation. The hypothesis is that the dependence between individuals is sufficiently small, so that our targeted estimator will still be asymptotically unbiased, but its variance will be under estimated. In brief, the SVP estimator computes a variance estimate for a range of thresholds ùúè, by considering individuals to be genetically independent if their genetic distance exceeds ùúè. The genetic distance between a pair of individuals (ùëñ, ùëó) equals 1 ‚àí GRMùëñ,ùëó , i.e., one minus their genetic relatedness value. As the distance threshold ùúè increases, fewer individuals are assumed to be genetically independent. For instance, the estimate corresponding to a distance of ùúè = 0 corresponds to the i.i.d. hypothesis, while a distance of ùúè = 1 incorporates pairs of individuals who are not genetically correlated. TarGene varies the threshold ùúè from 0 to 1 and fits a curve to the corresponding variance estimates. The maximum of this curve is the most conservative estimate of the variance of the target parameter estimator and constitutes our corrected variance estimator.","category":"page"},{"location":"sieve_variance/","page":"Correcting for population relatedness","title":"Correcting for population relatedness","text":"The following arguments can be changed to control the behaviour of the pipeline:","category":"page"},{"location":"sieve_variance/","page":"Correcting for population relatedness","title":"Correcting for population relatedness","text":"GRM_NSPLITS (default: 100): This is a purely computational argument. The GRM is typically very large and splitting enables a good memory/parallelization tradeoff.\nMAX_TAU (default: 0.8): Controls the maximum genetic distance considered.\nNB_VAR_ESTIMATORS (default: 0): Controls the number of points in the interval [0, MAX_TAU]. If 0, the Sieve Variance Plateau method will not be applied.\nPVAL_THRESHOLD (default: 0.05): Only estimates with a p-value lower than PVAL_THRESHOLD will be considered for SVP correction. This is because SVP will only increase the variance of the estimator.","category":"page"},{"location":"tmle/#Specifying-a-Targeted-Estimator","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"","category":"section"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"TMLE is an adaptive procedure that depends on the specification of learning algorithms for the estimation of the nuisance parameters (see TMLE.jl for a description of the assumed setting). In our case, there are two nuisance parameters for which we need to specify learning algorithms:","category":"page"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"E[Y|T, W, C]: The mean outcome given the treatment, confounders and extra covariates. It is commonly denoted by Q in the Targeted Learning litterature.\np(T|W): The propensity score. It is commonly denoted by G in the Targeted Learning litterature.","category":"page"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"The estimator configuration file describes the TMLE specification for the estimation of the parameters defined in the previous section. In order to provide maximum flexibility, this is provided as a plain Julia file via the ESTIMATORFILE parameter.","category":"page"},{"location":"tmle/#Description-of-the-file","page":"Specifying a Targeted Estimator","title":"Description of the file","text":"","category":"section"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"In order to provide maximum flexibility as to the choice of learning algorithms, the estimator file is a plain Julia file. This file is optional and omitting it defaults to using generalized linear models. If provided, it must define a NamedTuple called tmle_spec containing any of the following fields as follows (default configuration):","category":"page"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"\ntmle_spec = (\n  Q_continuous = LinearRegressor(),\n  Q_binary     = LogisticClassifier(lambda=0.),\n  G            = LogisticClassifier(lambda=0.),\n  threshold    = 1e-8,\n  cache        = false,\n  weighted_fluctuation = false\n)","category":"page"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"where:","category":"page"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"Q_continuous: is a MLJ model used for the estimation of E[Y|T, W, C] when the outcome Y is continuous.\nQ_binary: is a MLJ model used for the estimation of E[Y|T, W, C] when the outcome Y is binary.\nG: is a MLJ model used for the estimation of p(T|W).\nthreshold: is the minimum value the propensity score G is allowed to take.\ncache: controls caching of data by MLJ machines. Setting it to true may result in faster runtime but higher memory usage.\nweighted_fluctuation: controls whether the fluctuation for Q is a weighted glm or not. If some of the treatment values are rare it may lead to more robust estimation.","category":"page"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"Typically, Q_continuous, Q_binary and G will be adjusted and other fields can be left unspecified.","category":"page"},{"location":"tmle/#Ready-to-use-estimator-files","page":"Specifying a Targeted Estimator","title":"Ready to use estimator files","text":"","category":"section"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"We recognize not everyone will be familiar with Julia. We thus provide a set of ready to use estimator files that can be simplified or extended as needed:","category":"page"},{"location":"tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"Super Learning: with and without interaction terms in the GLM models for Q.\nSuper Learning for G and GLMNet for Q: here.\nSuper Learning for G and GLM for Q: here.\nGLMNet: with and without interaction terms in the GLM models for Q.\nGLM: with and without interaction terms in the GLM models for Q.\nXGBoost: with tuning.","category":"page"},{"location":"runtime_considerations/#Some-Runtime-considerations","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Runtime is discussed in detail here.","category":"page"},{"location":"associated_softwares/#Associated-Softwares","page":"Associated Softwares","title":"Associated Softwares","text":"","category":"section"},{"location":"associated_softwares/","page":"Associated Softwares","title":"Associated Softwares","text":"You may either not be interested in population genetics at all, or not willing to run the full pipeline on your project. If you are still eager to leverage the Targeted Learning framework, you can either rely on:","category":"page"},{"location":"associated_softwares/","page":"Associated Softwares","title":"Associated Softwares","text":"TMLE.jl: A Julia package for Targeted Maximum Likelihood Estimation (TMLE).\nTargetedEstimation.jl: A command line interface to run TMLE on your data.","category":"page"},{"location":"miscellaneous/#Tweaking-additional-behaviour","page":"Tweaking additional behaviour","title":"Tweaking additional behaviour","text":"","category":"section"},{"location":"miscellaneous/","page":"Tweaking additional behaviour","title":"Tweaking additional behaviour","text":"Further Nextflow parameter affecting the behaviour of the pipeline but that does not fit in any previously described category is listed here:","category":"page"},{"location":"miscellaneous/","page":"Tweaking additional behaviour","title":"Tweaking additional behaviour","text":"CALL_THRESHOLD (optional, default: 0.9): For putative causal variants (listed in the parameter files described in the Describing the causal parameters of interest section). If an individual's allele's probability is greater than the threshold, then it is called, otherwise it is considered missing.\nBATCH_SIZE (optional, default: 400): The set of parameters to be estimated is batched and the TMLE processes will run in parallel across batches on your platform.\nSAVE_IC (optional, default: true): For all parameters with an p-value below PVAL_THRESHOLD, the influence curve is saved. Make sure to keep to true if you want to use sieve variance correction, i.e. if NB_VAR_ESTIMATORS != 0.\nOUTDIR (optional, default: \"results\"): Output directory","category":"page"},{"location":"data_sources/#Setting-a-data-source","page":"Setting a data source","title":"Setting a data source","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"Currently, only the UK-Biobank is supported, work is under way to enable custom data sources!","category":"page"},{"location":"data_sources/#UK-Biobank","page":"Setting a data source","title":"UK-Biobank","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"The UK-Biobank is composed of both genetic data (.bed and .bgen files) and trait data. While there is a plan to use only bgen and trait data, the pipeline currently uses all sources of information. Below we explain how to specify those arguments to TarGene. For more information on the structure of the UK-Biobank data, please refer to their User Guide.","category":"page"},{"location":"data_sources/#Main-Dataset","page":"Setting a data source","title":"Main Dataset","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"The trait dataset is often called the \"main dataset\" and consists of an encrypted file containing individuals' trait information accessible via your project. The first option is thus to provide this dataset using the ENCRYPTED_DATASET parameter. Since the data is encrypted, the pipeline will also need the encoding file that you can provide with ENCODING_FILE.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"A \"main dataset\" is typically very large and only a few traits will be of interest to a given study. To extract those relevant traits from the dataset, a TRAITS_CONFIG YAML file must be provided. Since, writing by hand such a file for large scale study can quickly become tenuous, we provide a configuration file corresponding to the GeneAtlas study here.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"The structure of a file is composed of extraction rules that convert UK-Biobank fields to traits of interest. An example is presented below:","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"traits:\n    - fields:\n        - \"41202\"\n        - \"41204\"\n      phenotypes:\n        - name: \"Chronic lower respiratory diseases\"\n          codings:\n            - \"J40\"\n            - \"J41\"\n            - \"J410\"\n            - \"J411\"\n            - \"J418\"\n            - \"J42\"\n            - \"J43\"\n        - name: \"Other extrapyramidal and movement disorders\"\n          codings:\n            - \"G254\"\n            - \"G255\"\n            - \"G256\"\n            - \"G258\"\n    - fields:\n        - \"40006\"\n      phenotypes:\n        - name: \"Malignant melanoma of skin\"\n          codings:\n            - \"C430\"\n            - \"C431\"\n            - \"C432\"\n            - \"C433\"\n            - \"C434\"\n            - \"C435\"\n            - \"C436\"\n            - \"C437\"\n            - \"C438\"\n            - \"C439\"\n    - fields:\n        - \"1558\"\n      phenotypes:\n        - name: \"Alcohol intake frequency.\"\n    - fields:\n        - \"20117\"\n      phenotypes:\n        - name: \"Alcohol drinker status\"\n\nsubset: \n    - fields: 22001\n      phenotypes:\n        - name: Female\n          codings: 0\n    - fields: 21000\n      phenotypes:\n        - name: White\n          codings: [1001]","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"A file consists of two sections, a traits section and a subset section. Each section is further divided in a list of fields items. Each fields item is itself decomposed in multiple phenotypes items that are identified by a name and an optional list of codings. Alltogether, a phenotype element defines an extraction rule from a list of fields. The previous configuration file would thus restrict the analysis to white females and extract 5 traits.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"We now describe the currently available extraction rules based on variable types. Not that a variable type is determined by the metadata provided by the UK-Biobank and not a programmatic rule.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"Continuous/Count variables: Those consist in a single field and the extraction rule will take the first visit assessment value for that field. For instance, the \"Alcohol intake frequency.\" is simply the value from the \"1558-0.0\" column.\nOrdinal data: Some variables are encoded as categorical by the UK-Biobank but an ordinal interpretation seems more appropriate. A list of such variables has been identified and hard-coded. They will be treated like continuous variables.\nCategorical variables: For those variables again, the first visit assessment value for the specified field is used. An additional codings field can be provided. In that case the variable is turned into a indicator variable indicating whether an individual is included in the criterion given by the codings list. In the previous subset section, \"White\" is such a transformation into a binary variable. Note that we could add more elements to the list to subset on a wider population.\nCategorical Arrayed variables: Some fields in the UK-Biobank consist in list of traits for individuals. This is the case for at least the fields: 41202, 41204, 20002 and 40006. For those fields it is essential to define a codings section that describes a trait. For instance \"Malignant melanoma of skin\" corresponds to the declaration of any of the codings for an individual. Moreover, some fields share the same encoding, this is the case for  41202 and 41204. In that situation it may be useful to aggregate those sources of information. For that purpose multiple fields can be provided to the fields section. In that scenario the declaration of any of the coding in a codings section for any of the fields will define the trait.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"To come back to the pipeline specification, if a \"typical\" main dataset has already been decrypted outside of the pipeline, one may use the DECRYPTED_DATASET as an input to the pipeline instead of the ENCRYPTED_DATASET and ENCODING_FILE arguments. However, please make sure that all the fields in the TRAITS_CONFIG file described above are contained in this dataset.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"Note: Since this decrypted dataset is a plain CSV file, one may build and add extra columns to it. Any column in the decrypted dataset which does not correspond to a UK-Biobank field will be considered as such.","category":"page"},{"location":"data_sources/#Genetic-Data","page":"Setting a data source","title":"Genetic Data","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"We are currently using both .bgen and .bed files, furthermore, we assume that the data is unphased. Those are respectively provided with the UKBB_BGEN_FILES and UKBB_BED_FILES parameters. Since the UK-Biobank genotyping data is split in chromosomes, it should be of the form PREFIX_TO_CHROMOSOMES{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bgen,sample,bgen.bgi} and PREFIX_TO_CHROMOSOMES{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bed,bim,fam} respectively.","category":"page"},{"location":"data_sources/#Additional-Files","page":"Setting a data source","title":"Additional Files","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"Additional UK-Biobank required files for preprocessing and filtering are:","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"QC_FILE: A path to the UK-Biobank SNP quaility control ukb_snp_qc.txt file.\nWITHDRAWAL_LIST: A path to the withdrawal sample list to exclude removed participants from the study.","category":"page"},{"location":"negative_control/#Running-negative-control-checks","page":"Running negative control checks","title":"Running negative control checks","text":"","category":"section"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"After running TarGene, you may want to perform some quality control checks on your positive hits. Since true positives are not ubiquitous in biology, this is done by negative control checks. The corresponding pipeline can be run by:","category":"page"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"nextflow run https://github.com/TARGENE/targene-pipeline -entry negativeControl -r vX -profile P -resume -with-trace -with-report","category":"page"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"see the Overview section for more info on the command line options.","category":"page"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"At the moment, there are two types of analyses you can perform with this sub-pipeline: permutation tests and comparisons with \"non-functional\" random variants drawn from the genome. Furthermore, this is restricted to interaction parameters (IATE). In both cases, you will need to define positive hits. This is traditionally done via p-value filtering, you will thus need to specify both a pvalue column (from the \"summary.csv\" file) and a threshold: PVAL_COL and PVAL_THRESHOLD parameters.","category":"page"},{"location":"negative_control/#Permutation-tests","page":"Running negative control checks","title":"Permutation tests","text":"","category":"section"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"For each IATE hit, we can perturb the data by independently shuffling one or more columns in the data. By doing so, the interaction will be broken and become \"non-significant\". Because there are up to K variables in a IATE parameter, we can also permute up to K+1 variables. For instance, assuming we found a significant interaction between rs1234 and rs5678 on diabetes, we could permute:","category":"page"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"Any one of (rs1234, rs5678, diabetes) leading to 3 order 1 permutation tests.\nAny two of (rs1234, rs5678, diabetes) resulting also in 3 order 2 permutation tests.\nAll three of (rs1234, rs5678, diabetes) leading to 1 order 3 permutation test.","category":"page"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"Specifying those orders is made via the PERMUTATION_ORDERS variable which is a comma separated string of orders (e.g. \"1,2\").","category":"page"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"The results of those permutation tests will be found in the $(OUTDIR)/permutation_summary.csv file.","category":"page"},{"location":"negative_control/#Non-functional-randomly-drawn-variants","page":"Running negative control checks","title":"Non-functional randomly drawn variants","text":"","category":"section"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"If you are using the PARAMETER_PLAN = FROM_ACTORS, it is likely that you have not chosen the genetic variants at random. Mainly trans-acting variants have been defined because of a likely role in a biological mechanism. If this is the case, replacing a trans-actor by a random variant anywhere on the genome is likely to break the interaction. While unlikely, by chance alone (or lack thereof), one could pick a random trans-actor which is also interacting with any of the bQTLs. In order to account for that it is recommended to instead select a certain number of rancom variants, denoted by N_RANDOM_VARIANTS (default: 10). Furthermore, we enforce two criteria on each of the randomly chosen random variants:","category":"page"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"Its minor allele frequency should match that of the original trans-actor up to MAF_MATCHING_RELTOL (relative tolerance, default: 0.05).\nIt shouldn't lie in a known regulatory region.","category":"page"},{"location":"negative_control/","page":"Running negative control checks","title":"Running negative control checks","text":"The result of this part of the pipeline is a parameter file located at $(OUTDIR)/random_variants_parameters.yaml directory. To perform the actual tests, you will have to run TarGene again on that parameter file with the  PARAMETER_PLAN = FROM_PARAM_FILE mode.","category":"page"},{"location":"overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"overview/#Running-the-pipeline","page":"Overview","title":"Running the pipeline","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Since TarGene is a Nextflow pipeline, it can be run with a simple command line similar to the following:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"nextflow run https://github.com/TARGENE/targene-pipeline -r vX -profile P -resume -with-trace -with-report","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"All arguments are optional but encouraged. Here -r vX describes the version to be used and should be provided for reproducibility purposes, e.g. -r v0.3.7. If left unspecified, the latest development version will be used. The -resume option tells Nextflow to try to resume the pipeline if an error occured during the execution (if you misspecified a parameter for instance) and the -with-trace and -with-report generate additional report files. The -profile P option is described below and implicit, is the existence of a nextflow.config file, the content of which is also described here:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"It is likely that you will run TarGene on a HPC platform, in particular the Executors and Singularity configurations are required. Since Nextflow is so widespread, it is probable that such a configuration file is already available from your HPC administrators. Since this configuration only describes de computing platform and not your project, it is often described as a Profile. If your HPC uses the SGE executor, the -profile eddie may work with no, or minor adjustment (it can also serve as a template for other executors see file).\nYou need to provide the configuration details associated with your project, this is usually done in a nextflow.config file living at the root of your project's directory. The configuration parameters are described in the following sections:\nSetting a data source\nAdjusting for confounders\nDescribing the causal parameters of interest\nSpecifying a Targeted Estimator\nCorrecting for population relatedness\nTweaking additional behaviour\nRunning negative control checks","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"A list of all TarGene's parameters is available in the Index of the pipeline parameters.","category":"page"},{"location":"overview/#Outputs","page":"Overview","title":"Outputs","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"All outputs are generated in the $(OUTDIR) (default: results) directory. Here we succintly describe the most important ones:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"summary.csv: is the main output of the pipeline, it contains all summary statistics and information for each estimand of interest. Those are further described here.\nhdf5files/inf_curves: contains influence curves in HDF5 format if those were requested (see the SAVE_IC Nextflow parameter).\ntmle_inputs/final.data.csv: contains the input dataset to all TMLE processes.\nOther sub-directories contain intermediate results that may still be of interest for debugging purposes.","category":"page"},{"location":"#TarGene","page":"Home","title":"TarGene","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to TarGene, the software that brings Targeted Learning to population genetic studies! TarGene enables the estimation of various effect sizes including the Average Treatment Effect and the Interaction Average Treatment Effect (epistasis) up to any order. Because we follow the Targeted Learning framework, the final estimates provided by TarGene are covered by mathematical guarantees. The software is delivered as a Nexflow pipeline to bring scalability and reproducibility to your research.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Why using TarGene:","category":"page"},{"location":"","page":"Home","title":"Home","text":"To cover effect sizes with asymptotic mathematical guarantees.\nTo dispense with unrealistic and unnecessary parametric assumptions.\nTo embed a causal interpretation into reported estimates.\nTo perform any higher-order interaction (>=2) analysis, including GxG and GxE.","category":"page"},{"location":"#Overview-of-the-pipeline","page":"Home","title":"Overview of the pipeline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The pipeline can rougly be decoupled into three steps. The first, aims at pre-processing the data sources to convert them in a table data format that can be wielded by Targeted Maximum Likelihood Estimation (TMLE). The second is the TMLE itself. The third and final step is the Sieve Variance Plateau correction which revises the variance estimate to account for the fact that individuals in the population are not necessarily independent. The following diagram provides a high level interface of the organisation of the pipeline.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div style=\"text-align:center\">\n<img src=\"assets/targene_diagram.png\" alt=\"Targene Pipeline\" style=\"width:800px;\"/>\n</div>","category":"page"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The pipeline has been tested with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Nextflow: 22.04\nSingularity: 3.8","category":"page"},{"location":"#Quick-start-for-Eddie-users","page":"Home","title":"Quick start for Eddie users","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are a University of Edinburgh researcher and have access to the Eddie cluster you may want to use the Eddie-Template to quickly setup your project. ","category":"page"},{"location":"#Getting-in-touch","page":"Home","title":"Getting in touch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please feel free to raise an issue if you face a problem or would benefit from a new feature. Contributions are most welcome.","category":"page"},{"location":"project_organization/#Project-Organization","page":"Project Organization","title":"Project Organization","text":"","category":"section"},{"location":"project_organization/","page":"Project Organization","title":"Project Organization","text":"The TarGene project is organized around the targene-pipeline repository which contains the Nextflow pipeline which will be the entry-point for most users. However, this repository does not contain the executables that are used by the Nextflow processes. Those executables originate from complementary repositories:","category":"page"},{"location":"project_organization/","page":"Project Organization","title":"Project Organization","text":"TargetedEstimation.jl\nTargeneCore.jl\nUKBMain.jl\nNegativeControl.jl","category":"page"},{"location":"project_organization/","page":"Project Organization","title":"Project Organization","text":"The following diagram presents a high level perspective of the project's organization and dependence structure.","category":"page"},{"location":"project_organization/","page":"Project Organization","title":"Project Organization","text":"<div style=\"text-align:center\">\n<img src=\"../assets/targene_organization.png\" alt=\"TarGene Organization\" style=\"width:800px;\"/>\n</div>","category":"page"}]
}
