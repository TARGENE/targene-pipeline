var documenterSearchIndex = {"docs":
[{"location":"publications/#Publications","page":"Related Publications","title":"Publications","text":"","category":"section"},{"location":"publications/","page":"Related Publications","title":"Related Publications","text":"Dispensing with unnecessary assumptions in population genetics analysis. With accompanying code repository tag for PheWAS' and pairwise interactions and this tag for 3 points interactions.","category":"page"},{"location":"parameter_specification/#Parameter-plans","page":"Parameter plans","title":"Parameter plans","text":"","category":"section"},{"location":"parameter_specification/#Parameter-Files","page":"Parameter plans","title":"Parameter Files","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"In this section, by parameter, we mean the statistical parameter that represents the scientific quantity of interest and will be estimated via TarGene. The complete specification of a parameter requires the description of a causal model which can be represented by the following graph.","category":"page"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"<div style=\"text-align:center\">\n<img src=\"assets/causal_graph.png\" alt=\"Causal Model\" style=\"width:800px;\"/>\n</div>","category":"page"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"Parameters are specified by a YAML file with one section for each variable in the causal model and a section for the parameters that need to be estimated from this causal model. Here is a succinct description for each section and the behaviour of the pipeline for each variable:","category":"page"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"Treatments: The treatment variables, typically one or multiple SNPs and potential environmental exposures.\nConfounders: Confounding variables, typically the principal components which are computed by the pipeline. This section can (must) be ommited.\nCovariates: Additional covariates for the prediction of the traits. Not yet working and must be omitted.\nTargets: The traits of interest. The algorithm will loop through all traits in this section for the given treatments, confounders and covariates. This enables the reuse of the propensity score estimation. If this section is omitted (usually the case) all traits will be used.\nParameters: For each target in Targets multiple parameters may be of interest depending on the exact case/control scenario of the treatment variables. For instance, since each genotyped locus can take up to 3 different values there could be up to 3 Average treatment Effect parameters if the treatment section consists of only one SNP.","category":"page"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"Since an example may be worth a thousand words, here are a couple of such files.","category":"page"},{"location":"parameter_specification/#Average-Treatment-Effect","page":"Parameter plans","title":"Average Treatment Effect","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"One SNP RSID_10 and 2 ATEs for all traits under study.","category":"page"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"Treatments:\n  - RSID_10\nParameters:\n  - name: GG_TO_AG\n    RSID_10:\n      control: GG\n      case: AG\n  - name: GG_TO_AA\n    RSID_10:\n      control: GG\n      case: AA","category":"page"},{"location":"parameter_specification/#Interaction-Average-Treatment-Effect","page":"Parameter plans","title":"Interaction Average Treatment Effect","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"Two interacting SNPs RSID_10 and RSID_100 with only one target PHENOTYPE_1 and only one parameter.","category":"page"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"Treatments:\n  - RSID_10\n  - RSID_100\n\nTargets:\n  - PHENOTYPE_1\n\nParameters:\n  - name: IATE\n    RSID_10:\n      control: GG\n      case: AG\n    RSID_100:\n      control: GG\n      case: AG","category":"page"},{"location":"parameter_specification/#Parameter-Plans","page":"Parameter plans","title":"Parameter Plans","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"There are two main ways one can specify parameters that need to be estimated during a targene-pipeline run. This is done via the MODE parameter.","category":"page"},{"location":"parameter_specification/#PARAMETER_PLAN-FROM_PARAM_FILES","page":"Parameter plans","title":"PARAMETER_PLAN = FROM_PARAM_FILES","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"This is the most general setting and should match the needs of any project, however it requires some preliminary work. In this setting, one typically provides a set of parameter files as described above. The path to those parameters is then provided with the PARAMETER_FILES nextflow parameter. It is not necessary to declare the principal components (from PCA) in the W section of those parameter files, they will dynamically be added. The same is true for the target section (Y), all variables from the traits dataset that are not part of other sections will be added to the target section. If the Y is provided for some parameter file, only those targets will be considered for this specific parameter file.","category":"page"},{"location":"parameter_specification/#PARAMETER_PLAN-FROM_ACTORS","page":"Parameter plans","title":"PARAMETER_PLAN = FROM_ACTORS","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"In this setting the goal is to infer the interaction effect between multiple variants and potential external factors, interacting together via a specific biological mechanism. Typically, multiple sets of variants are of interest and each set is identified with a specific molecule, contributing to the mechanism. In particular, it is assumed that a set of variants, usually binding quantitative trait loci (bQTLs) play a pivotal role because they can be precisely located in the genome and don't suffer from linkage disequilibrium. All interactions of interest are thus defined with respect to that set of genetic variations. Let's Consider the following scenario: we know that a transcription factor binds to molecules x and y and then differentially binds to specific regions in the genome (bQTLs) to regulate downstream genes. We suspect that an alteration of this mechanism is responsible for some diseases. A set of xQTLs, associated with the expression of x and a set of yQTLs associated with the expression of y have been identified. Together xQTLs and yQTLs variants are termed \"trans actors\". We further suspect that some environmental factors may influence this process. From that scenario, there are many questions that can be asked, for instance : \"What is the interaction effects of a bQTL with an environmental factor?\". This is a simple pairwise interaction setting and more complex scenarios can be envisaged as described in the following graph.","category":"page"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"<div style=\"text-align:center\">\n<img src=\"assets/from_actors.png\" alt=\"FromActors\" style=\"width:800px;\"/>\n</div>","category":"page"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"Let us now turn to the pipeline specification for this parameter plan:","category":"page"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"BQTLS: A path to a .csv file containing at least an ID column for each rsID and an optional CHR column for the chromosome on which the SNP is located.\nTRANS_ACTORS: A path prefix to a set of .csv files identifying different trans-acting variants. Each file has the same format as for the bQTLs.\nENVIRONMENTALS: A path to a .txt file containing a list of environmental exposures with no header and one exposure per line. Each exposure should be available from the trait dataset.\nORDERS: A comma separated string that specifies the various interaction orders of interest. All combinations satisfying the positivity constraint will be generated. The order 1 corresponds to the Average Treatment Effect (ATE) for bQTLs, any higher order corresponds to the Interaction Average Treatment Effect (IATE) for the various actors. For example, in the previous scenario, assume we provided ORDERS=1,2. This would generate parameter files for the estimation of all:\nATEs parameters for all bQTLs\nIATEs parameters for all (bQTLs, xQTLs), (bQTLs, yQTLs), (bQTLs, Envs) pairs.","category":"page"},{"location":"parameter_specification/#Parallelization","page":"Parameter plans","title":"Parallelization","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter plans","title":"Parameter plans","text":"Since the same estimator for p(T|W) can be used for multiple target parameters, it may be useful to batch phenotypes using PHENOTYPES_BATCH_SIZE(default: 1) in order to reduce the computational burden.","category":"page"},{"location":"confounding_adjustment/#Confounding-Adjustment","page":"Confounding Adjustment","title":"Confounding Adjustment","text":"","category":"section"},{"location":"confounding_adjustment/","page":"Confounding Adjustment","title":"Confounding Adjustment","text":"To account for potential confounding effect due to population stratification, we extract principal components from the genetic data using flashpca. We follow the recommended procedure for this tool which implies some preprocessing and filtering. The following arguments are compulsory:","category":"page"},{"location":"confounding_adjustment/","page":"Confounding Adjustment","title":"Confounding Adjustment","text":"LD_BLOCKS: A path to pre-identified linkage desequlibrium blocks around the variants that will be queried for causal effect estimation. Those will be removed from the data.\nFLASHPCA_EXCLUSION_REGIONS: A path to the flashpca special exclusion regions which is provided in their repository.\nNB_PCS (default: 6): The number of PCA components to extract.","category":"page"},{"location":"nextflow_params/#Index-of-the-pipeline-parameters","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"Here is a list of all the pipeline parameters:","category":"page"},{"location":"nextflow_params/#UK-Biobank-Data","page":"Index of the pipeline parameters","title":"UK-Biobank Data","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"ENCRYPTED_DATASET: Path to a UK-Biobank encrypted main dataset.\nENCODING_FILE: If an encrypted dataset is provided, an encoding file must accompany it.\nDECRYPTED_DATASET: Path to a UK-Biobank decrypted main dataset. If set, ENCRYPTED_DATASET is ignored.\nTRAITS_CONFIG: Configuration file describing which traits should be extracted from the main dataset.\nWITHDRAWAL_LIST: List of participants withdrawn from the study.\nQC_FILE: Genotyping quality control file from the UK-Biobank study.\nUKBB_BED_FILES: Path expression to PLINK BED files.\nUKBB_BGEN_FILES: Path expression to iputed BGEN files.","category":"page"},{"location":"nextflow_params/#Statistical-Parameters","page":"Index of the pipeline parameters","title":"Statistical Parameters","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"PARAMETER_PLAN: One of \"FROMPARAMFILES\", \"FROM_ACTORS\". See the section.","category":"page"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If PARAMETER_PLAN=\"FROMPARAMFILES\":","category":"page"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"PARAMETER_FILES: Path expression to the parameter files.","category":"page"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If PARAMETER_PLAN=\"FROM_ACTORS\":","category":"page"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"BQTLS: CSV file containing binding quantitative trait loci.\nTRANS_ACTORS: CSV file containing quantitative trait loci potentially interacting with the previous bqtls.\nEXTRA_CONFOUNDERS: Path to additional confounders file, one per line, no header.\nEXTRA_COVARIATES: Path to additional covariates file, one per line, no header.\nENVIRONMENTALS: Path to additional environmental treatments file, one per line, no header.\nORDERS: Comma separated list describing the order of desired interaction parameters, 1 for the ATE (no interaction), 2 for pairwise interactions etc... e.g. \"1,2\"","category":"page"},{"location":"nextflow_params/#TMLE","page":"Index of the pipeline parameters","title":"TMLE","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"ESTIMATORFILE: YAML configuration file describing the nuisance parameters learners.\nPHENOTYPES_BATCH_SIZE: For a given causal model, phenotypes can be batched to save computational time. Setting this value to 0 will result in max size batches.","category":"page"},{"location":"nextflow_params/#Sieve-Variance-Correction","page":"Index of the pipeline parameters","title":"Sieve Variance Correction","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"GRM_NSPLITS: To fasten GRM computation, it is typically split in batches.\nNB_VAR_ESTIMATORS: Number of sieve variance estimates per curve. Setting this value to 0 results in skipping sieve variance correction.\nMAX_TAU: Variance estimates are computed for tau ranging from 0 to MAX_TAU\nPVAL_SIEVE: To save computation time and disk, only parameters with a p-value below this threshold are considered for sieve variance correction.","category":"page"},{"location":"nextflow_params/#Miscellaneous","page":"Index of the pipeline parameters","title":"Miscellaneous","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"CALL_THRESHOLD: Variants from BGEN files are called beyond that threhsold.\nPOSITIVITY_CONSTRAINT: Treatment variables rarest configuration should have at least that frequency.\nMAF_THRESHOLD: Only variants with that minor allele frequency are considered\nNB_PCS: Number of principal components to include in confouding adjustment\nFLASHPCA_EXCLUSION_REGIONS: Special file from flashpca repository.\nOUTDIR: Output directory","category":"page"},{"location":"contribution_guide/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"Contributions, whether bug fixes, new features or documentation improvements are very welcome!","category":"page"},{"location":"contribution_guide/#Raise-an-issue","page":"Contributing","title":"Raise an issue","text":"","category":"section"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"In order to discuss and track the evolution of the project, please first raise an issue on the targene-pipeline repository. If a change is agreed upon, the discussion should identify the relevant repositories that are concerned by the change. For instance, if one wishes to improve the extraction of traits from the UK-Biobank, the UKBMain.jl would surely be impacted and a new release for that package necessary. For now, at the pipeline level, each release is considered \"breaking\". This means that to propagate the changes, a new release, updating the dependency on the UKBMain package must also be made for the targene-pipeline.","category":"page"},{"location":"contribution_guide/#Developping","page":"Contributing","title":"Developping","text":"","category":"section"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"For each repository identified in the previous step, create a new branch for your contribution. When you think your work is ready, open a pull request for review.","category":"page"},{"location":"contribution_guide/#Releasing","page":"Contributing","title":"Releasing","text":"","category":"section"},{"location":"contribution_guide/","page":"Contributing","title":"Contributing","text":"Apart from the targene-pipeline, a release consists in a docker image published on the docker hub. For each repository, by creating a github release with an appropriate tag, an action will be triggered and the image built and pushed to the registry.","category":"page"},{"location":"sieve_variance/#Sieve-Variance-Plateau-Correction","page":"Sieve Variance Plateau Correction","title":"Sieve Variance Plateau Correction","text":"","category":"section"},{"location":"sieve_variance/","page":"Sieve Variance Plateau Correction","title":"Sieve Variance Plateau Correction","text":"Finally, the variance estimator can be adjusted via the Sieve Variance Plateau method. For that, we need to compute the GRM which is typically split via GRM_NSPLITS (default: 100). Then the number of estimators to compute in the interval [0, MAX_TAU (default: 0.8)] is given by NB_VAR_ESTIMATORS (default: 0). If NB_VAR_ESTIMATORS is set to 0, the Sieve Variance Plateau method will not be applied. It is also possible, in order to reduce the computational burden to perform this correction only if the initial p-value is below a specific threshold PVAL_SIEVE (default: 0.05). This is because in the correction will only increase the variance estimate.","category":"page"},{"location":"tmle/#Targeted-Maximum-Likelihood-Estimation","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"","category":"section"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"The estimator configuration file describes the TMLE specification for the estimation of the parameters defined in the previous section. This YAML configuration file is provided to the pipeline via the ESTIMATORFILE parameter. A set of predifined estimator files are available from https://github.com/TARGENE/targene-pipeline/estimators/.","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"That being said, you may need to use a custom set of learning algorithms for your project and write your own and contains 4 sections:","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"The threshold section is a simple floating point number that specifies the minimum allowed value for p(Treatments|Confounders).\nThe Q_binary, Q_continuous and G sections describe the learners for the nuisance parameters. Each of them contains a model that corresponds to a valid MLJ model constructor and further keyword hyperparameters. For instance, a Stack can be provided a measures argument to evaluate internal algorithms during cross validation. It can also be provided a potentially adaptive resampling strategy and the library of models. Each of those models can specify a grid of hyperparameters that will individually define a learning algorithm.\nQ_binary corresponds to E[Target| Confounders, Covariates] when the targets are binary variables.\nQ_continuous corresponds to E[Target| Confounders, Covariates] when the targets are continuous variables.\nG corresponds to the joint distribution p(Treatments|Confounders).","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"Here are two example estimator configurations that can serve as a template.","category":"page"},{"location":"tmle/#Example-1","page":"Targeted Maximum Likelihood Estimation","title":"Example 1","text":"","category":"section"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"In this example, Super Learners are used for both Q and G models. To perform a grid search across model hyperparameters, one can use a list of hyper-parameters. For instance, for the following Q_continuous learner, two EvoTreeRegressors will be part of the Super Learner with respectively 10 and 20 trees. The cross-validation procedure can be made adaptive based on the outcome class balance by using the adaptive: true option.","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"threshold: 1e-8\n# For the estimation of E[Y|W, T]: continuous target\nQ_continuous:\n  model: Stack\n  # Description of the resampling strategy\n  resampling:\n    type: CV\n    # The number of folds is determined based on the data\n    adaptive: true\n  # List all models and hyperparameters\n  models: \n    - type: GLMNetRegressor\n\n    - type: EvoTreeRegressor\n      nrounds: [10, 20]\n\n    - type: ConstantRegressor\n\n    - type: HALRegressor\n      max_degree: 1\n      smoothness_orders: 1\n      num_knots: [[10, 5]]\n      lambda: 10\n      cv_select: false\n\n# For the estimation of E[Y|W, T]: binary target\nQ_binary:\n  model: Stack\n  # Description of the resampling strategy\n  resampling:\n    type: \"StratifiedCV\"\n    # The number of folds is determined based on the data\n    adaptive: true\n  # List all models and hyperparameters\n  models:\n    - type: GLMNetClassifier\n\n    - type: ConstantClassifier\n\n    - type: HALClassifier\n      max_degree: 1\n      smoothness_orders: 1\n      num_knots: [[10, 5]]\n      lambda: 10\n      cv_select: false\n\n    - type: EvoTreeClassifier\n      nrounds: 10\n\n# For the estimation of p(T| W)\nG:\n  model: Stack\n  # Description of the resampling strategy\n  resampling:\n    type: \"StratifiedCV\"\n    # The number of folds is determined based on the data\n    adaptive: true\n    # List all models and hyperparameters\n  models:\n    - type: LogisticClassifier\n      fit_intercept: true\n    - type: ConstantClassifier\n    - type: EvoTreeClassifier\n      nrounds: 10","category":"page"},{"location":"tmle/#Example-2","page":"Targeted Maximum Likelihood Estimation","title":"Example 2","text":"","category":"section"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"While Super Learning is encouraged it is not strictly necessary, here for instance the propensity score is a simple gradient boosting tree and the outcome mode is a logistic regression when the outcome is binary.","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"threshold: 0.001\n# For the estimation of E[Y|W, T]: continuous target\nQ_continuous:\n  model: Stack\n  # Description of the resampling strategy\n  resampling:\n    type: CV\n    # The number of folds is determined based on the data\n    adaptive: true\n  # List all models and hyperparameters\n  models: \n    - type: InteractionGLMNetRegressor\n\n    - type: EvoTreeRegressor\n      nrounds: [10, 20]\n      λ: [0., 1.]\n      γ: [0.3]\n\n    - type: ConstantRegressor\n\n# For the estimation of E[Y|W, T]: binary target\nQ_binary:\n  model: LogisticClassifier\n  lambda: 10.\n\n# For the estimation of p(T| W)\nG:\n  model: EvoTreeClassifier\n  nrounds: 10","category":"page"},{"location":"tmle/#List-of-supported-models","page":"Targeted Maximum Likelihood Estimation","title":"List of supported models","text":"","category":"section"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"Here is a list of currently supported models, get in touch if you need more:","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"Model Regression Classification Source Package Comment\nGradient Boosting Trees EvoTreeRegressor EvoTreeClassifier EvoTrees.jl Pure Julia implementation of histogram based gradient boosting trees\nSelf Tuning EvoTree GridSearchEvoTreeRegressor GridSearchEvoTreeClassifier TargetedEstimation.jl Performs a grid search cross-validation over specified hyper parameters\nXGBoost XGBoostRegressor XGBoostClassifier XGBoost Julia wrapper around the original libxgboost\nSelf Tuning XGBoost GridSearchXGBoostRegressor GridSearchXGBoostClassifier TargetedEstimation.jl Performs a grid search cross-validation over specified hyper parameters\nLinear Models LinearRegressor LogisticClassifier MLJLinearModels.jl More models available, see: the docs\nGLM with penalization constant tuning GLMNetRegressor GLMNetClassifier TargetedEstimation.jl This is a simple MLJ API around GLMNet.jl\nSame as above with interaction terms InteractionGLMNetRegressor InteractionGLMNetClassifier TargetedEstimation.jl order of interaction is specified with order\nHighly Adaptive Lasso HALRegressor HALClassifier HighlyAdaptiveLasso.jl Simple wrapper around the original R package\nConstant model ConstantRegressor ConstantClassifier MLJModels.jl Always outputs the target's mean","category":"page"},{"location":"runtime_considerations/#Some-Runtime-considerations","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"If you are running a large scale analysis using TarGene, it is likely that you will need to make some trade offs about the estimation of nuisance parameters to keep the runtime under control. This runtime is mostly driven by the Targeted Maximum Likelihood Estimation process which will scale with the size of the study.","category":"page"},{"location":"runtime_considerations/#Unit-of-work","page":"Some Runtime considerations","title":"Unit of work","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"The unit of work is represented by a set of treatment variables, typically one or multiple SNPs and a set of outcome variables (phenotypes). This unit of work, which corresponds to a Phenome-Wide association study, can be coarsely decomposed as follows:","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"One estimation of the propensity score: P(TW) as specified in the ESTIMATORFILE configuration file.\nFor each outcome:\nThe initial estimation of EYT W as specified in the ESTIMATORFILE configuration file.\nFor each treatment case/control setting:\nThe TMLE step which roughly consists in fitting a Generalized Linear Model.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Let's illustrate with two examples:","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Scenario 1:\nParameters of interest:\nAverage Treatment Effects of Rs1799971: 0 → 1.\nAverage Treatment Effects of Rs1799971: 0 → 2.\nOutcomes: Height, diabetes.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"This scenario will require the execution of 1 unit of work which will be composed of:","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"1  P(TW) + 2  EYT W + 4  TMLE  steps","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Scenario 2:\nParameters of interest:\nAverage Treatment Effects of Rs4680: 0 → 1.\nAverage Treatment Effects of Rs1799971: 0 → 2.\nOutcomes: Height","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"This scenario will require the execution of 2 units of work each composed of:","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"1  P(TW) + 1  EYT W + 1  TMLE  step","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Those units of work can and will be run in parallel by TarGene if resources are available.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Since it is advocated to use Super Learning for both P(TW) and EYT W, those operations are typically driving the run time. We also note that estimating P(TW) is usually more expensive than EYT W.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Since genetic studies usually involve many SNPs and/or outcomes, we recognize that Super Learning in its purest form, may not always be a practical choice. We describe below some pointer to reduce computational time.","category":"page"},{"location":"runtime_considerations/#Super-Learning:-Cross-validation-scheme","page":"Some Runtime considerations","title":"Super Learning: Cross validation scheme","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Because Super Learning employs cross-validation, each learning algorithm in the library needs to be trained n times, n being the number of folds. If the adaptive cross-validation scheme is selected this can result in up to 20 folds cross-validation which will dramatically increase runtime. Here are three alternatives:","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"To keep that complexity bounded, one can resort to a fixed k-folds cross-validation where k is an arbitrary number, e.g. 3.\nThe least expensive resampling strategy is the resampling: Holdout which will only consist in a single out-of-sample evaluation.\nDon't use Super Learning. It is perfectly possible to use TarGene without Super Learning.","category":"page"},{"location":"runtime_considerations/#Algorithms","page":"Some Runtime considerations","title":"Algorithms","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"In TarGene, we try to provide a variety of different learning algorithms that can be combined with Super Learning. By restricting the number of algorithms in a Super Learner, runtime will evidently be reduced. Also, the runtime of all learning algorithms is not equal and will depend on hyperparameter choices. Developing an understanding of a learning algorithm helps to make more informed decision about it's usage.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Here are a few examples:","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"The GLMNetRegressor/GLMNetClassifier contain an internal k-fold cross-validation procedure that tunes the regularization hyperparameter. Combined with a p-folds Super Learner, this effectively results in k*p training procedures.\nThe GridSearch... models correspond to self-tuning models that will also perform an inner cross-validation to select the best hyper parameter setting among the provided grid.\nThe XGBoostRegressor/XGBoostClassifier have a tree_method hyperparameter that defaults to exact. The hist method is usually way faster and more appropriate for big datasets.","category":"page"},{"location":"runtime_considerations/#Typical-workloads","page":"Some Runtime considerations","title":"Typical workloads","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"The two following tables present the current runtimes for the two most common genetic association studies: PheWAS and GWAS. In both settings, we use 8 confounding variables and report runtimes for the 4 following learning strategies using an 8-cores compute node:","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"GLM: Standard generalized linear model\nGLMNet: GLM with regularization hyperparameter tuning over 3-folds cross-validation.\nXGBoost: This is the famous gradient boosting trees method with hyperparameter tuning over 10 different settings in a 3-folds cross-validation scheme.\nSL: Super Learning including both the previous XGBoost and GLMNet with a 3-folds cross-validation.","category":"page"},{"location":"runtime_considerations/#PheWAS","page":"Some Runtime considerations","title":"PheWAS","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"The following figures correspond to a typical PheWAS setting including 768 traits.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Learning Algorithm Time (hours)\nGLM 2.2\nGLMNet 4.5\nXGBoost 8.8\nSL 30","category":"page"},{"location":"runtime_considerations/#GWAS","page":"Some Runtime considerations","title":"GWAS","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"The following figures correspond to a typical GWAS setting. Since the propensity score's fit runtime is quite variable across SNPs, we perform TMLE for 100 SNPs and report the mean with two standard deviations. Further columns indicate estimated runtime when scaling to 600 000 SNPs with or without parallelizing over a high-performance computing platform (200 parallel jobs). While it would be impossible to run a GWAS on a personal laptop, we find that access to a modern computing platform makes this kind of study feasible using Targeted Learning.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Learning Algorithm Unit Time Projected GWAS Time on HPC Projected GWAS Time no HPC\nGLM 13 ± 4 seconds 10 hours 90 days\nGLMNet 57 ± 48 seconds 48 hours 1 year\nXGBoost 95 ± 6 seconds 72 hours 2 years\nSL 451 ± 141 seconds 375 hours 10 years","category":"page"},{"location":"associated_softwares/#Associated-Softwares","page":"Associated Softwares","title":"Associated Softwares","text":"","category":"section"},{"location":"associated_softwares/","page":"Associated Softwares","title":"Associated Softwares","text":"You may either not be interested in population genetics at all, or not willing to run the full pipeline on your project. If you are still eager to leverage the Targeted Learning framework, you can either rely on:","category":"page"},{"location":"associated_softwares/","page":"Associated Softwares","title":"Associated Softwares","text":"TMLE.jl: A Julia package for Targeted Maximum Likelihood Estimation (TMLE).\nTargetedEstimation.jl: A command line interface to run TMLE on your data.","category":"page"},{"location":"data_sources/#Setting-a-data-source","page":"Setting a data source","title":"Setting a data source","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"Currently, only the UK-Biobank is supported, stay tuned for further data sources!","category":"page"},{"location":"data_sources/#UK-Biobank","page":"Setting a data source","title":"UK-Biobank","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"The UK-Biobank is composed of both genetic data (.bed and .bgen files) and trait data. While there is a plan to use only bgen and trait data, the pipeline currently uses all sources of information. Below we explain how to specify those arguments to TarGene. For more information on the structure of the UK-Biobank data, please refer to their User Guide.","category":"page"},{"location":"data_sources/#Main-Dataset","page":"Setting a data source","title":"Main Dataset","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"The trait dataset is often called the \"main dataset\" and consists of an encrypted file containing individuals' trait information accessible via your project. The first option is thus to provide this dataset using the ENCRYPTED_DATASET parameter. Since the data is encrypted, the pipeline will also need the encoding file that you can provide with ENCODING_FILE.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"A \"main dataset\" is typically very large and only a few traits will be of interest to a given study. To extract those relevant traits from the dataset, a TRAITS_CONFIG YAML file must be provided. The structure of a file is composed of extraction rules that convert UK-Biobank fields to traits of interest. An example is presented below:","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"traits:\n    - fields:\n        - \"41202\"\n        - \"41204\"\n      phenotypes:\n        - name: \"Chronic lower respiratory diseases\"\n          codings:\n            - \"J40\"\n            - \"J41\"\n            - \"J410\"\n            - \"J411\"\n            - \"J418\"\n            - \"J42\"\n            - \"J43\"\n        - name: \"Other extrapyramidal and movement disorders\"\n          codings:\n            - \"G254\"\n            - \"G255\"\n            - \"G256\"\n            - \"G258\"\n    - fields:\n        - \"40006\"\n      phenotypes:\n        - name: \"Malignant melanoma of skin\"\n          codings:\n            - \"C430\"\n            - \"C431\"\n            - \"C432\"\n            - \"C433\"\n            - \"C434\"\n            - \"C435\"\n            - \"C436\"\n            - \"C437\"\n            - \"C438\"\n            - \"C439\"\n    - fields:\n        - \"1558\"\n      phenotypes:\n        - name: \"Alcohol intake frequency.\"\n    - fields:\n        - \"20117\"\n      phenotypes:\n        - name: \"Alcohol drinker status\"\n\nsubset: \n    - fields: 22001\n      phenotypes:\n        - name: Female\n          codings: 0\n    - fields: 21000\n      phenotypes:\n        - name: White\n          codings: [1001]","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"A file consists of two sections, a traits section and a subset section. Each section is further divided in a list of fields items. Each fields item is itself decomposed in multiple phenotypes items that are identified by a name and an optional list of codings. Alltogether, a phenotype element defines an extraction rule from a list of fields. The previous configuration file would thus restrict the analysis to white females and extract 5 traits. Since, writing by hand such a file for large scale study can quickly become tenuous, we provide a configuration file corresponding to the GeneAtlas study here.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"We now describe the currently available extraction rules based on variable types. Not that a variable type is determined by the metadata provided by the UK-Biobank and not a programmatic rule.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"Continuous/Count variables: Those consist in a single field and the extraction rule will take the first visit assessment value for that field. For instance, the \"Alcohol intake frequency.\" is simply the value from the \"1558-0.0\" column.\nOrdinal data: Some variables are encoded as categorical by the UK-Biobank but an ordinal interpretation seems more appropriate. A list of such variables has been identified and hard-coded. They will be treated like continuous variables.\nCategorical variables: For those variables again, the first visit assessment value for the specified field is used. An additional codings field can be provided. In that case the variable is turned into a indicator variable indicating whether an individual is included in the criterion given by the codings list. In the previous subset section, \"White\" is such a transformation into a binary variable. Note that we could add more elements to the list to subset on a wider population.\nCategorical Arrayed variables: Some fields in the UK-Biobank consist in list of traits for individuals. This is the case for at least the fields: 41202, 41204, 20002 and 40006. For those fields it is essential to define a codings section that describes a trait. For instance \"Malignant melanoma of skin\" corresponds to the declaration of any of the codings for an individual. Moreover, some fields share the same encoding, this is the case for  41202 and 41204. In that situation it may be useful to aggregate those sources of information. For that purpose multiple fields can be provided to the fields section. In that scenario the declaration of any of the coding in a codings section for any of the fields will define the trait.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"To come back to the pipeline specification, if a \"typical\" main dataset has already been decrypted outside of the pipeline, one may use the DECRYPTED_DATASET as an input to the pipeline instead of the ENCRYPTED_DATASET and ENCODING_FILE arguments. However, please make sure that all the fields in the TRAITS_CONFIG file described above are contained in the main dataset.","category":"page"},{"location":"data_sources/#Genetic-Data","page":"Setting a data source","title":"Genetic Data","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"We are currently using both .bgen and .bed files. Those are respectively provided with the UKBB_BGEN_FILES and UKBB_BED_FILES parameters. Since the UK-Biobank genotypic data is split in chromosomes, it should be of the form PREFIX_TO_CHROMOSOMES{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bgen,sample,bgen.bgi} and PREFIX_TO_CHROMOSOMES{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bed,bim,fam} respectively.","category":"page"},{"location":"data_sources/#Additional-Files","page":"Setting a data source","title":"Additional Files","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"Additional UK-Biobank required files for preprocessing and filtering are:","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"QC_FILE: A path to the UK-Biobank SNP quaility control ukb_snp_qc.txt file.\nWITHDRAWAL_LIST: A path to the withdrawal sample list to exclude removed participants from the study.","category":"page"},{"location":"overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Since TarGene is a Nextflow pipeline, the \"only\" thing that needs to be done to run a pipeline is to write a nextflow.config file providing a specification for each of the pipeline parameters. Most of those parameters are paths to additional files located in your project or in an accessible location on your system. The following sections describe those parameters and provide some examples to help you get started.","category":"page"},{"location":"#TarGene","page":"Home","title":"TarGene","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to TarGene, the software that brings Targeted Learning to population genetic studies! TarGene enables the estimation of various effect sizes including the Average Treatment Effect and the Interaction Average Treatment Effect (epistasis) up to any order. Because we follow the Targeted Learning framework, the final estimates provided by TarGene are covered by mathematical guarantees. The software is delivered as a Nexflow pipeline to bring scalability and reproducibility to your research.","category":"page"},{"location":"#Overview-of-the-pipeline","page":"Home","title":"Overview of the pipeline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The pipeline can rougly be decoupled into three steps. The first, aims at pre-processing the data sources to convert them in a table data format that can be wielded by Targeted Maximum Likelihood Estimation (TMLE). The second is the TMLE itself. The third and final step is the Sieve Variance Plateau correction which revises the variance estimate to account for the fact that individuals in the population are not necessarily independent. The following diagram provides a high level interface of the organisation of the pipeline.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div style=\"text-align:center\">\n<img src=\"assets/targene_diagram.png\" alt=\"Targene Pipeline\" style=\"width:800px;\"/>\n</div>","category":"page"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The pipeline has been tested with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Nextflow: 22.04\nSingularity: 3.8","category":"page"},{"location":"#Quick-start-for-Eddie-users","page":"Home","title":"Quick start for Eddie users","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are a University of Edinburgh researcher and have access to the Eddie cluster you may want to use the Eddie-Template to quickly setup your project. ","category":"page"},{"location":"#Getting-in-touch","page":"Home","title":"Getting in touch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please feel free to raise an issue if you face a problem or would benefit from a new feature. Contributions are most welcome.","category":"page"},{"location":"project_organization/#Project-Organization","page":"Project Organization","title":"Project Organization","text":"","category":"section"},{"location":"project_organization/","page":"Project Organization","title":"Project Organization","text":"The TarGene project is organized around the targene-pipeline repository which contains the Nextflow pipeline which will be the entry-point for most users. However, this repository does not contain the executables that are used by the Nextflow processes. Those executables originate from additional repositories:","category":"page"},{"location":"project_organization/","page":"Project Organization","title":"Project Organization","text":"UKBMain.jl\nTargetedEstimation.jl\nTargetedEstimation.jl","category":"page"},{"location":"project_organization/","page":"Project Organization","title":"Project Organization","text":"The following diagram presents a high level perspective of the project's organization.","category":"page"},{"location":"project_organization/","page":"Project Organization","title":"Project Organization","text":"<div style=\"text-align:center\">\n<img src=\"assets/targene_organization.png\" alt=\"TarGene Organization\" style=\"width:800px;\"/>\n</div>","category":"page"}]
}
