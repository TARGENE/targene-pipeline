var documenterSearchIndex = {"docs":
[{"location":"all_workflows_parameters/#Index-of-Workflows-Parameters","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"","category":"section"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"List of all workflows parameters. For more information on them please visit the dedicated user guides.","category":"page"},{"location":"all_workflows_parameters/#Required-Parameters","page":"Index of Workflows Parameters","title":"Required Parameters","text":"","category":"section"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"These must be provided, or the workflow will not run.","category":"page"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"ESTIMANDS_CONFIG: YAML configuration file describing the effect sizes of interest.\nTRAITS_DATASET: Path to a traits dataset. If you are running this for a non-UKB cohort, your sample IDs must be specified in the first column of this CSV file, with the column name SAMPLE_ID.\nBED_FILES: Path to PLINK BED files.\nBGEN_FILES: Path to indexed imputed BGEN files (optional for a GWAS).","category":"page"},{"location":"all_workflows_parameters/#Main-Options","page":"Index of Workflows Parameters","title":"Main Options","text":"","category":"section"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"These are optional but important as they can have a significant impact on the workflow (speed, estimates, ...).","category":"page"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"ESTIMATORS_CONFIG (default: wtmle-ose--tunedxgboost): Estimator name or Julia file containing the description of the Targeted Estimators to use. To be consistent it should match the argument provided to the previous TarGene run.\nBATCH_SIZE (default: 50): The set of estimands to be estimated is batched and the Targeted Learning processes will run in parallel across batches. This is the main driver of computational speed on High Performance Computing Platforms.\nCOHORT (default: UKB): Current default for this is UKB. If set to a value other than UKB, this will not run UKB-specific trait extraction.\nPOSITIVITY_CONSTRAINT (default: 0.01): When the list of estimands is generated or validated. Treatment variables' rarest configuration should have at least that frequency. For example if the treatment variables are two variants with minor allele A and T respectively. The rarest configuration will be (AA, TT) and should have a frequency of at least POSITIVITY_CONSTRAINT.\nNB_PCS (default: 6): The number of PCA components to extract.","category":"page"},{"location":"all_workflows_parameters/#UK-Biobank-Specific","page":"Index of Workflows Parameters","title":"UK Biobank Specific","text":"","category":"section"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"For UK Biobank analyses (COHORT=UKB).","category":"page"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"UKB_CONFIG (default: ${projectDir}/assets/ukbconfig.yaml): YAML configuration file describing which traits should be extracted and how the population should be subsetted.\nUKB_ENCODING_FILE: If the TRAITS_DATASET is encrypted, an encoding file must be provided.\nUKB_WITHDRAWAL_LIST: List of participants withdrawn from the study.\nQC_FILE: Genotyping quality control file from the UK-Biobank study.","category":"page"},{"location":"all_workflows_parameters/#Secondary-Options","page":"Index of Workflows Parameters","title":"Secondary Options","text":"","category":"section"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"These are of less importance.","category":"page"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"SVP (default: false): Whether Sieve Variance Plateau correction should be performed.\nFLASHPCA_EXCLUSION_REGIONS (default: assets/exclusion_regions_hg19.txt): A path to the flashpca special exclusion regions.\nMAF_THRESHOLD (default: 0.01): Only variants with that minor allele frequency are considered for PCA.\nLD_BLOCKS: A path to pre-identified linkage disequilibrium blocks to be removed from the BED files for PCA. It is good practice to specify LD_BLOCKS, as it will remove SNPs correlated with your variants-of-interest before running PCA.\nVERBOSITY (default: 0): Verbosity level of the the Workflow's processes.\nTL_SAVE_EVERY (default: BATCH_SIZE): During the estimation process, results are appended to the file in chunks to free memory.\nKEEP_IC (default: SVP): To save the Influence Curves for each estimate. Depending on the size of your dataset, this can result in very large disk usage.","category":"page"},{"location":"all_workflows_parameters/#Sieve-Variance-Plateau-Options","page":"Index of Workflows Parameters","title":"Sieve Variance Plateau Options","text":"","category":"section"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"These are only relevant if you which to apply the experimental sieve variane plateau correction (SVP=true).","category":"page"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"GRM_NSPLITS, default: 100: To fasten GRM computation, it is typically split in batches.\nNB_SVP_ESTIMATORS, default: 100: Number of sieve variance estimates per curve. Setting this value to 0 results in skipping sieve variance correction.\nMAX_SVP_THRESHOLD, default: 0.9: Variance estimates are computed for tau ranging from 0 to MAXSVPTHRESHOLD\nPVAL_THRESHOLD, default: 0.05: Only results with a p-value below this threshold are considered for Sieve Plateau Variance correction.\nESTIMATOR_KEY, default: 1: Identifies an estimator from ESTIMATORS_CONFIG. The p-value for PVAL_THRESHOLD is computed using the result from this estimator.","category":"page"},{"location":"all_workflows_parameters/#Simulations-Specific","page":"Index of Workflows Parameters","title":"Simulations Specific","text":"","category":"section"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"These apply to both null and realistic simulations (-entry NULL_SIMULATION or -entry REALISTIC_SIMULATION).","category":"page"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"N_REPEATS (default: 2): Number of bootstrap samples within a single process for each simulation task.\nRNGS (default: 1..250): Random number generators defining the number of parallel bootstrap processes, each running N_REPEATS bootstrap samples.\nSAMPLE_SIZES (default: [500000]): The dataset sample sizes for which the simulations will be run.\nMIN_FACTOR_LEVEL_OCCURENCES (default: 10): Each level of each factor should be sampled at least this amount of time.\nMAX_SAMPLING_ATTEMPTS (default: 10000): Number of sampling attempts to be made to respect MIN_FACTOR_LEVEL_OCCURENCES.\nNSAMPLES_FOR_TRUTH (default: 1000000): Monte Carlo samples used to evaluate the true effects.","category":"page"},{"location":"all_workflows_parameters/#Realistic-Simulation-Specific","page":"Index of Workflows Parameters","title":"Realistic Simulation Specific","text":"","category":"section"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"These are specific to the realistic simulation workflow (-entry REALISTIC_SIMULATION).","category":"page"},{"location":"all_workflows_parameters/","page":"Index of Workflows Parameters","title":"Index of Workflows Parameters","text":"TRAIN_RATIO (default: 6): Part of the data (out of 10) which is used for the training set of density estimators.\nSAMPLE_GA_HITS (default: true): Whether additional variants should be sampled from the geneATLAS.\nGA_MAX_VARIANTS (default: 50): The maximum number of variants to sample if SAMPLE_GA_HITS is true.\nGA_DISTANCE_THRESHOLD (default, 1000000): Minimum number of base pairs between sampled geneATLAS variants.\nGA_PVAL_THRESHOLD (default: 1e-5): Minimum p-value between geneATLAS sampled variants and the traits.\nGA_MAF_THRESHOLD (default: 0.01): Minimum minor allele frequency of geneATLAS sampled variants.\nGA_TRAIT_TABLE (default: \"${projectDir}/assets/Traits_Table_GeneATLAS.csv\"): Path to the downloaded geneATLAS association table.","category":"page"},{"location":"targene/tmle/#Specifying-a-Targeted-Estimator","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"","category":"section"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"TarGene is a flexible procedure that does not impose any constraint on the functional form of the relationship between genetic variants, environmental variables and traits. In practice, we rely on MLJ for all machine learning algorithms. In population genetics studies, there are two learning algorithms we need to specify:","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"Q_Y = mathbbEYT W C: The mean outcome given the treatment, confounders and extra covariates.\nG = P(TW): The propensity score, which enables the targeting step of the estimation procedure.","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"In TarGene, the default it to use for both models, a cross-validated version of XGBoost over 3-folds and a range of hyper-parameters.","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"Furthermore, we estimate genetic effects using both weighted Targeted Minimum-Loss Estimation (wTMLE) and One-Step Estimation (OSE). Note that these estimators are not combined in a single estimator, two distinct estimation procedures are performed. The reason for this is that the finite sample behavior of semi-parametric estimators in population genetics is still under study. Providing the results for both estimation methods thus enables a straightforward comparison.","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"info: Note\nRest assured though, from our experience, these results should be almost indistinguishable. Furthermore, the computational cost of the OSE is almost negligible if (w)TMLE is performed and vice versa.","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"A default TarGene run will thus result in two estimates for each estimand:","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"wTMLE using XGBoost for both Q_Y and G\nOSE using XGBoost for both Q_Y and G","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"Note however that this is not compulsory. A single estimator can be used and the machine-learning models customised. This is done via the ESTIMATORS_CONFIG which is either a simple configuration string or a more elaborate Julia file. Both options are further discussed here.","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"For example, using a configuration string, a computationally cheaper run with ESTIMATORS_CONFIG=ose--glmnet uses only One-Step Estimation with a GLMNet model for both Q_Y and G.","category":"page"},{"location":"targene/outputs/#Understanding-TarGene's-Outputs","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"","category":"section"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"The successful completion of the workflow will produce the following files in the output directory OUTDIR (default: results):","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"QQ.png: A Quantile-Quantile summary plot.\nresults.summary.yaml: A summary statistics file of your results.\nresults.hdf5: A file containing the complete description of estimands and associated estimates in HDF5 format.\nsvp.hdf5: An optional file containing Sieve Variance Plateau corrected variance estimates (see Correcting for population relatedness (Experimental)).","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"If you come from the world of linear models where a single beta and p-value is output for each variant, TarGene's output may seem difficult to read at first. In this section we explain how they are structured and how to work with them. ","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"The crucial difference, is that TarGene estimates the effect of change. As such there is one estimate (hatbeta) per genotype change. Consider a single variant with genotypes CC, CT and TT. TarGene estimates non-redundant changes, in this case it would be both CC rightarrow CT and CT rightarrow TT. For each of these changes, there is thus an effect size, e.g. hatbeta_CC rightarrow CT, and a p-value, e.g. textpval_hatbeta_CC rightarrow CT. ","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"However, it is often more intuitive to think of the \"full\" effect of a variant on an outcome. This is simply defined as the joint effect of all changes, i.e., beta_CC rightarrow CT beta_CT rightarrow TT. This effect is nonzero if it is different from 0 0. The multivariate central limit theorem tells us that this joint effect is a multivariate normal and a single p-value can be obtained for it.","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"Finally, estimation can only be accurate for genotypes that are not too rare. This is known as the positivity condition, and is similar to the traditional minor allele frequency threshold used in GWAS. In TarGene this is defined by the POSITIVITY_CONSTRAINT (default: 0.01) parameter, for which a default value was chosen based on simulation studies. In TarGene, only changes that satisfy the positivity constraint are considered for estimation. In the example above, if the genotypes frequencies were 0.745, 0.25 and 0.005 respectively, only the CC rightarrow CT would satisfy the constraint, and only this change would be estimated.","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"We have discussed single variant effects, but interactions are defined exactly in the same way. The difference is that they are defined by two or more variants, each of which with possibly multiple genotype changes. Consider two variants V_1 and V_2, the interaction between V_1 and V_2 is defined by the following genotype changes:","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"V_1 CC rightarrow CT V_2 AA rightarrow AG\nV_1 CT rightarrow TT V_2 AA rightarrow AG\nV_1 CC rightarrow CT V_2 AG rightarrow GG\nV_1 CT rightarrow TT V_2 AG rightarrow GG","category":"page"},{"location":"targene/outputs/#The-QQ.png","page":"Understanding TarGene's Outputs","title":"The QQ.png","text":"","category":"section"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"If you understood the previous section, now you know there is one p-value per genetic effect, even if this effect has multiple components. These p-values can be plotted against the p-values that would be obtained if null hypothesis was true in a QQ plot. Because a TarGene run may comprise multiple estimators, there can be multiple overlapping QQ plots, one for each estimator.","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"(Image: QQ)","category":"page"},{"location":"targene/outputs/#The-results.summary.yaml","page":"Understanding TarGene's Outputs","title":"The results.summary.yaml","text":"","category":"section"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"The results.summary.yaml is a summary file which probably contains all the information you need. It provides a list of estimates corresponding to your run specification. The following example illustrates the format.","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"- EFFECT_TYPE: \"ATE\"\n  OUTCOME: hypothyroidism/myxoedema\n  TREATMENTS:\n    - rs238411180: \"CC => CT\"\n    - rs238411180: \"CT => TT\"\n  OSE:\n    PVALUE: 0.5270550612752147\n    COMPONENTS:\n      - PVALUE: 3.1691860303379483e-24\n        EFFECT_SIZE: 0.03549905917106507\n      - PVALUE: 0.011507985165943658\n        EFFECT_SIZE: -0.0011000033630272567\n- EFFECT_TYPE: \"AIE\"\n  OUTCOME: C50-C50 Malignant neoplasm of breast\n  TREATMENTS:\n    - rs3502414: \"TT => CT\"\n      Cheese intake: \"3 => 2\"\n    - rs3502414: \"TT => CT\"\n      Cheese intake: \"2 => 1\"\n    - rs3502414: \"TT => CT\"\n      Cheese intake: \"1 => 4\"\n  OSE:\n    PVALUE: 0.6007356699283757\n    COMPONENTS:\n      - PVALUE: 3.1691860303379483e-24\n        EFFECT_SIZE: 0.03549905917106507\n      - PVALUE: 0.011507985165943658\n        EFFECT_SIZE: -0.0011000033630272567\n      - PVALUE: 0.98\n        EFFECT_SIZE: 0.09190645731696134\n- EFFECT_TYPE: \"ATE\"\n  OUTCOME: L50-L54 Urticaria and erythema\n  TREATMENTS:\n    rs117913124: \"GG => GA\"\n    rs53453: \"GG => GA\"\n  OSE:\n    PVALUE: .NaN\n    EFFECT_SIZE: \"Failed\"","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"The first estimate corresponds to the Average Treatment Effect (ATE) of rs238411180 on hypothyroidism/myxoedema. The TREATMENTS section contains all variables whose effect on outcome is estimated. In this case, the two genotype changes are presented, indicating that all genotypes passed the positivity constraint. Then one section per estimator is reported (see Specifying a Targeted Estimator). In this case a single estimator, the One-Step Estimator, corresponding to the OSE section. The joint p-value of the effect is provided in PVALUE together with the list of changes' effect sizes in COMPONENTS. Each component actually contain both an effect size EFFECT_SIZE and a p-value PVALUE corresponding to the single change's test.","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"The second estimate corresponds to the Average Interaction Effect (AIE) of rs3502414 and Cheese intake on C50-C50 Malignant neoplasm of breast. Only the TT rightarrow CT change is presented, indicating that the other change did not pass the positivity constraint. The other sections are similar to the previous estimate.","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"Finally, the third element corresponds to the Average Treatment Effect of two variants (rs117913124, rs53453) on L50-L54 Urticaria and erythema. In this case, only a single change was requested by the user. We can see that because the TREATMENTS section is not a list. Also, in some cases, estimation can fail as reported here.","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"Together, these three examples should help you get started with post processing your results!","category":"page"},{"location":"targene/outputs/#The-results.hdf5-(Advanced)","page":"Understanding TarGene's Outputs","title":"The results.hdf5 (Advanced)","text":"","category":"section"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"In some cases, you may want to perform additional operations on your estimates. For example, if you would like to compute differential allelic effects corresponding to the following question: are the beta_CC rightarrow CT and  beta_CT rightarrow TT identical? This can be done using the comprehensive results.hdf5 file, which retains all the estimation information (e.g., covariance matrices).","category":"page"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"This file contains a DataFrame which can be loaded within a Julia session for instance. This DataFrame contains one column per estimator for the TarGene run (only OSE in the example above). Each element of a column is an estimation result corresponding to a given genetic effect (there were 2 in the above example). These are objects that are defined within the TMLE.jl package and it is recommended to use it for downstream analysis. The DataFrame also contains an additional p-value column for each of these estimators. In the previous example there would be a total of only 2 columns: [OSE, OSE_PVALUE].","category":"page"},{"location":"targene/outputs/#The-svp.hdf5-(Experimental)","page":"Understanding TarGene's Outputs","title":"The svp.hdf5 (Experimental)","text":"","category":"section"},{"location":"targene/outputs/","page":"Understanding TarGene's Outputs","title":"Understanding TarGene's Outputs","text":"This file is similar to the results.hdf5 except that the standard errors are computed with a corrected variance estimator.","category":"page"},{"location":"examples/setup/#Setup-(Read-First)","page":"Setup (Read First)","title":"Setup (Read First)","text":"","category":"section"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"A population genetics analysis via TarGene is a 4 steps process where steps 1 and 2 are done only once.","category":"page"},{"location":"examples/setup/#1.-Installation","page":"Setup (Read First)","title":"1. Installation","text":"","category":"section"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"Install TarGene's dependencies as per the Installation section. Here I will assume that both Nextflow and Singularity are installed. To make sure this is the case you can for instance try","category":"page"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"nextflow -v","category":"page"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"and","category":"page"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"singularity --version","category":"page"},{"location":"examples/setup/#2.-Obtaining-the-Data","page":"Setup (Read First)","title":"2. Obtaining the Data","text":"","category":"section"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"Make sure the data is available. Here I will rely on TarGene's test data located here. To obtain it, the easiest way is maybe to clone the entire project like so ","category":"page"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"git clone https://github.com/TARGENE/targene-pipeline","category":"page"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"You can then open the test/assets directory (or simply the full project but you need to open a terminal in test/assets) using your favorite text editor. ","category":"page"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"note: This is only to get the data\nEven though we have cloned the repository, we do not need to look at the code at all!","category":"page"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"All examples below assume that the dataset is the UK-Biobank data (we are not really using the UK-Biobank but the data was created to mimic its structure).","category":"page"},{"location":"examples/setup/#3.-Writing-a-Run-Configuration-File","page":"Setup (Read First)","title":"3. Writing a Run Configuration File","text":"","category":"section"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"Write a run configuration file for your project, this can be decomposed further into 2 sub-steps:","category":"page"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"Write the part of your configuration file specific to your platform. Here we will simply use the local profile.\nWrite the part of your configuration file specific to your run. This is the topic of the remainder of the following sections.","category":"page"},{"location":"examples/setup/#4.-Analyse-the-Results","page":"Setup (Read First)","title":"4. Analyse the Results","text":"","category":"section"},{"location":"examples/setup/","page":"Setup (Read First)","title":"Setup (Read First)","text":"Hopefully, this is where you'll find something new or interesting in some way. This section is up to you but we recommend to read the section Understanding TarGene's Outputs or you will likely be a little lost...","category":"page"},{"location":"examples/gwas/#GWAS","page":"GWAS","title":"GWAS","text":"","category":"section"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"Perhaps the most common study design in population genetics is the Genome-Wide Association Study (GWAS). In TarGene, a GWAS can only be performed across plink BED files. A minimalist run configuration for a GWAS in TarGene looks like the following:","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"params {\n    ESTIMANDS_CONFIG = \"gwas_config.yaml\"\n    ESTIMATORS_CONFIG = \"wtmle--glm\"\n\n    // UK-Biobank specific parameters\n    BED_FILES = \"unphased_bed/ukb_chr{1,2,3}.{bed,bim,fam}\"\n    UKB_CONFIG = \"ukbconfig_gwas.yaml\"\n    TRAITS_DATASET = \"dataset.csv\"\n    UKB_WITHDRAWAL_LIST = \"withdrawal_list.txt\"\n}","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"Apart from the data related parameters, there are two main parameters here: ESTIMANDS_CONFIG and the ESTIMATORS_CONFIG. These parameters describe the estimands (questions of interest) and how to estimate them respectively. Since we are performing a GWAS, we are interested in the effect of all variants, across all phenotypes in the UKB_CONFIG file. ","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"The ESTIMANDS_CONFIG is a pretty succinct YAML file for a GWAS, it could contain only one line. Here we will be a little more fancy and add extra outcome predictors to improve the precision of inference","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"type: gwas\n\noutcome_extra_covariates:\n  - \"Number of vehicles in household\"\n  - \"Cheese intake\"","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"The optional outcome_extra_covariates are variables to be used as extra predictors of the outcome (but not as confounders). How does TarGene know how to extract these traits from the the UK Biobank main dataset? This is thanks to the UKB_CONFIG file, which maps UK Biobank data fields to traits (see The UKB_CONFIG Configuration File). In our case, this file will contain both the outcome of interest (BMI) and the extra predictors we need (Number of vehicles in household and Cheese intake).","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"traits:\n  - fields:\n      - \"21001\"\n    phenotypes:\n      - name: \"Body mass index (BMI)\"\n  - fields:\n      - \"728\"\n    phenotypes:\n      - name: \"Number of vehicles in household\"\n  - fields:\n      - \"1408\"\n    phenotypes:\n      - name: \"Cheese intake\"","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"Note that the outcome is defined implicitely, any trait in the UKB_CONFIG file which is not in the outcome_extra_covariates will be considered as an outcome. So you can run multiple GWAS at once by simply adding another trait definition to the above file.","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"Finally, the ESTIMATORS_CONFIG = \"wtmle--glm\" defines the estimation strategy. We will be using Targeted Minimum-Loss Estimator with a simple Generalized Linear Model (glm) to learn the outcome models (Q_Y)and propensity scores (G).","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"The GWAS can then be run as follows:","category":"page"},{"location":"examples/gwas/","page":"GWAS","title":"GWAS","text":"nextflow run https://github.com/TARGENE/targene-pipeline -r v0.11.1 -profile local","category":"page"},{"location":"developer_guide/project_organization/#Project-Organization","page":"Project Organization","title":"Project Organization","text":"","category":"section"},{"location":"developer_guide/project_organization/","page":"Project Organization","title":"Project Organization","text":"The TarGene project is organized around the targene-pipeline repository which contains the Nextflow workflows. However, this repository does not contain the executables that are used by the Nextflow processes. Those executables originate from complementary repositories or modules. These are all Julia packages. However, this is just an implementation detail since each package releases a Docker image within which the various command-line executables can be used.","category":"page"},{"location":"developer_guide/project_organization/","page":"Project Organization","title":"Project Organization","text":"TMLECLI.jl\nTargeneCore.jl\nUKBMain.jl\nSimulations.jl","category":"page"},{"location":"developer_guide/project_organization/","page":"Project Organization","title":"Project Organization","text":"The following diagram presents a high level perspective of the project's organization and dependence structure.","category":"page"},{"location":"developer_guide/project_organization/","page":"Project Organization","title":"Project Organization","text":"(Image: TarGene Organization)","category":"page"},{"location":"simulations/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"The asymptotic performance of semi-parametric estimators is theoretically optimal. However, in practice, asymptotic regimes are not necessarily achieved even for large sample sizes because some events are extremely rare. This is particularly prevalent in population genetics where some genetic variants and traits are found in less than 1 of individuals. In the absence of finite sample guarantees, simulation studies provide an effective way to validate statistical methods. An ideal simulation would both yield ground-truth values for the causal effects of interest and be representative of real data. In TarGene, we propose two types of simulations to analyse the performance of semi-parametric estimators.","category":"page"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"The first type of simulations, called the null simulation, aims at analysing the behaviour of the estimators when the null hypothesis of \"no effect\" is true. The rationale behind this simulation is that most genetic variants are believed to have no effect, it is thus of particular importance that the type 1 error rate be controlled appropriately.","category":"page"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"The second type of simulations, called the realistic simulation, fits the generating process using data-adaptive flexible machine-learning generative models. As such, it retains important features of the true generating process, but also provides ground truth values via Monte-Carlo sampling.","category":"page"},{"location":"simulations/overview/#Bootstrap-Size","page":"Overview","title":"Bootstrap Size","text":"","category":"section"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"Both simulations rely on bootstrap sampling, that is, we resample the dataset B times to obtain the sampling distribution of the estimators. This is computationally expensive since machine learning models need to be fitted for each bootstrap sample. In order to maximise parallelisation opportunities offered by HPCs, instead of specifying B, we specify two parameters: N_REPEATS (default: 2) and RNGS (default: 1..250). For each rng in RNGS, a single process performing N_REPEATS bootstrap will be run. Thus, we effectively have B = N_REPEATS times length(RNGS). For example the default uses B=500 bootstrap samples, but if we let N_REPEATS=10 and RNGS=1..250, then B=2500.","category":"page"},{"location":"simulations/overview/#Simulation-Tasks","page":"Overview","title":"Simulation Tasks","text":"","category":"section"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"We are interested in the performance of estimators for various estimands, but also potentially for various sample sizes. The triple (sample size, estimator, estimand), thus defines a simulation unit or task. In other words, B bootstrap samples will be run for each task defined by the cartesian product of all sample sizes, estimators and estimands, hence defining the total simulation scope.","category":"page"},{"location":"simulations/overview/#Sample-Sizes","page":"Overview","title":"Sample Sizes","text":"","category":"section"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"For each sample size in SAMPLE_SIZES (default: [500000]), the dataset is resampled with replacement according to the processes defined in the next sections. For example if SAMPLE_SIZES = [1000, 500000], the estimators will be evaluated against all estimands for both sample sizes.","category":"page"},{"location":"simulations/overview/#Estimators","page":"Overview","title":"Estimators","text":"","category":"section"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"Like for a normal discovery run, the set of estimators to be evaluated is defined by the ESTIMATORS_CONFIG parameter, which in this case can be a list. For example if ESTIMATORS_CONFIG = [\"wtmle-ose--glm\", \"wtmle-ose--tunedxgboost\"], a total of 4 estimators will actually be evaluated:","category":"page"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"wTMLE with a GLM for both Q_Y and G\nOSE with a GLM for both Q_Y and G\nwTMLE with a cross-validated XGBoost for both Q_Y and G\nOSE with a cross-validated XGBoost for both Q_Y and G","category":"page"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"For more information on defining estimators, have a look at the Specifying a Targeted Estimator section.","category":"page"},{"location":"simulations/overview/#Estimands","page":"Overview","title":"Estimands","text":"","category":"section"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"Finally, the set of estimands is provided via the ESTIMANDS_CONFIG which should be a list of estimands like in the Custom (Advanced) section.","category":"page"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"Easier ways to describe estimands may be added in the future but note that given the computational complexity of these simulations, a genome-wide simulation is currently unrealistic.","category":"page"},{"location":"simulations/overview/","page":"Overview","title":"Overview","text":"info: Number of Estimands\nKeep it small to start with, at the moment experiments have been run with around 30 different genetic effects.","category":"page"},{"location":"targene/data_sources/#Genetic-and-Traits-Data","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"The dataset is the main ingredient manipulated by TarGene. Population Genetics datasets are typically split into a variety of files and formats. The following section describes these formats and how to input them to a TarGene run.","category":"page"},{"location":"targene/data_sources/#Genetic-Data","page":"Genetic and Traits Data","title":"Genetic Data","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"TarGene currently only works with genotyping arrays datasets. These datasets can come in multiple formats and we currently use both bgen and bed formats. It is also assumed that the genotyping data is unphased. and split in chromosomes.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Unless you are performing a GWAS, both bgen and bed files are required. This is because PCA uses the bed files while the variants of interest are looked up within the more comprehensive bgen files. Since the later format does not provide variant calls but only imputation probabilities, the CALL_THRESHOLD (default: 0.9) parameter is used to call genotypes. In the GWAS setting, the complete list of all variants within the .bed files is used for analysis.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"The bgen and bed files are provided via the BGEN_FILES and BED_FILES Nextflow parameters respectively. For example:","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"BGEN_FILES=\"path_to_folder/imputed_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bgen,sample,bgen.bgi}\" \nBED_FILES=\"path_to_folder/genotyped_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bed,bim,fam}","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"tip: File Names\nIt is advised to deviate as little as possible from the prefix above (prefix ending in \"chr\") to make sure everything works smoothly.","category":"page"},{"location":"targene/data_sources/#Traits-Dataset","page":"Genetic and Traits Data","title":"Traits Dataset","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"In addition to genetic data, a set of traits is usually also available in a different format. We currently support two types of cohorts: the UK Biobank and custom CSV datasets.","category":"page"},{"location":"targene/data_sources/#Custom-Traits-Dataset","page":"Genetic and Traits Data","title":"Custom Traits Dataset","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Select with COHORT = \"MY_COHORT\" (The actual name does not matter as long as it does not match an existing cohort).","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"TarGene supports custom traits datasets in CSV format, that can be provided via the TRAITS_DATASET parameter. Please ensure the sample-ids identifying individuals in your cohort are included as the first column of your trait data, with the column name SAMPLE_ID.","category":"page"},{"location":"targene/data_sources/#UK-Biobank","page":"Genetic and Traits Data","title":"UK-Biobank","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Select with COHORT = \"UKB\".","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"If the dataset provided as TRAITS_DATASET is the encrypted dataset (potentially suffixed by enc_ukb), then the decryption file must be provided via UKB_ENCODING_FILE. If you have already decrypted the dataset, you don't need to provide a UKB_ENCODING_FILE. We refer to either the encrypted or decrypted dataset as the main dataset.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"The information contained in the UK-Biobank main dataset is not directly usable for statistical analyses and needs to be processed. This is both because the data format is non-standard and because some fields contain information about multiple phenotypes. Furthermore, multiple fields can also be combined to define custom phenotypes. The UKB_CONFIG file lets you provide a mapping between the UK Biobank Data-Fields and actual phenotypes. By default, it defines 110 non-binary and 660 binary traits as previously defined by the geneATLAS. In cases, you may be interested in only a few traits or would like to define more. The goal of the following sections is to help you do that. We first describe some of the main UK-Biobank fields, then explain the rules according to which they are converted into individual phenotypes and end with some examples.","category":"page"},{"location":"targene/data_sources/#Data-Fields-Description","page":"Genetic and Traits Data","title":"Data Fields Description","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Phenotypes in the UK-Biobank are organised in Data-Fields, that may contain information about multiple phenotypes or can be combined, here is a non-exhaustive lit of them.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Diagnoses made during hospital inpatient admission (467 traits, Data-Fields 41204 and 41202). This category contains information relating to main and secondary diagnoses made during hospital inpatient admissions. Each trait in this category corresponds to a node or set of nodes in the International Classification of Diseases 10-th Revision (ICD10) ontology. For example, \"K41 Femoral hernia\" is defined by any of the following diagnoses (\"K410\", \"K411\", \"K412\", \"K413\", \"K414\", \"K419\").","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Self reported conditions (161 traits, Data-Field 20002). This category contains data obtained through a verbal interview by a trained nurse on past and current medical conditions, including type of cancer and other illnesses, the number of medical conditions, and date of diagnosis. This field can contain inaccuracies due to recall bias or intentional misreporting.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Self reported conditions (29 traits, Data-Field 40006) This category contains coded data on cancer incidence, obtained through linkage to national cancer registries. Because data is continually accruing, the number of cases may vary depending on the time the data is sent out to researchers.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Further Miscellaneous fields (113 traits) These traits do not correspond to diseases but to various lifestyle behaviours or biological measurements. For instance Data-Field 30180 corresponds to Lymphocyte percentage in a blood assay while Data-Field 1389 corresponds to Pork intake.","category":"page"},{"location":"targene/data_sources/#The-Extraction-Rules","page":"Genetic and Traits Data","title":"The Extraction Rules","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Phenotypes are defined from fields according to rules. These rules are either defined at the field level, or based on its \"value type\" metadata, which indicates the data type the field contains. Here we summarise these rules.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Continuous and Integer Fields (value type = 31, 11 respectively). These contain one or more values per individual corresponding to the multiple visit assessments. The value of the field at the first visit assessment is extracted. For instance, the \"Alcohol intake frequency.\" is simply the value from the \"1558-0.0\" column.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Ordinal Fields (1408, 1727, 1548, 728, 1717, 1389, 1478, 1518, 1558, 1349, 1359, 1369, 1379, 1329, 1339, 1239, 1687, 1697, 1319, 1498). Some variables are encoded as categorical by the UK-Biobank, but an ordinal interpretation seems more appropriate. For instance, \"Lamb intake\" ranges from 0 (Never) to 5 (Once or more daily). Similarly to continuous fields, the value of the field at the first visit assessment is extracted. Negative values (Do not know, Prefer not to answer) are treated as missing.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Categorical Fields (value type = 21). These variables cannot be turned into continuous variables and are usually represented via a coding. For instance, the field 21000, described as Ethnic background, uses coding 1001 for British individuals, 4001 for Caribbeans 5 for Chinese etc... These can currently be processed in two different ways by TarGene. Either the coding of each individual is extracted, or, if multiple codings are specified, an indicator of these codings is produced (1 or 0). For instance, \"White\" could be defined as any of the following codings {1001, 1002, 1003}, that is British, Irish or Any other white background. Similarly to ordinal variables, the value at the first visit assessment is extracted and negative values are considered missing.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Categorical Arrayed Fields ({40006, 20002, 41202, 41204}). Some fields in the UK-Biobank comprise a list of codings for each individual at each visit. Each coding represents, for example, a disease or condition that was diagnosed or self reported. A phenotype is then defined by a set of codings. As an example, we define \"oesophageal disorder\" as the set of the following codings: {1134, 1139, 1140, 1141, 1138, 1474} from Data-Field 20002. An individual annotated with any of these codings, at any assessment visit, is considered a case for \"oesophageal disorder\". Moreover, some fields share the same codings, this is the case for 41202 and 41204. In that situation it may be useful to aggregate these sources of information. For instance, \"G20 Parkinson's disease\" is defined by the single element set {G20}, in any of the 41202 and 41204 Data-Fields.","category":"page"},{"location":"targene/data_sources/#The-UKB_CONFIG-Configuration-File","page":"Genetic and Traits Data","title":"The UKB_CONFIG Configuration File","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"We now explain how these rules are encoded within the UKB_CONFIG YAML file. For reference, the default configuration file corresponds to the geneATLAS study and can be used as a template.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"The structure of the UKB_CONFIG YAML file consists of extraction rules that convert UK-Biobank fields to traits of interest. It contains two sections, a traits section and a subset section.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"The traits section describes the phenotypes that need to be extracted from the main TRAITS_DATASET.\nThe subset section provides a way to filter the data based on specific traits and hence estimate conditional effects. Each element in this section corresponds to an additional condition to be met by an individual. For instance, the default configuration only includes White individuals.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Each section is further divided in a list of fields items that match the UK-Biobank data fields and a list of phenotypes that can be extracted from these fields. Each phenotype in the phenotypes subsection of the UKB_CONFIG YAML file is identified by a name and an optional list of codings defining it. Altogether, a phenotype element defines an extraction rule from a list of fields and codings.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"It is probably more helpful to look at an example:","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"traits:\n    - fields:\n        - \"41202\"\n        - \"41204\"\n      phenotypes:\n        - name: \"Chronic lower respiratory diseases\"\n          codings:\n            - \"J40\"\n            - \"J41\"\n            - \"J410\"\n            - \"J411\"\n            - \"J418\"\n            - \"J42\"\n            - \"J43\"\n        - name: \"Other extrapyramidal and movement disorders\"\n          codings:\n            - \"G254\"\n            - \"G255\"\n            - \"G256\"\n            - \"G258\"\n    - fields:\n        - \"40006\"\n      phenotypes:\n        - name: \"Malignant melanoma of skin\"\n          codings:\n            - \"C430\"\n            - \"C431\"\n            - \"C432\"\n            - \"C433\"\n            - \"C434\"\n            - \"C435\"\n            - \"C436\"\n            - \"C437\"\n            - \"C438\"\n            - \"C439\"\n    - fields:\n        - \"1558\"\n      phenotypes:\n        - name: \"Alcohol intake frequency.\"\n    - fields:\n        - \"20117\"\n      phenotypes:\n        - name: \"Alcohol drinker status\"\n\nsubset: \n    - fields: 22001\n      phenotypes:\n        - name: Female\n          codings: 0\n    - fields: 21000\n      phenotypes:\n        - name: White\n          codings: [1001]","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"This configuration file restricts the analysis to White females and extract 5 traits.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"The first trait, \"Chronic lower respiratory diseases\" is defined by any of the (J40, J41, J410, J411, J418, J42, J43) codings within any of the fields (41202, 41204).","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"The phenotype \"Malignant melanoma of skin\" is defined by any of (C430, C431, C432, C433, C434, C435, C436, C437, C438, C439) within the 40006 field.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"The \"Alcohol drinker status\" is defined from the first visit assessment questionnaire status.","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"tip: Constructing Additional columns\nIf you are providing a decrypted TRAITS_DATASET, you can build and add extra columns to it (for instance to define new phenotypes). Any column in the TRAITS_DATASET which does not correspond to a UK-Biobank field will be considered as such.","category":"page"},{"location":"targene/data_sources/#Additional-Files","page":"Genetic and Traits Data","title":"Additional Files","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Additional optional UK-Biobank files for preprocessing and filtering are:","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"QC_FILE: A path to the UK-Biobank SNP quality control ukb_snp_qc.txt file.\nUKB_WITHDRAWAL_LIST: A path to the withdrawal sample list to exclude removed participants from the study.","category":"page"},{"location":"targene/data_sources/#All-of-Us-cohort","page":"Genetic and Traits Data","title":"All of Us cohort","text":"","category":"section"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"TarGene can now be run on the All of Us (AoU) Cohort through the AoU Researcher Workbench. Your Traits Dataset must be built on the AoU Researcher Workbench through their interactive Dataset Builder tool (See Dataset Builder for more information on this). This must be done on a Workspace created to run your analysis-of-interest (See Creating a Workspace). The dataset created using these tools must be tailored to pull the traits, confounders and/or covariates-of-interest from the All of Us cohort, subsetting for samples for which genetic data is available. As TarGene requires both PLINK BED and BGEN files, these must be available for the participants for which trait data is being pulled. ","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Through Dataset Builder tool on the AoU Researcher Workbench, you must create a matrix that matches the input required by the COHORT = \"CUSTOM\" mode. This requires your partipicant IDs (named, by default, as person_id in AoU) to be within a column named SAMPLE_ID, and any covariates or confounders to be included in subsequent columns. Each covariate or confounder in the subsequent columns must have a one-to-one mapping with each SAMPLE_ID. Please note that the AoU cohort includes data compiled across Electronic Health Care records, and therefore the same patient may have multiple entries for a given measurement (for example, BMI measured across multiple GP appointments). This must be dealt with accordingly when configuring your Traits Dataset. For example, you may choose to pick the most recent measurement for a given participant. The AoU Researcher Workbench provides interactive jupyter notebooks where python can be leveraged to perform these kinds of operations. ","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"The AoU cohort provides some smaller callsets derived from WGS data, for which both PLINK BED and BGEN files are available (termed srWGS callsets). We recommend using these with TarGene if you decide to run analyses using the AoU cohort. For more information about these callsets, see Short Read WGS Callsets. ","category":"page"},{"location":"targene/data_sources/","page":"Genetic and Traits Data","title":"Genetic and Traits Data","text":"Please be aware that the callset for ACAF-thresholded genetic data is extremely large and may result in longer runtimes. This is because TarGene is run in parallel using the Google Lifesciences API on the AoU Researcher Workbench. Since this is a cloud-based platform, each task is run individually on a runner Virtual Machine (VM), so all data relevant to a task must be copied onto that VM before the task can be executed. This results in a large amount of copying overhead, as symbolic links cannot be leveraged between tasks. ","category":"page"},{"location":"examples/interactions/#Interaction-Study","page":"Interaction Study","title":"Interaction Study","text":"","category":"section"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"In TarGene you can investigate the interacting effects between genetic variants and between genetic variants and the environment up to any order (GxG, GxE, GxGxG, ...). However, the computational complexity associated with interaction testing grows exponentially. Interaction studies are thus very often targeted, for instance towards a specific transcription factor TF. For this transcription factor, multiple variants may have been identified because they are believed to play different roles. In TarGene such a scenario can be encoded via the group mode of the ESTIMANDS_CONFIG file. An example is more informative than a long description. Let's save the following content to a \"newinteractionconfig.yaml\" file.","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"type: groups\n\nestimands:\n  - type: AIE\n    orders: [2, 3]\n\nvariants:\n  TF:\n    variants_1:\n      - \"1:238411180:T:C\"\n      - \"3:3502414:T:C\"\n    variants_2:\n      - \"2:14983:G:A\"\n\nextra_treatments:\n  - \"Cheese intake\"\n\noutcome_extra_covariates:\n  - \"Number of vehicles in household\"","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"The estimands section describes the types of estimands we are interested in, here Average Interaction Effects (AIE). We would like to scan both orders 2 and 3.","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"The variants section contains groups of variants for which the interactions will be generated independently. Here there is only one group called TF, and it contains two subgroups: variants_1 and variants_2. This means that combinations of variants_1 and variants_2 will define the interactions under investigation. Here they are simply (1:238411180:T:C, 2:14983:G:A) and (3:3502414:T:C, 2:14983:G:A). Why did we set the interaction orders to be both 2 and 3 since only pairwise interactions can be generated here?","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"This is because the extra_treatments section can list additional environmental variables, thus increasing the potential interaction order. In this case, there is one: Cheese intake. This means that additional pairwise interactions will be generated: (1:238411180:T:C, Cheese intake), (2:14983:G:A, Cheese intake) and (3:3502414:T:C, Cheese intake). But also the following order 3 interactions: (1:238411180:T:C, 2:14983:G:A, Cheese intake) and (3:3502414:T:C, 2:14983:G:A, Cheese intake).","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"note: Note on variant IDs\nThe variant IDs must match the IDs of your BGEN files. Here they are identified by chr:pos:ref:alt as but in your case it may be via the rsID.","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"Finally, the outcome_extra_covariates describes additional variables predictive of the outcome, but that do not confound the effect of treatment variables (variants and extra_treatments).","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"Importantly, all traits variables in the TRAITS_DATASET that are not listed as either outcome_extra_covariates or extra_treatments will be considered as outcome variables.","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"We will now point to this newly created ESTIMANDS_CONFIG in our nextflow.config file:","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"params {\n    ESTIMANDS_CONFIG = \"new_interaction_config.yaml\"\n\n    // UK-Biobank specific parameters\n    BED_FILES = \"unphased_bed/ukb_chr{1,2,3}.{bed,bim,fam}\"\n    BGEN_FILES = \"unphased_bgen/ukb_chr{1,2,3}.{bgen,bgen.bgi,sample}\"\n    UKB_CONFIG = \"ukbconfig_small.yaml\"\n    TRAITS_DATASET = \"dataset.csv\"\n    UKB_WITHDRAWAL_LIST = \"withdrawal_list.txt\"\n}","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"As usual, the pipeline can then be run as follows:","category":"page"},{"location":"examples/interactions/","page":"Interaction Study","title":"Interaction Study","text":"nextflow run https://github.com/TARGENE/targene-pipeline -r v0.11.1 -profile local","category":"page"},{"location":"simulations/simulation_outputs/#Description-of-Simulations-Outputs","page":"Description of Simulations Outputs","title":"Description of Simulations Outputs","text":"","category":"section"},{"location":"simulations/simulation_outputs/","page":"Description of Simulations Outputs","title":"Description of Simulations Outputs","text":"The output of the simulation workflows is a HDF5 file containing a DataFrame which can be loaded within a Julia session for instance. Each row of this DataFrame corresponds to a simulation task, that is a triple (estimand, estimator, sample size). Some of the elements within this DataFrame are objects that are defined within the TMLE.jl package and it is recommended to use it for downstream analysis. In particular, if it is not imported before loading the results, the objects will have to be reconstructed by the HDF5 library. The DataFrame contains the following columns:","category":"page"},{"location":"simulations/simulation_outputs/","page":"Description of Simulations Outputs","title":"Description of Simulations Outputs","text":"ESTIMATOR: The task's estimator\nESTIMAND: The task's estimand\nSAMPLE_SIZE: The task's sample size\nESTIMATES: The list of bootstrap estimates, each element is a TMLE.jl structure.\nN_FAILED: The number of failed bootstrap samples.\nOUTCOME: The estimand's outcome.\nTRUE_EFFECT: The true genetic effect for the estimand.\nMEAN_COVERAGE: The mean coverage of the estimator for this task.\nMEAN_BIAS: The mean bias of the estimator for this task.\nMEAN_VARIANCE: The mean variance of the estimator for this task.","category":"page"},{"location":"simulations/realistic_simulation/#Realistic-Simulation","page":"Realistic Simulation","title":"Realistic Simulation","text":"","category":"section"},{"location":"simulations/realistic_simulation/#Motivation","page":"Realistic Simulation","title":"Motivation","text":"","category":"section"},{"location":"simulations/realistic_simulation/","page":"Realistic Simulation","title":"Realistic Simulation","text":"In order to understand how well semi-parametric estimators will perform in real-world scenarios, we aim to simulate new data which is as close as possible to the original dataset. For each estimand of interest, we assume a causal graph similar to that of the following figure.","category":"page"},{"location":"simulations/realistic_simulation/","page":"Realistic Simulation","title":"Realistic Simulation","text":"(Image: Realistic Simulation)","category":"page"},{"location":"simulations/realistic_simulation/","page":"Realistic Simulation","title":"Realistic Simulation","text":"Each estimand is thus associated with a natural generating process, or equivalently, set of densities. For example, the interaction of (V_1 V_2) on Y requires three density estimates, namely: hatP_n(V_1PCs), hatP_n(V_2PCs) and hatP_n(YV_1 V_2 PCs C). Similarly, the single variant effect of V_1 on Y requires: hatP_n(V_1PCs) and hatP_n(YV_1 PCs C). Once these conditional densities have been estimated, new data can be generated via ancestral sampling. Implicitly, the empirical distribution hatP_n(PCs C) is always used to sample from the root nodes.","category":"page"},{"location":"simulations/realistic_simulation/","page":"Realistic Simulation","title":"Realistic Simulation","text":"For the simulation to be realistic, the conditional density estimators must be able to capture the complexity of the data, which has two main implications. The first is that the Causal Model should include as many causal variables as possible to generate the corresponding children variables. The second is that the density estimators must be flexible and data adaptive to capture arbitrarily complex data generating processes.","category":"page"},{"location":"simulations/realistic_simulation/#Variable-Selection","page":"Realistic Simulation","title":"Variable Selection","text":"","category":"section"},{"location":"simulations/realistic_simulation/","page":"Realistic Simulation","title":"Realistic Simulation","text":"Ideally, we would include the set of all genetic variants within the model and let it decide which ones are important in a data driven way. This is the approach taken by REGENIE, and integrating this idea within TarGene is an interesting direction. At the present moment though we restrict the dimensionality of the problem and only consider a small subset of these putative causes. This is done using published GWAS results from the geneATLAS. Precisely, we sub-sample a maximum of GA_MAX_VARIANTS (default: 50) variants from all variants associated with the outcome of interest (GA_PVAL_THRESHOLD (default: 1e-5)). Furthermore, these variants must be at least GA_DISTANCE_THRESHOLD (default: 1000000) base pairs away from each other and have a minor allele frequency of at least GA_MAF_THRESHOLD (default: 0.01). With respect to the above figure, these sub-sampled variants are assumed to be contained within the C variables.","category":"page"},{"location":"simulations/realistic_simulation/#Density-Estimation","page":"Realistic Simulation","title":"Density Estimation","text":"","category":"section"},{"location":"simulations/realistic_simulation/","page":"Realistic Simulation","title":"Realistic Simulation","text":"The second requirement for the simulation to be realistic is that the density estimators should capture complex patterns, which means the model class must be large. Neural networks, have been shown to be able to approximate a large class of function and scale seamlessly to large datasets. We thus used two types of neural networks depending on the type of the density's outcome variable. For categorical variables, including binary outcomes, a multi-layer perceptron is used, while for continuous variables we used a mixture density network. In all cases, in order to prevent over-fitting, density estimators are trained as sieve estimators. That is, the size of the model is chosen data adaptively by sequentially increasing the model capacity and early-stopping based on cross-validation performance.","category":"page"},{"location":"simulations/realistic_simulation/#Running-the-Workflow","page":"Realistic Simulation","title":"Running the Workflow","text":"","category":"section"},{"location":"simulations/realistic_simulation/","page":"Realistic Simulation","title":"Realistic Simulation","text":"To run the null simulation, the REALISTIC_SIMULATION entry should be added to the Nextflow command-line as follows","category":"page"},{"location":"simulations/realistic_simulation/","page":"Realistic Simulation","title":"Realistic Simulation","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r v0.11.1 -entry REALISTIC_SIMULATION","category":"page"},{"location":"simulations/realistic_simulation/#Output","page":"Realistic Simulation","title":"Output","text":"","category":"section"},{"location":"simulations/realistic_simulation/","page":"Realistic Simulation","title":"Realistic Simulation","text":"The output is a \"realistic_simulation_results.hdf5\" file (see Description of Simulations Outputs).","category":"page"},{"location":"runtime_considerations/#Some-Runtime-considerations","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Targeted Learning can quickly become computationally intensive compared to traditional parametric inference. Here, we describe the drivers of complexity, explain the tricks we use to keep it under control and illustrate with two typical study designs.","category":"page"},{"location":"runtime_considerations/#The-Drivers-of-Complexity","page":"Some Runtime considerations","title":"The Drivers of Complexity","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Remember that for each estimand of interest, a Targeted Learning estimator requires 3 ingredients:","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"An estimator for the propensity score: G = P(TW).\nAn estimator for the outcome's mean: Q_Y = mathbbEYT W.\nA targeting (or debiasing) step.","category":"page"},{"location":"runtime_considerations/#Complexity-Driven-by-the-Targeting-Step","page":"Some Runtime considerations","title":"Complexity Driven by the Targeting Step","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"The targeting step is the essential ingredient that makes the estimators in TarGene \"work\". However, all semi-parametric estimators work slightly differently and are thus not computationally equal. For instance the targeting step of the Targeted Minimum-Loss Estimator (TMLE) requires a generalised linear model fit while the One-Step Estimator (OSE) only requires a simple computation of the bias. The benefit is that, unlike the OSE, the TMLE is guaranteed to always respect the natural bounds of the genetic effect (think probabilities between 0 and 1).","category":"page"},{"location":"runtime_considerations/#Complexity-Driven-by-ML-Models","page":"Some Runtime considerations","title":"Complexity Driven by ML Models","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"The more complex the machine-learning models, the longer the runtime. For instance using a generalized linear model for both G and Q_Y would only roughly double the runtime of a regular estimation strategy based on linear models. In contrast, a Super Learning strategy relies on cross-validation and multiple models fits, and is thus much more expensive. However, because the G model does not involve the outcome Y, it can be efficiently reused hence reducing computational burden if many outcomes are of interest. This is particularly interesting in the PheWAS setting where a single G model can be used throughout for all estimands.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Similarly, but to a smaller extent, the above targeting step is specific to the variant's genotype change under investigation (e.g., CC rightarrow CT). Most of the time, multiple genotype changes are of interest (e.g., CC rightarrow CT and CT rightarrow TT) and both Q_Y and G can be reused across these changes.","category":"page"},{"location":"runtime_considerations/#Complexity-Driven-by-the-Estimators","page":"Some Runtime considerations","title":"Complexity Driven by the Estimators","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"All estimators exist in both a canonical and cross-validated version. The latter relaxes some non-parametric conditions under which the estimators will be asymptotically unbiased. However, it comes at a cost since it requires splitting the data into K folds and fitting the G and Q_Y models K times instead of once. Furthermore, the G model cannot be reused anymore because this outer cross-validation scheme is typically stratified by the outcome as well.","category":"page"},{"location":"runtime_considerations/#Controlling-Runtime-via-Parallelisation","page":"Some Runtime considerations","title":"Controlling Runtime via Parallelisation","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"If you are running TarGene on a high performance computing platform, you have access to many nodes that can be leveraged. For that purpose we use batching, that is we split all the run's estimands in \"cleverly\" organised batches to maximize caching of machine learning models. This is controlled by the BATCH_SIZE (default: 50) parameter which can be adjusted based on your specific study and platform.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"tip: Estimation Resources\nThe estimation is likely going to be the rate limiting step of your run. Each estimation process can further be adjusted in your config file based on your specific problem. For example as follows to retry with more memory as follows. process{\n    withName: TMLE {\n        memory = { 10.GB * task.attempt  }\n        time = { 48.hour }\n        cpus = 1\n    }\n}","category":"page"},{"location":"runtime_considerations/#Examples-Through-Study-Designs","page":"Some Runtime considerations","title":"Examples Through Study Designs","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"The numbers provided below were obtained with TarGene v0.9.0 and better performance is expected in the future.","category":"page"},{"location":"runtime_considerations/#The-PheWAS-study-design","page":"Some Runtime considerations","title":"The PheWAS study design","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"In a PheWAS, one is interested in the effect of a genetic variation across many outcomes (typically around 1000). Because the treatment variable is always the same, the propensity score G can be reused across all parameters, which drastically reduces computational complexity.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"With this setup in mind, the computational complexity is mostly driven by the specification of the learning algorithms for Q_Y, which will have to be fitted for each outcome. In the table below are presented some runtimes for various specifications of G and Q_Y using a single cpu. The \"Unit runtime\" is the average runtime across all estimands averaged over 10 traits and can roughly be extrapolated to bigger studies.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Estimator Unit runtime (s) Extrapolated runtime to 1000 outcomes\nglm 4.65 ≈ 1h20\nglmnet 7.19 ≈ 2h\nG-superlearning-Q-glmnet 50.05 ≈ 13h45\nsuperlearning 168.98 ≈ 46h","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Depending on the available resources, this means one can probably afford to use more expensive ML models. This is because the above does not leverage any sort of parallelisation.","category":"page"},{"location":"runtime_considerations/#The-GWAS-study-design","page":"Some Runtime considerations","title":"The GWAS study design","text":"","category":"section"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"In a GWAS, the outcome variable is held fixed and we are interested in the effects of very many genetic variations on this outcome (typically 800 000 for a genotyping array). The propensity score cannot be reused across parameters resulting in a more expensive run.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"In this case we look at 3 different genetic variations and only one outcome. In the table below are presented some runtimes for various specifications of G and Q_Y using a single cpu. The \"Unit runtime\" is the average runtime across all estimands and can roughly be extrapolated to bigger studies.","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Estimator file Continuous outcome unit runtime (s) Binary outcome unit runtime (s) Projected Time on HPC (200 folds //)\nglm 5.64 6.14 ≈ 6h30\nglmnet 17.46 22.24 ≈ 22h\nG-superlearning-Q-glmnet 430.54 438.67 ≈ 20 days\nsuperlearning 511.26 567.72 ≈ 24 days","category":"page"},{"location":"runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"We can see that modern high performance computing platforms definitely enable this study design when using GLMs or GLMNets. It is unlikely however, that you will be able to use Super Learning for any of G or Q_Y if you don't have privileged access to such platforms.","category":"page"},{"location":"targene/miscellaneous/#Final-Tweaks","page":"Final Tweaks","title":"Final Tweaks","text":"","category":"section"},{"location":"targene/miscellaneous/","page":"Final Tweaks","title":"Final Tweaks","text":"Further Nextflow parameter affecting the behaviour of the pipeline but that does not fit in any previously described category is listed here:","category":"page"},{"location":"targene/miscellaneous/","page":"Final Tweaks","title":"Final Tweaks","text":"BATCH_SIZE (optional, default: 50): The set of estimands to be estimated is batched and the TMLE processes will run in parallel across batches on your platform.\nKEEP_IC (optional, default: false): For all estimands with a p-value below PVAL_THRESHOLD, the influence curve is saved.\nOUTDIR (optional, default: \"results\"): Output directory","category":"page"},{"location":"examples/allofus/#All-of-Us","page":"All of Us","title":"All of Us","text":"","category":"section"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"Analyses within the All of Us (AoU) Researcher Workbench using genetic data must be run within the Controlled Tier Access (See Data Access). Workspaces launched within this tier will automatically have nextflow installed and can use TarGene immediately. Each project will be assigned a bucket for storage on Google Cloud (this can be viewed ), as well as all relevant nextflow parameters required to submit jobs using the Google Lifesciences API. Your workspace-specific nextflow profile will be found in ~/.nextflow/config and will be automatically available to you by using the flag -profile gls when you run nextflow. The AoU Researcher workbench requires some additional specifications that are built into TarGene in the allofus profile, and can be combined with your Workspace-specific gls configuration when running TarGene.","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"We reccommend running this by first entering a Cloud Analysis Terminal on your current Workspace, creating a configuration for the analysis you would like to run, and running TarGene in a screen session. See Workflows in the All of Us Researched Workbench for more information.","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"A minimalist run configuration to run a flat config run on the AoU Researcher Workbench using TarGene looks like the following:","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"params {\n    COHORT = \"ALLOFUS\"\n    ESTIMANDS_CONFIG = \"allofus_config.yaml\"\n\n    // UK-Biobank specific parameters\n    BGEN_FILE = \"gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/clinvar_v7.1/bgen/clinvar.chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bgen,sample,bgen.bgi}\"\n    BED_FILES = \"gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/clinvar_v7.1/plink_bed/clinvar.chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bed,bim,fam}\"\n    TRAITS_DATASET = \"allofus_traits.csv\"\n}","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"Apart from the data related parameters, there are two main parameters here: ESTIMANDS_CONFIG and the ESTIMATORS_CONFIG. These parameters describe the estimands (questions of interest) and how to estimate them respectively. ","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"The ESTIMANDS_CONFIG here follows the same format as the flat configuration detailed in the PheWas section. Here we are estimating the ATE of the FTO variant (encoded as chr16:53767042:T:C in the BGEN data for the AoU cohort) on the traits present in our allofus_traits.csv file. We have also added sex_at_birth as a covariate. ","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"type: flat\n\nestimands:\n  - type: ATE\n\nvariants:\n  - chr16:53767042:T:C\n\noutcome_extra_covariates:\n  - \"sex_at_birth\"","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"The optional outcome_extra_covariates are variables to be used as extra predictors of the outcome (but not as confounders). This information must be contained in the allofus_traits.csv file, along with your outcomes-of-interest. ","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"The allofus_traits.csv might look as follows:","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"SAMPLE_ID sexatbirth Height (cm)\n100000 Male 180\n100002 Female 165\n100004 Male 175\n100010 Female 160\n... ... ...","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"Here we have not specified any value for ESTIMATORS_CONFIG, and so the default, ESTIMATORS_CONFIG = \"wtmle-ose--tunedxgboost\", will be used. This defines the estimation strategy, and more specifically, that we will be using Targeted Minimum-Loss Estimator as well as a One Step Estimator with a tuned XGBoost model to learn the outcome models (Q_Y) and propensity scores (G).","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"The TarGene can then be run on the AoU Researcher Workbench as follows:","category":"page"},{"location":"examples/allofus/","page":"All of Us","title":"All of Us","text":"nextflow run https://github.com/TARGENE/targene-pipeline -r v0.11.1 -profile gls,allofus","category":"page"},{"location":"secondary_workflows/make_dataset/#The-Make-Dataset-Workflow","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"","category":"section"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"This workflow extracts an aggregated dataset containing traits (TRAITS_DATASET), genetic confounders (NB_PCS) and genetic variants (VARIANTS_LIST) in an Arrow tabular format.","category":"page"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"(Image: Make Dataset)","category":"page"},{"location":"secondary_workflows/make_dataset/#Example-Run-Command","page":"The Make Dataset Workflow","title":"Example Run Command","text":"","category":"section"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TAG -entry MAKE_DATASET -profile P -resume","category":"page"},{"location":"secondary_workflows/make_dataset/#List-Of-Workflow-Arguments","page":"The Make Dataset Workflow","title":"List Of Workflow Arguments","text":"","category":"section"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"VARIANTS_LIST (required): A text file (one rsid per line) specifying the variants of interest.\nBGEN_FILES (required): Path to imputed BGEN files from which the variants in VARIANTS_LIST will be extracted.\nNB_PCS (optional, default: 6): The number of PCA components to extract.\nBED_FILES (required): Path expression to PLINK BED files.\nCOHORT (optional: \"UKB\"): Current default for this is UKB. If set to a value other than UKB, this will not run UKB-specific trait extraction.\nTRAITS_DATASET (required): Path to a traits dataset. If you are running this for a non-UKB cohort, your sample IDs must be specified in the first column of this CSV file, with the column name SAMPLE_ID.\nFLASHPCA_EXCLUSION_REGIONS (optional, default: assets/exclusionregionshg19.txt): A path to the flashpca special exclusion regions.\nMAF_THRESHOLD (optional, default: 0.01): Only variants with that minor allele frequency are considered\nLD_BLOCKS (optional): A path to pre-identified linkage disequlibrium blocks to be removed from the BED files. It is good practice to specify LD_BLOCKS, as it will remove SNPs correlated with your variants-of-interest before running PCA.","category":"page"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"If the COHORT argument is set to UKB:","category":"page"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"UKB_CONFIG (required): YAML configuration file describing which traits should be extracted and how the population should be subsetted.\nUKB_ENCODING_FILE (optional): If the TRAITS_DATASET is encrypted, an encoding file must be provided.\nUKB_WITHDRAWAL_LIST (optional): List of participants withdrawn from the study.\nQC_FILE (optional): Genotyping quality control file from the UK-Biobank study.","category":"page"},{"location":"targene/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"targene/overview/#General-Workflow-Structure","page":"Overview","title":"General Workflow Structure","text":"","category":"section"},{"location":"targene/overview/","page":"Overview","title":"Overview","text":"This is the main workflow within TarGene, its purpose is to estimate a wide variety of genetic effects using the Targeted Learning framework. This is an end-to-end workflow, meaning that you don't need to perform any QC on your genotypes files. The workflow can be roughly decomposed into two main steps:","category":"page"},{"location":"targene/overview/","page":"Overview","title":"Overview","text":"In the first step, an integrated tabular dataset is built, it contains\nPhenotypes: Potentially extracted from the UK Biobank\nVariants of Interest: Extracted from genotyping data.\nPCs: Constructed from genotyping data using standard methodology (A LOCO approach is used for GWAS)\nIn the second step, all genetic effects are estimated via Targeted Learning in parallel using the estimators of your choice.","category":"page"},{"location":"targene/overview/","page":"Overview","title":"Overview","text":"An overview of the workflow is presented in the following diagram.","category":"page"},{"location":"targene/overview/","page":"Overview","title":"Overview","text":"(Image: TarGene Workflow High Level)","category":"page"},{"location":"targene/overview/#Example-Run-Command","page":"Overview","title":"Example Run Command","text":"","category":"section"},{"location":"targene/overview/","page":"Overview","title":"Overview","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r v0.11.0 -profile local -resume","category":"page"},{"location":"targene/overview/","page":"Overview","title":"Overview","text":"We now describe step by step how to setup a TarGene run configuration.","category":"page"},{"location":"secondary_workflows/pca/#The-PCA-Workflow","page":"The PCA Workflow","title":"The PCA Workflow","text":"","category":"section"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"This workflow computes the number of specified principal components (NB_PCS) using flashpca from a set of genetic files in BED format (BED_FILES) and a trait dataset specifying the population of interest (TRAITS_DATASET).","category":"page"},{"location":"secondary_workflows/pca/#Example-Run-Command","page":"The PCA Workflow","title":"Example Run Command","text":"","category":"section"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TAG -entry PCA -profile P -resume","category":"page"},{"location":"secondary_workflows/pca/#List-Of-Workflow-Arguments","page":"The PCA Workflow","title":"List Of Workflow Arguments","text":"","category":"section"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"NB_PCS (optional, default: 6): The number of PCA components to extract.\nBED_FILES (required): Path expression to PLINK BED files.\nCOHORT (optional: \"UKB\"): Current default for this is UKB. If set to a value other than UKB, this will not run UKB-specific trait extraction.\nTRAITS_DATASET (required): Path to a traits dataset. If you are running this for a non-UKB cohort, your sample IDs must be specified in the first column of this CSV file, with the column name SAMPLE_ID.\nFLASHPCA_EXCLUSION_REGIONS (optional, default: assets/exclusionregionshg19.txt): A path to the flashpca special exclusion regions.\nMAF_THRESHOLD (optional, default: 0.01): Only variants with that minor allele frequency are considered\nLD_BLOCKS (optional): A path to pre-identified linkage disequlibrium blocks to be removed from the BED files. It is good practice to specify LD_BLOCKS, as it will remove SNPs correlated with your variants-of-interest before running PCA.","category":"page"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"If the COHORT argument is set to UKB:","category":"page"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"UKB_CONFIG (required): YAML configuration file describing which traits should be extracted and how the population should be subsetted.\nUKB_ENCODING_FILE (optional): If the TRAITS_DATASET is encrypted, an encoding file must be provided.\nUKB_WITHDRAWAL_LIST (optional): List of participants withdrawn from the study.\nQC_FILE (optional): Genotyping quality control file from the UK-Biobank study.","category":"page"},{"location":"targene/confounding_adjustment/#PCA-Adjustment","page":"PCA Adjustment","title":"PCA Adjustment","text":"","category":"section"},{"location":"targene/confounding_adjustment/","page":"PCA Adjustment","title":"PCA Adjustment","text":"To account for potential confounding effect due to population stratification, we extract principal components from the genetic data using flashpca. We follow the recommended procedure for this tool which implies some preprocessing and filtering.","category":"page"},{"location":"targene/confounding_adjustment/","page":"PCA Adjustment","title":"PCA Adjustment","text":"In principle, you shouldn't need to change these parameters apart from the number of principal components (NB_PCS).","category":"page"},{"location":"targene/confounding_adjustment/","page":"PCA Adjustment","title":"PCA Adjustment","text":"NB_PCS (optional, default: 6): The number of PCA components to extract.\nLD_BLOCKS (optional): A path to pre-identified linkage disequilibrium blocks around the variants that will be queried for causal effect estimation. Those LD blocks will be removed from the data used for PCA.\nFLASHPCA_EXCLUSION_REGIONS (required, default: assets/exclusionregionshg19.txt): A path to the flashpca special exclusion regions which is provided in their repository.\nMAF_THRESHOLD (optional, default: 0.01): Only variants with that minor allele frequency are used to compute principal components.","category":"page"},{"location":"developer_guide/contribution_guide/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Contributions, whether bug fixes, new features or documentation improvements are very welcome!","category":"page"},{"location":"developer_guide/contribution_guide/#Raise-an-issue","page":"Contributing","title":"Raise an issue","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"In order to discuss and track the evolution of the project, please first raise an issue on the targene-pipeline repository. If a change is agreed upon, the discussion should identify the relevant repositories that are concerned by the change and open an issue on each of the repository. For instance, if one wishes to improve the extraction of traits from the UK-Biobank, the UKBMain.jl would surely be impacted and a new release for that package necessary.","category":"page"},{"location":"developer_guide/contribution_guide/#Suggested-Development-Workflow","page":"Contributing","title":"Suggested Development Workflow","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Following our previous UKBMain.jl example, there are two repositories that need to be updated, the current workflow is as follows:","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Develop\nUKBMain.jl\nCreate a new git branch for your change\nDevelop and test\nRelease an image for your branch by selecting it after clicking the Run workflow button. If the tests pass, a new docker image will be generated and hosted on Docker hub with your branch's name (see Docker Images).\ntargene-pipeline\nCreate a new git branch for your change\nFor each Nextflow process using the UKBMain.jl's docker image, update to the branch's image name.\nDevelop further required changes and run/add the tests (see Workflows Tests).\nReview: When everything is working, ask for a review\nRelease UKBMain.jl:\nMerge your branch into main\nCreate a new Github release following semantic versioning, this will create a new docker image with your release name.\nRelease TarGene\nFor each Nextflow process using the UKBMain.jl's docker image, update to the released image name (as before).\nCreate a new Github release following semantic versioning","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"(Image: CI/CD)","category":"page"},{"location":"developer_guide/contribution_guide/#Docker-Images","page":"Contributing","title":"Docker Images","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Currently, all TarGene building blocks (executables) are provided as docker images. The following table provides a map linking each TarGene repository to the associated Docker image tags.","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Repository Docker tag\nTargeneCore.jl tl-core\nUKBMain.jl ukbmain\nTMLECLI.jl targeted-estimation\nSimulations.jl targene-simulations","category":"page"},{"location":"developer_guide/contribution_guide/#Workflows-Tests","page":"Contributing","title":"Workflows Tests","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"The pipeline is automatically tested for every push/pull-request against a variety of tasks listed in the testrun section of the CI YAML file. To add a new test, you thus need to add a testrun in this section and a corresponding Julia file in the test folder.","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"In can also be useful to run the tests locally for debugging. This can be done if you have Nextflow, Julia and Singularity installed and requires a Linux machine. If you have instantiated the test project, a test can be fully run like so:","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"julia --project=test --startup-file=no test/TESTFILE -profile local -resume","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Alternatively, within the Julia REPL you can manually run the code sequentially.","category":"page"},{"location":"developer_guide/contribution_guide/#Working-on-the-Documentation","page":"Contributing","title":"Working on the Documentation","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"The documentation is built and deployed using Documenter.jl. It only relies on Markdown files which can simply be edited.","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"However, it may be useful to see how your changes will be reflected within the documentation as you are working in real time. For that, you can use the following workflow (steps 1 and 2 are done only once):","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Install Julia\nFrom the Julia REPL, install in the main Julia environment the LiveServer package: add LiveServer.\nIn the project directory (targene-pipeline) run: julia --startup-file=no --project=docs -e'using LiveServer; servedocs()'","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"The documentation should be accessible in your browser at http://localhost:8000/.","category":"page"},{"location":"targene/sieve_variance/#Correcting-for-population-relatedness-(Experimental)","page":"Correcting for population relatedness (Experimental)","title":"Correcting for population relatedness (Experimental)","text":"","category":"section"},{"location":"targene/sieve_variance/","page":"Correcting for population relatedness (Experimental)","title":"Correcting for population relatedness (Experimental)","text":"If the i.i.d. (independent and identically distributed) hypothesis is not satisfied, most of the traditional statistical inference theory falls apart. This is typically possible in population genetics where a study may contain related individuals. Here we leverage a non-parametric method called Sieve Variance Plateau (SVP) estimation. The hypothesis is that the dependence between individuals is sufficiently small, so that our targeted estimator will still be asymptotically unbiased, but its variance will be under estimated. In brief, the SVP estimator computes a variance estimate for a range of thresholds tau, by considering individuals to be genetically independent if their genetic distance exceeds tau. The genetic distance between a pair of individuals (i j) equals 1  GRM_ij , i.e., one minus their genetic relatedness value. As the distance threshold tau increases, fewer individuals are assumed to be genetically independent. For instance, the estimate corresponding to a distance of tau = 0 corresponds to the i.i.d. hypothesis, while a distance of tau = 1 incorporates pairs of individuals who are not genetically correlated. TarGene varies the threshold tau from 0 to 1 and fits a curve to the corresponding variance estimates. The maximum of this curve is the most conservative estimate of the variance estimator and constitutes the corrected variance estimator. To illustrate, an example of curve is presented below.","category":"page"},{"location":"targene/sieve_variance/","page":"Correcting for population relatedness (Experimental)","title":"Correcting for population relatedness (Experimental)","text":"(Image: SVP)","category":"page"},{"location":"targene/sieve_variance/","page":"Correcting for population relatedness (Experimental)","title":"Correcting for population relatedness (Experimental)","text":"The following arguments can be changed to control the behaviour of the pipeline:","category":"page"},{"location":"targene/sieve_variance/","page":"Correcting for population relatedness (Experimental)","title":"Correcting for population relatedness (Experimental)","text":"SVP (default: false): Must be set to true to enable variance adjustement.\nESTIMATOR_KEY (default: 1): Either the name of an estimator from ESTIMATORS_CONFIG or an integer, indicating which estimator's results will be used for adjustment.\nGRM_NSPLITS (default: 100): This is a purely computational argument. The GRM is typically very large and splitting enables a good memory/parallelization tradeoff.\nMAX_SVP_THRESHOLD (default: 0.8): Controls the maximum genetic distance considered.\nNB_SVP_ESTIMATORS (default: 100): Controls the number of points in the interval [0, MAX_SVP_THRESHOLD]. If 0, the Sieve Variance Plateau method will not be applied.\nPVAL_THRESHOLD (default: 0.05): Only estimates with a p-value lower than PVAL_THRESHOLD will be considered for SVP correction. This is because SVP will only increase the variance of the estimator.","category":"page"},{"location":"targene/sieve_variance/","page":"Correcting for population relatedness (Experimental)","title":"Correcting for population relatedness (Experimental)","text":"tip: SVP Resources\nThe sieve variance plateau estimation will use memory mapping to avoid using too much RAM. On some platforms this is not allowed and will lead to massive memory consumption. With the UK Biobank this is around 500GB. The process also mostly revolves around matrix multiplication so that multithreading can improve the performance. With Nextflow you can control resources at the process level, for instance you could add the following to your config file.process{\n    withName: SVP {\n        memory = { 500.GB  }\n        cpus = 10\n    }\n}","category":"page"},{"location":"simulations/null_simulation/#Null-Simulation","page":"Null Simulation","title":"Null Simulation","text":"","category":"section"},{"location":"simulations/null_simulation/#Motivation","page":"Null Simulation","title":"Motivation","text":"","category":"section"},{"location":"simulations/null_simulation/","page":"Null Simulation","title":"Null Simulation","text":"The goal of the Null Generating Process is to result in the theoretical null hypothesis of \"no effect\". The main rationale behind this simulation is that most of the genome is still believed to be irrelevant. It is thus of particular importance that the proposed semi-parametric estimators are not too sensitive to noise in order to control the false discovery rate. Since we are interested in the effects of genetic variants on traits, the data generating process must satisfy:  j Y   V_j, where Y is a given trait. In practice, we enforce an even stronger condition, where all variables are pairwise independent. This is done by drawing independently n samples with replacement from the empirical marginal distribution of each variable. The only exception is that (PCs C) are sampled jointly. This generating process hence preserves many characteristics of the original dataset, while resulting in the Null hypothesis.","category":"page"},{"location":"simulations/null_simulation/","page":"Null Simulation","title":"Null Simulation","text":"(Image: Null Simulation)","category":"page"},{"location":"simulations/null_simulation/#Running-the-Workflow","page":"Null Simulation","title":"Running the Workflow","text":"","category":"section"},{"location":"simulations/null_simulation/","page":"Null Simulation","title":"Null Simulation","text":"To run the null simulation, the NULL_SIMULATION entry should be added to the Nextflow command-line as follows","category":"page"},{"location":"simulations/null_simulation/","page":"Null Simulation","title":"Null Simulation","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r v0.11.1 -entry NULL_SIMULATION","category":"page"},{"location":"simulations/null_simulation/#Output","page":"Null Simulation","title":"Output","text":"","category":"section"},{"location":"simulations/null_simulation/","page":"Null Simulation","title":"Null Simulation","text":"The output is a \"null_simulation_results.hdf5\" file (see Description of Simulations Outputs).","category":"page"},{"location":"overview/#Nextflow-Basics","page":"Nextflow Basics","title":"Nextflow Basics","text":"","category":"section"},{"location":"overview/#Running-the-Workflows","page":"Nextflow Basics","title":"Running the Workflows","text":"","category":"section"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"Since TarGene uses Nextflow, all workflows can be run in the same way from the command line:","category":"page"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TARGENE_VERSION -entry WORKFLOW_NAME -profile P -resume","category":"page"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"where:","category":"page"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"TARGENE_VERSION is the latest TarGene version, e.g. v0.11.1\nWORKFLOW_NAME is any of the TarGene workflows\nP is an optional Nextflow profile describing the computing platform (see Platform Configuration).","category":"page"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"Additional Nextflow command line arguments can be found in the official documentation, for example important options are:","category":"page"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"-resume: Tells Nextflow to try to resume the pipeline if an error occurred during the execution (if you forgot to specify a parameter for instance)\n-with-trace and -with-report will generate additional report files.","category":"page"},{"location":"overview/#Workflows-Configurations","page":"Nextflow Basics","title":"Workflows Configurations","text":"","category":"section"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"There are mainly two parts to configuring a workflow run. The first part describes the computing environment, it tells Nextflow how it should execute the various processes of the workflow. The second part provides the actual inputs to the TarGene workflows. When running the nextflow run command, Nextflow will look for a nextflow.config configuration file in your current directory. If you are new to Nextflow, you can use this file to setup both the platform and project configurations. As your project grows you may want to split them in distinct files. This is so that the platform configuration can be easily reused across many TarGene runs.","category":"page"},{"location":"overview/#Platform-Configuration","page":"Nextflow Basics","title":"Platform Configuration","text":"","category":"section"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"It is likely that you will run TarGene on a HPC platform, in particular the Executors and Singularity configurations are required. Since Nextflow is so widespread, it is probable that such a configuration file is already available from your HPC administrators. Since this configuration only describes de computing platform and not your project, it is often described as a Profile.","category":"page"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"tip: University of Edinburgh\nIf you are using the University of Edinburgh Eddie cluster, you can simply use TarGene with the -profile eddie option.","category":"page"},{"location":"overview/#Project-Configuration","page":"Nextflow Basics","title":"Project Configuration","text":"","category":"section"},{"location":"overview/","page":"Nextflow Basics","title":"Nextflow Basics","text":"These are the configuration details associated with your project, this is usually done in a nextflow.config file living at the root of your project's directory. The configuration parameters are specific to each workflow and described in the following sections.","category":"page"},{"location":"examples/phewas/#PheWAS","page":"PheWAS","title":"PheWAS","text":"","category":"section"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"A phenome-wide association study (PheWAS) is a study design whereby the effect of a variant, or set of variants is estimated for a large number of traits or outcomes. In TarGene, running a PheWAS is always done implicitely because the outcomes are defined as all variables in the TRAITS_DATASET that do not play a particular role as per the ESTIMANDS_FILE.","category":"page"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"To run a PheWAS for 3 genetic variants we will use the flat configuration mode. Let's create an ESTIMANDS_FILE called \"newphewasconfig.yaml\" with the following content.","category":"page"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"type: flat\n\nestimands:\n  - type: ATE\n\nvariants:\n  - 1:238411180:T:C\n  - 3:3502414:T:C\n  - 2:14983:G:A\n\noutcome_extra_covariates:\n  - \"Skin colour\"\n  - \"Cheese intake\"","category":"page"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"The estimands section is set to include Average Treatment Effects (ATE) which correspond to single variant effects. Because there are 3 variants in the variants section, 3 PheWAS will actually be run in paralllel. These will be run for all traits in the UKB_CONFIG that are not in the outcome_extra_covariates section.","category":"page"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"note: Note on variant IDs\nThe variant IDs must match the IDs of your BGEN files. Here they are identified by chr:pos:ref:alt but in your files it may be via the rsID. If this is the case, multi-allelic variants are currently not supported.","category":"page"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"The nextflow.config file for this study is:","category":"page"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"params {\n    ESTIMANDS_CONFIG = \"new_phewas_config.yaml\"\n\n    // UK-Biobank specific parameters\n    BED_FILES = \"unphased_bed/ukb_chr{1,2,3}.{bed,bim,fam}\"\n    BGEN_FILES = \"unphased_bgen/ukb_chr{1,2,3}.{bgen,bgen.bgi,sample}\"\n    UKB_CONFIG = \"ukbconfig_small.yaml\"\n    TRAITS_DATASET = \"dataset.csv\"\n    UKB_WITHDRAWAL_LIST = \"withdrawal_list.txt\"\n}","category":"page"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"warning: Genotype Files\nNote that we need to provide both BED_FILES and BGEN_FILES.","category":"page"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"And the command-line to be run:","category":"page"},{"location":"examples/phewas/","page":"PheWAS","title":"PheWAS","text":"nextflow run https://github.com/TARGENE/targene-pipeline -r v0.11.1 -profile local","category":"page"},{"location":"#TarGene","page":"Home","title":"TarGene","text":"","category":"section"},{"location":"#What-is-TarGene?","page":"Home","title":"What is TarGene?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TarGene is a reproducible and scalable Nexflow pipeline that estimates the effect of genetic variations on human traits via Targeted Learning.","category":"page"},{"location":"#TarGene-Estimates-the-Effect-of-Change","page":"Home","title":"TarGene Estimates the Effect of Change","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Genetic effects are defined as causal quantities measuring the effect of change. The following picture illustrates two distinct genotype changes, mathrmCC rightarrow mathrmCT and mathrmCT rightarrow mathrmTT, each corresponding to a different genetic effect. Note that there is no need for a parametric model to define the effect of such changes. A single variant effect is defined model independently by the Average Treatment Effect. For a single genotype change, it is for example given by","category":"page"},{"location":"","page":"Home","title":"Home","text":"beta_mathrmCC rightarrow mathrmCT = mathbbEY  do(CT) - mathbbEY  do(CC)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: \"Illustrated Causal Model\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Causal Interpretation\nThe fact that genetic effects can be interpreted as causal effects is entirely dependent on the validity of the causal model, which rely on two critical assumptionsNo unobserved confounders. In TarGene, confounding variables are currently inferred via Principal Component Analysis. While PCA could partly capture genetic ancestry, the problem of linkage disequilibrium remains.\nPositivity. This means that the genetic variants under investigation should have non-zero probability under all confounding variables values. In TarGene, we tackle this issue via a heuristic, which is similar to the usual minor allele frequency threshold employed in GWAS.","category":"page"},{"location":"#How-TarGene-Works?","page":"Home","title":"How TarGene Works?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Targeted Learning is a modern framework that combines advances in causal inference, machine-learning and statistical theory to answer impactful scientific questions. In population genetics, these questions are diverse: single variant effect, epistatic interactions, gene-environment interactions and more! In TarGene, we thus provide targeted estimators that leverage machine-learning algorithms to answer these questions, while preserving valid statistical inference.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Briefly, TarGene uses machine-learning models to estimate two predictive functions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"The outcome regression model: Q_Y = mathbbEY  V W.\nThe propensity score or variant model: G = P(V  W).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The problem is that naively using machine-learning models to estimate Q_Y, will lead to biased estimates for our genetic effects like beta_mathrmCC rightarrow mathrmCT. The estimators we use in TarGene use the propensity score to reduce this bias and yield a targeted hatbeta^*_mathrmCC rightarrow mathrmCT. The resulting estimators are asymptotically normal, unbiased and efficient (with minimum variance). We can thus use flexible modelling strategies and still obtain valid confidence regions and p-values.","category":"page"},{"location":"#What-TarGene-Can-Do","page":"Home","title":"What TarGene Can Do","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TarGene provides baked-in facilities for most popular study designs that aim at understanding the effect of genetic variations on human traits. Hopefully you can find what you are looking for in the following list of supported study-designs:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Genome-Wide Association Studies\nPhenome-Wide Association Studies\nFocused Study of:\nSingle or Joint variants effects\nGene-Gene interactions up to any order\nGene-Environment interactions up to any order","category":"page"},{"location":"#What-TarGene-Can't-Do-(Yet)","page":"Home","title":"What TarGene Can't Do (Yet)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Targeted Learning is extremly powerful because it provides taylored estimation strategies for each question of interest. Most of the questions that can be currently answered with statistical confidence are population level questions. That is, while TarGene uses predictive machine-learning models to estimate genetic effects, these predictive models are not the output of the software themselves. Furthermore, because each estimator is targeted, it means additional work must be done for each new question that comes up. For instance, one thing that TarGene can't do just yet is to estimate Heritability, but if you are interested we'd love to hear from you!","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are using TarGene on a High-Performance Computing platform, it is likely software dependencies are already available.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Since TarGene is a Nextflow pipeline, all you need is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Nextflow >= 24.04.4","category":"page"},{"location":"","page":"Home","title":"Home","text":"For reproducibility, TarGene uses containerization technologies. You will also need one of:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Singularity >= 3.8.6\nDocker >= 27.0.3","category":"page"},{"location":"","page":"Home","title":"Home","text":"Versions are indicative, any recent version should work.","category":"page"},{"location":"#Basic-Usage","page":"Home","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TarGene can be run from the command line through Nextflow:","category":"page"},{"location":"","page":"Home","title":"Home","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TARGENE_VERSION -c CONFIG_FILE -resume","category":"page"},{"location":"","page":"Home","title":"Home","text":"where:","category":"page"},{"location":"","page":"Home","title":"Home","text":"TARGENE_VERSION is the latest TarGene version, e.g. v0.11.0.\nCONFIG_FILE is a plain Nextflow configuration file describing what you want to do. Writing this configuration file is the hard work that this documentation is all about! However it needs not be scary, can could be as simple as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"params {\n    ESTIMANDS_CONFIG = \"path to estimands config\"\n    ESTIMATORS_CONFIG = \"value or path to estimator config\"\n    BED_FILES = \"path to bed files\"\n    ...\n}","category":"page"},{"location":"#Citing-TarGene","page":"Home","title":"Citing TarGene","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Labayle, O., Tetley-Campbell, K., Slaughter, J., Roskams-Hieter, B., Beentjes, S., Khamseh, A., & Ponting, C. TarGene [Computer software]. https://github.com/TARGENE/targene-pipeline","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dispensing with unnecessary assumptions in population genetics analysis. Olivier Labayle Pabet, Kelsey Tetley-Campbell, Mark J. van der Laan, Chris P. Ponting, Sjoerd Viktor Beentjes, Ava Khamseh. bioRxiv 2022.09.12.507656; doi: https://doi.org/10.1101/2022.09.12.507656","category":"page"},{"location":"#Getting-in-touch","page":"Home","title":"Getting in touch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please feel free to raise an issue if you face a problem or would benefit from a new feature. Contributions are most welcome.","category":"page"},{"location":"targene/study_designs/#Defining-the-Estimands-of-Interest","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"","category":"section"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"The ESTIMANDS_CONFIG describes the genetic effects that will be estimated within a TarGene run. In particular, it answers the two following questions:","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"What variants are of interest?\nWhat are the quantities of interest: Average Treatment Effects, Epistatic Interactions, Gene by Environment Interactions, ...?","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"In all cases, the ESTIMANDS_CONFIG is a plain YAML file.","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"While specifying your estimands, it may be useful to keep the following causal model in mind.","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"(Image: Causal Graph)","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"Where V_1V_p are a set of genetic variants, the Y_1Y_K are a set of traits and C are a set of additional predictors for Y but not confounding the genetic effects.","category":"page"},{"location":"targene/study_designs/#Preliminaries","page":"Defining the Estimands of Interest","title":"Preliminaries","text":"","category":"section"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"In what follows, the genetic effects are defined by the following abbreviations:","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"ATE: Average Treatment Effect (G)\nAIE: Average Interaction Effect (GxG, GxE, GxGxE, ...)\nCM: Counterfactual Mean (Unlikely to be of interest)","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"Also, depending on the study design, some of the YAML file sections are the same, instead of repeating them we list them here.","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"type: Defines the study design (see below).\nestimands: A list of the followings:\ntype: The type of generated estimands (CM, ATE, AIE)\norders: With respect to treatment variables if more than two are provided, the combination orders to be generated. For interactions, the order is always greater or equal than 2.\nvariants: The list of genetic variants of interest, see below for how this can be specified.\nextra_treatments: Environmental treatment variables that are added to the treatments combinations.\noutcome_extra_covariates: Additional covariates predictive of the outcomes (C in the causal model above).\nextra_confounders: Additional confounding variables other than Principal Components (W in the causal model above).","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"info: Outcomes\nNotice the absence of an outcomes section. These are defined by omission, that is, any variable within the trait dataset which is not in the outcome_extra_covariates or extra_confounders sections will be treated as an outcome.","category":"page"},{"location":"targene/study_designs/#Genome-Wide-Association-Study-(GWAS)","page":"Defining the Estimands of Interest","title":"Genome-Wide Association Study (GWAS)","text":"","category":"section"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"This is the most popular study design in population genetics and a traditional Leave One Chromosome Out (LOCO) strategy is used to build principal components. To run a GWAS, simply provide an ESTIMANDS_CONFIG as follows:","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"type: gwas\n\noutcome_extra_covariates:\n  - Age At Assessment","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"The outcome_extra_covariates section lists additional variables that are predictive of the outcomes in the traits dataset.","category":"page"},{"location":"targene/study_designs/#Flat-Configuration","page":"Defining the Estimands of Interest","title":"Flat Configuration","text":"","category":"section"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"In this mode, genetic variants are provided as a flat list, for each estimand type and order specified in the estimands section, the estimands are generated using combinations.","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"For example, with the following file:","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"type: flat\n\nestimands:\n  - type: AIE\n    orders: [2, 3]\n  - type: ATE\n\nvariants:\n  - RSID_17\n  - RSID_99\n  - RSID_102\n\nextra_treatments:\n  - TREAT_1\n\noutcome_extra_covariates:\n  - COV_1\n\nextra_confounders:\n  - 21003\n  - 22001","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"Average Treatment Effects of order 1 (default) for all (RSID17, RSID99, RSID102, TREAT1) are generated\nInteraction Effects of order 2 and 3 for all combinations of (RSID17, RSID99, RSID102, TREAT1) are generated. Some of these combinations are (RSID17, RSID99), (RSID102, TREAT1) or (RSID102, RSID99, TREAT_1).","category":"page"},{"location":"targene/study_designs/#Grouped-Configuration","page":"Defining the Estimands of Interest","title":"Grouped Configuration","text":"","category":"section"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"When the number of variants becomes large, estimating all combinations becomes both computationally expensive and reduces power due to the associated multiple testing burden. If the genetic variants are not chosen at random but based on some biological mechanism (e.g. a transcription factor), it is more efficient to group them. Within each group further subgroups can be defined for the roles played by each variant, only combinations across groups are considered.","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"For example, the following file describes two groups each consisting of two subgroups:","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"type: groups\nestimands:\n  - type: AIE\n    orders: [2, 3]\nvariants:\n  TF1:\n    bQTLs:\n      - RSID_17\n      - RSID_99\n    eQTLs:\n      - RSID_102\n  TF2:\n    bQTLs:\n      - RSID_17\n      - RSID_198\n    eQTLs:\n      - RSID_2\nextra_treatments:\n  - TREAT_1\noutcome_extra_covariates:\n  - COV_1\nextra_confounders:\n  - 21003\n  - 22001","category":"page"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"For group TF1, only (RSID17, RSID102) and (RSID99, RSID102) are considered and similarly for group TF2.","category":"page"},{"location":"targene/study_designs/#Custom-(Advanced)","page":"Defining the Estimands of Interest","title":"Custom (Advanced)","text":"","category":"section"},{"location":"targene/study_designs/","page":"Defining the Estimands of Interest","title":"Defining the Estimands of Interest","text":"Even though the previous sections should cover most common cases, it may be interesting to define more specific effects. Almost any effect defined within TMLE.jl can be estimated in TarGene. It is thus recommended to use the package to create these effects, group them in a TMLE.Configuration object and save them to a file using the TMLE.write_yaml function.","category":"page"}]
}
