var documenterSearchIndex = {"docs":
[{"location":"confounding_adjustment/#Confounding-Adjustment","page":"Confounding Adjustment","title":"Confounding Adjustment","text":"","category":"section"},{"location":"confounding_adjustment/","page":"Confounding Adjustment","title":"Confounding Adjustment","text":"To account for potential confounding effect due to population stratification, we extract principal components from the genetic data using flashpca. We follow the recommended procedure for this tool which implies some preprocessing and filtering. The following arguments are compulsory:","category":"page"},{"location":"confounding_adjustment/","page":"Confounding Adjustment","title":"Confounding Adjustment","text":"LD_BLOCKS: A path to pre-identified linkage desequlibrium blocks around the variants that will be queried for causal effect estimation. Those will be removed from the data.\nFLASHPCA_EXCLUSION_REGIONS: A path to the flashpca special exclusion regions which is provided in their repository.\nNB_PCS (default: 6): The number of PCA components to extract.","category":"page"},{"location":"nextflow_params/#Index-of-the-pipeline-parameters","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"","category":"section"},{"location":"nextflow_params/","page":"Index of the pipeline parameters","title":"Index of the pipeline parameters","text":"Todo","category":"page"},{"location":"publications/#Publications","page":"Related Publications","title":"Publications","text":"","category":"section"},{"location":"publications/","page":"Related Publications","title":"Related Publications","text":"Dispensing with unnecessary assumptions in population genetics analysis. With accompanying code repository tag for PheWAS' and pairwise interactions and this tag for 3 points interactions.","category":"page"},{"location":"sieve_variance/#Sieve-Variance-Plateau-Correction","page":"Sieve Variance Plateau Correction","title":"Sieve Variance Plateau Correction","text":"","category":"section"},{"location":"sieve_variance/","page":"Sieve Variance Plateau Correction","title":"Sieve Variance Plateau Correction","text":"Finally, the variance estimator can be adjusted via the Sieve Variance Plateau method. For that, we need to compute the GRM which is typically split via GRM_NSPLITS (default: 100). Then the number of estimators to compute in the interval [0, MAX_TAU (default: 0.8)] is given by NB_VAR_ESTIMATORS (default: 0). If NB_VAR_ESTIMATORS is set to 0, the Sieve Variance Plateau method will not be applied. It is also possible, in order to reduce the computational burden to perform this correction only if the initial p-value is below a specific threshold PVAL_SIEVE (default: 0.05). This is because in the correction will only increase the variance estimate.","category":"page"},{"location":"parameter_specification/#Parameter-specification","page":"Parameter specification","title":"Parameter specification","text":"","category":"section"},{"location":"parameter_specification/#Parameter-Files","page":"Parameter specification","title":"Parameter Files","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"In this section, by parameter, we mean the statistical parameter that represents the scientific quantity of interest and will be estimated via TarGene. The complete specification of a parameter requires the description of a causal model which can be represented by the following graph.","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"<div style=\"text-align:center\">\n<img src=\"assets/causal_graph.png\" alt=\"Causal Model\" style=\"width:400px;\"/>\n</div>","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"Parameters are specified by a YAML file with one section for each variable in the causal model and a section for the parameters that need to be estimated from this causal model. Here is a succinct description for each section and the behaviour of the pipeline for each variable:","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"Treatments: The treatment variables, typically one or multiple SNPs and potential environmental exposures.\nConfounders: Confounding variables, typically the principal components which are computed by the pipeline. This section can (must) be ommited.\nCovariates: Additional covariates for the prediction of the traits. Not yet working and must be omitted.\nTargets: The traits of interest. The algorithm will loop through all traits in this section for the given treatments, confounders and covariates. This enables the reuse of the propensity score estimation. If this section is omitted (usually the case) all traits will be used.\nParameters: For each target in Targets multiple parameters may be of interest depending on the exact case/control scenario of the treatment variables. For instance, since each genotyped locus can take up to 3 different values there could be up to 3 Average treatment Effect parameters if the treatment section consists of only one SNP.","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"Since an example may be worth a thousand words, here are a couple of such files.","category":"page"},{"location":"parameter_specification/#Average-Treatment-Effect","page":"Parameter specification","title":"Average Treatment Effect","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"One SNP RSID_10 and 2 ATEs for all traits under study.","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"Treatments:\n  - RSID_10\nParameters:\n  - name: GG_TO_AG\n    RSID_10:\n      control: GG\n      case: AG\n  - name: GG_TO_AA\n    RSID_10:\n      control: GG\n      case: AA","category":"page"},{"location":"parameter_specification/#Interaction-Average-Treatment-Effect","page":"Parameter specification","title":"Interaction Average Treatment Effect","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"Two interacting SNPs RSID_10 and RSID_100 with only one target PHENOTYPE_1 and only one parameter.","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"Treatments:\n  - RSID_10\n  - RSID_100\n\nTargets:\n  - PHENOTYPE_1\n\nParameters:\n  - name: IATE\n    RSID_10:\n      control: GG\n      case: AG\n    RSID_100:\n      control: GG\n      case: AG","category":"page"},{"location":"parameter_specification/#Parameter-strategies","page":"Parameter specification","title":"Parameter strategies","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"There are two main ways one can specify parameters that need to be estimated during a targene-pipeline run. This is done via the MODE parameter.","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"MODE = GivenParameters","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"In this setting, one typically write by \"hand\" or generate a set of parameter files as described above. The path to those parameters is then provided with the PARAMETER_FILES nextflow parameter.","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"MODE = ASBxTransActors","category":"page"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"It is assumed that an initial set of variants has been pre-identified from a previous allele-specific binding (ASB) study as output by the ball-nf pipeline: ASB_FILES parameter. It is also assumed that another set of potential trans-actors is given in a .csv file: TRANS_ACTORS_FILE parameter. In that scenario, the target parameter will be the Interaction Average Treatment Effect between every pair of SNPs. Additionally, if template parameters configuration files containing extra treatments are provided (via PARAMETER_FILES), nth-order interaction parameters will be generated.","category":"page"},{"location":"parameter_specification/#Parallelization","page":"Parameter specification","title":"Parallelization","text":"","category":"section"},{"location":"parameter_specification/","page":"Parameter specification","title":"Parameter specification","text":"Since the same estimator for p(T|W) can be used for multiple target parameters, it may be useful to batch phenotypes using PHENOTYPES_BATCH_SIZE(default: 1) in order to reduce the computational burden.","category":"page"},{"location":"tmle/#Targeted-Maximum-Likelihood-Estimation","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"","category":"section"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"The estimator configuration file describes the TMLE specification for the estimation of the parameters defined in the previous section. This YAML configuration file is provided to the pipeline via the ESTIMATORFILE parameter and contains 4 sections:","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"The threshold section is a simple floating point number that specifies the minimum allowed value for p(Treatments|Confounders).\nThe Q_binary, Q_continuous and G sections describe the learners for the nuisance parameters. Each of them contains a model that corresponds to a valid MLJ model constructor and further keyword hyperparameters. For instance, a Stack can be provided a measures argument to evaluate internal algorithms during cross validation. It can also be provided a potentially adaptive resampling strategy and the library of models. Each of those models can specify a grid of hyperparameters that will individually define a learning algorithm.\nQ_binary corresponds to E[Target| Confounders, Covariates] when the targets are binary variables.\nQ_continuous corresponds to E[Target| Confounders, Covariates] when the targets are continuous variables.\nG corresponds to the joint distribution p(Treatments|Confounders).","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"Here are two example estimator configurations that can serve as a template.","category":"page"},{"location":"tmle/#Example-1","page":"Targeted Maximum Likelihood Estimation","title":"Example 1","text":"","category":"section"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"In this example Super Learner are used for both Q and G models. To perform a grid search across model hyperparameters, one can use a list of hyper-parameters. For instance, for the following Q_continuous, two EvoTreeRegressors will be built constructing respectively 10 and 20 trees. The cross-validation procedure can be made adaptive based on the outcome class balance by using the adaptive: true option.","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"threshold: 1e-8\n# For the estimation of E[Y|W, T]: continuous target\nQ_continuous:\n  model: Stack\n  # Description of the resampling strategy\n  resampling:\n    type: CV\n    # The number of folds is determined based on the data\n    adaptive: true\n  # List all models and hyperparameters\n  models: \n    - type: InteractionLMRegressor\n      column_pattern: [\"^RS_\"]\n\n    - type: EvoTreeRegressor\n      nrounds: [10, 20]\n\n    - type: ConstantRegressor\n\n    - type: HALRegressor\n      max_degree: 1\n      smoothness_orders: 1\n      num_knots: [[10, 5]]\n      lambda: 10\n      cv_select: false\n\n# For the estimation of E[Y|W, T]: bianry target\nQ_binary:\n  model: Stack\n  # Description of the resampling strategy\n  resampling:\n    type: \"StratifiedCV\"\n    # The number of folds is determined based on the data\n    adaptive: true\n  # List all models and hyperparameters\n  models:\n    - type: InteractionLMClassifier\n      column_pattern: [\"^RS_\"]\n\n    - type: ConstantClassifier\n\n    - type: HALClassifier\n      max_degree: 1\n      smoothness_orders: 1\n      num_knots: [[10, 5]]\n      lambda: 10\n      cv_select: false\n\n    - type: EvoTreeClassifier\n      nrounds: 10\n\n# For the estimation of p(T| W)\nG:\n  model: Stack\n  measures: [\"log_loss\"]\n  # Description of the resampling strategy\n  resampling:\n    type: \"StratifiedCV\"\n    # The number of folds is determined based on the data\n    adaptive: true\n    # List all models and hyperparameters\n  models:\n    - type: LogisticClassifier\n      fit_intercept: true\n    - type: ConstantClassifier\n    - type: EvoTreeClassifier\n      nrounds: 10","category":"page"},{"location":"tmle/#Example-2","page":"Targeted Maximum Likelihood Estimation","title":"Example 2","text":"","category":"section"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"While Super Learning is encouraged it is not strictly necessary, here for instance the propensity score is a simple gradient boosting tree and the outcome mode is a logistic regression when the outcome is binary.","category":"page"},{"location":"tmle/","page":"Targeted Maximum Likelihood Estimation","title":"Targeted Maximum Likelihood Estimation","text":"threshold: 0.001\n# For the estimation of E[Y|W, T]: continuous target\nQ_continuous:\n  model: Stack\n  # Description of the resampling strategy\n  resampling:\n    type: CV\n    # The number of folds is determined based on the data\n    adaptive: true\n  # List all models and hyperparameters\n  models: \n    - type: InteractionLMRegressor\n      column_pattern: [\"^RS_\"]\n\n    - type: EvoTreeRegressor\n      nrounds: [10, 20]\n      λ: [0., 1.]\n      γ: [0.3]\n\n    - type: ConstantRegressor\n\n# For the estimation of E[Y|W, T]: binary target\nQ_binary:\n  model: LogisticClassifier\n  lambda: 10.\n\n# For the estimation of p(T| W)\nG:\n  model: EvoTreeClassifier\n  nrounds: 10","category":"page"},{"location":"data_sources/#Setting-a-data-source","page":"Setting a data source","title":"Setting a data source","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"Currently, only the UK-Biobank is supported, stay tuned for further data sources!","category":"page"},{"location":"data_sources/#UK-Biobank","page":"Setting a data source","title":"UK-Biobank","text":"","category":"section"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"The UK-Biobank is composed of both genetic data (.bed and .bgen files) and trait data.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"The trait data: The first option is to provide the path to the encrypted dataset ENCRYPTED_DATASET which must be decoded via the ukbconv software and the encoding file ENCODING_FILE. The second option is to decrypt the dataset only once outside of the pipeline and then use the DECRYPTED_DATASET as an input to the pipeline. Finally, since one is usually not interested in all of the traits, and those traits can play a different role in the causal model, the TRAITS_CONFIG file described here has to be provided.\nThe genetic data: We are currently using both .bgen and .bed files. Those are respectively provided with the UKBB_BGEN_FILES and UKBB_BED_FILES parameters. Since the UK-Biobank genotypic data is split in chromosomes, it should be of the form PREFIX_TO_CHROMOSOMES{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bgen,sample,bgen.bgi} and PREFIX_TO_CHROMOSOMES{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bed,bim,fam} respectively.","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"Additional UK-Biobank required files for preprocessing and filtering are:","category":"page"},{"location":"data_sources/","page":"Setting a data source","title":"Setting a data source","text":"QC_FILE: A path to the UK-Biobank SNP quaility control ukb_snp_qc.txt file.\nWITHDRAWAL_LIST: A path to the withdrawal sample list to exclude removed participants from the study.","category":"page"},{"location":"overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Since TarGene is a Nextflow pipeline, the \"only\" thing that needs to be done to run a pipeline is to write a nextflow.config file providing a specification for each of the pipeline parameters. Most of those parameters are paths to additional files located in your project or in an accessible location on your system. The following sections describe those parameters and provide some examples to help you get started.","category":"page"},{"location":"associated_softwares/#Associated-Softwares","page":"Associated Softwares","title":"Associated Softwares","text":"","category":"section"},{"location":"associated_softwares/","page":"Associated Softwares","title":"Associated Softwares","text":"You may either not be interested in population genetics at all, or not willing to run the full pipeline on your project. If you are still eager to leverage the Targeted Learning framework, you can either rely on:","category":"page"},{"location":"associated_softwares/","page":"Associated Softwares","title":"Associated Softwares","text":"TMLE.jl: A Julia package for Targeted Maximum Likelihood Estimation (TMLE).\nTargetedEstimation.jl: A command line interface to run TMLE on your data.","category":"page"},{"location":"#TarGene","page":"Home","title":"TarGene","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to TarGene, the software that brings Targeted Learning to population genetic studies! TarGene enables the estimation of various effect sizes including the Average Treatment Effect and the Interaction Average Treatment Effect (epistasis) up to any order. Because we follow the Targeted Learning framework, the final estimates provided by TarGene are covered by mathematical guarantees. The software is delivered as a Nexflow pipeline to bring scalability and reproducibility to your research.","category":"page"},{"location":"#Overview-of-the-pipeline","page":"Home","title":"Overview of the pipeline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The pipeline can rougly be decoupled into three steps. The first, aims at pre-processing the data sources to convert them in a table data format that can be wielded by Targeted Maximum Likelihood Estimation (TMLE). The second is the TMLE itself. The third and final step is the Sieve Variance Plateau correction which revises the variance estimate to account for the fact that individuals in the population are not necessarily independent. The following diagram provides a high level interface of the organisation of the pipeline.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div style=\"text-align:center\">\n<img src=\"assets/targene_diagram.png\" alt=\"Targene Pipeline\" style=\"width:800px;\"/>\n</div>","category":"page"},{"location":"#Quick-start-for-Eddie-users","page":"Home","title":"Quick start for Eddie users","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are a University of Edinburgh researcher and have access to the Eddie cluster you may want to use the Eddie-Template to quickly setup your project. ","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Todo: depends on PI discussion.","category":"page"},{"location":"#Getting-in-touch","page":"Home","title":"Getting in touch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please feel free to raise an issue if you face a problem or would benefit from a new feature. Contributions are most welcome.","category":"page"}]
}
