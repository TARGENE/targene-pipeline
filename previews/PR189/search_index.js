var documenterSearchIndex = {"docs":
[{"location":"publications/#Publications","page":"Related Publications","title":"Publications","text":"","category":"section"},{"location":"publications/","page":"Related Publications","title":"Related Publications","text":"Dispensing with unnecessary assumptions in population genetics analysis. With accompanying code repository tag for PheWAS' and pairwise interactions and this tag for 3 points interactions.","category":"page"},{"location":"targene/runtime_considerations/#Some-Runtime-considerations","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"","category":"section"},{"location":"targene/runtime_considerations/","page":"Some Runtime considerations","title":"Some Runtime considerations","text":"Runtime is discussed in detail here.","category":"page"},{"location":"targene/tmle/#Specifying-a-Targeted-Estimator","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"","category":"section"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"TarGene is a flexible procedure that does not impose any constraint on the functional form of the relationship between genetic variants, environmental variables and traits. In practice, we rely on MLJ to provide machine learning algorithms. In population genetics studies, there are two learning algorithms we need to specify:","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"E[Y|T, W, C]: The mean outcome given the treatment, confounders and extra covariates. It is commonly denoted by Q in the Targeted Learning literature. In reality, we will need one specification for continuous outcomes and one specification for binary outcomes.\np(T|W): The propensity score, which enables the targeting step of the estimation procedure. It is commonly denoted by G in the Targeted Learning literature.","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"There are two main ways to define targeted estimators, from a predefined configuration or from a custom file.","category":"page"},{"location":"targene/tmle/#Predefined-estimators","page":"Specifying a Targeted Estimator","title":"Predefined estimators","text":"","category":"section"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"A set of predefined estimators is readily available and can be accessed by using the configuration's name as the ESTIMATOR_FILE parameter. For instance, we provide the following:","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"G-superlearning-Q-glm\nG-superlearning-Q-glmnet\nglm-with-interactions-for-Q\nglm\nglmnet-with-interactions-for-Q\nglmnet\nsuperlearning-with-interactions-for-Q\nsuperlearning\ntuned-xgboost","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"all using TMLE as the meta statistical inference method. While these should cover most use cases, it may be useful to define a custom estimation strategy. This can be achieved by writing a small Julia file described below.","category":"page"},{"location":"targene/tmle/#Custom-estimators-from-a-file","page":"Specifying a Targeted Estimator","title":"Custom estimators from a file","text":"","category":"section"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"Writing a Julia estimators file is the most flexible way to define an estimation strategy for your study.","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"This file should simply define a NamedTuple called ESTIMATORS and listing estimators to be used for inference. For example, the following:","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"default_models = TMLE.default_models(\n  # For the estimation of E[Y|W, T]: continuous outcome\n  Q_continuous = LinearRegressor(),\n  # For the estimation of E[Y|W, T]: binary target\n  Q_binary = LogisticClassifier(lambda=0.),\n  # For the estimation of p(T| W)\n  G = LogisticClassifier(lambda=0.)\n)\n\nESTIMATORS = (\n  TMLE_weighted   = TMLEE(models=default_models, weighted=true),\n  TMLE_unweighted = TMLEE(models=default_models, weighted=false),\n  OSE             = OSE(models=default_models)\n)","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"defines three estimators:","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"A weighted-fluctuation Targeted Maximum Likelihood Estimator: TMLE\nAn unweighted-fluctuation Targeted Maximum Likelihood Estimator: TMLE\nA One-Step Estimator: OSE","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"All estimators will learn the nuisance functions Q and G with the provided models NamedTuple:","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"Q_continuous: A MLJ model used for the estimation of E[Y|T, W, C] when the outcome Y is continuous.\nQ_binary: A MLJ model used for the estimation of E[Y|T, W, C] when the outcome Y is binary.\nG: A MLJ model used for the estimation of p(T|W).","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"For the list of available models and resampling strategies, checkout the TargetedEstimation documentation.","category":"page"},{"location":"targene/tmle/","page":"Specifying a Targeted Estimator","title":"Specifying a Targeted Estimator","text":"For full details, on available estimators and how to specify them, visit the TMLE.jl documentation.","category":"page"},{"location":"negative_control/overview/#Negative-Control-Overview","page":"Negative Control Overview","title":"Negative Control Overview","text":"","category":"section"},{"location":"negative_control/overview/","page":"Negative Control Overview","title":"Negative Control Overview","text":"After running a TarGene workflow, you may want to perform some quality control checks on your positive hits. This is done via negative control. By negative control we mean inducing a transformation on the dataset that alters the relationship between the variables of interest. Typically, between a genetic variant and a trait. The expectation is that any subsequent TarGene run, on this transformed dataset, will not deviate from the null hypothesis of no association.","category":"page"},{"location":"negative_control/overview/","page":"Negative Control Overview","title":"Negative Control Overview","text":"At the moment, there are two main strategies for negative control in TarGene:","category":"page"},{"location":"negative_control/overview/","page":"Negative Control Overview","title":"Negative Control Overview","text":"The Permutation Test Workflow\nThe Randomized Variants Workflow","category":"page"},{"location":"developer_guide/project_organization/#Project-Organization","page":"Project Organization","title":"Project Organization","text":"","category":"section"},{"location":"developer_guide/project_organization/","page":"Project Organization","title":"Project Organization","text":"The TarGene project is organized around the targene-pipeline repository which contains the Nextflow workflows. However, this repository does not contain the executables that are used by the Nextflow processes. Those executables originate from complementary repositories:","category":"page"},{"location":"developer_guide/project_organization/","page":"Project Organization","title":"Project Organization","text":"TargetedEstimation.jl\nTargeneCore.jl\nUKBMain.jl","category":"page"},{"location":"developer_guide/project_organization/","page":"Project Organization","title":"Project Organization","text":"The following diagram presents a high level perspective of the project's organization and dependence structure.","category":"page"},{"location":"developer_guide/project_organization/","page":"Project Organization","title":"Project Organization","text":"(Image: TarGene Organization)","category":"page"},{"location":"targene/data_sources/#Setting-a-Data-Source","page":"Setting a Data Source","title":"Setting a Data Source","text":"","category":"section"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"The dataset is the main ingredient manipulated by TarGene. Population Genetics datasets are typically split into a variety of files and formats. The following section describes these formats and how to input them to a TarGene run.","category":"page"},{"location":"targene/data_sources/#Genetic-Data","page":"Setting a Data Source","title":"Genetic Data","text":"","category":"section"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"TarGene currently works onlywith genotyping arrays datasets. These datasets can come in multiple formats and we currently use both .bgen and .bed formats. The .bgen files contain imputed variants and are provided via the BGEN_FILES argument. These files are used to lookup for the variants of interest in the analysis that may not have been directly genotyped. The .bed files are provided via the BED_FILES parameter and are used to extract genetic confounders (via PCA). ","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"It is also assumed that the genotyping data is unphased. and split in chromosomes. An example for the BGEN_FILES and BED_FILES arguments are:","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"BGEN_FILES=\"path_to_folder/imputed_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bgen,sample,bgen.bgi}\" \nBED_FILES=\"path_to_folder/genotyped_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}.{bed,bim,fam}","category":"page"},{"location":"targene/data_sources/#Traits-Dataset","page":"Setting a Data Source","title":"Traits Dataset","text":"","category":"section"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"In addition to genetic data, a set of traits is usually also available in a different format.","category":"page"},{"location":"targene/data_sources/#Custom-Dataset","page":"Setting a Data Source","title":"Custom Dataset","text":"","category":"section"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"Select with COHORT = \"MY_COHORT\" (This can be anything).","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"TarGene supports custom traits datasets in CSV format, that can be provided via the TRAITS_DATASET parameter. Please ensure the Sample IDs that identify individuals in your cohort are included as the first column of your trait data, with the column name SAMPLE_ID.","category":"page"},{"location":"targene/data_sources/#UK-Biobank","page":"Setting a Data Source","title":"UK-Biobank","text":"","category":"section"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"Select with COHORT = \"UKBB\", for more information on the structure of the UK-Biobank data, please refer to their User Guide.","category":"page"},{"location":"targene/data_sources/#Overview","page":"Setting a Data Source","title":"Overview","text":"","category":"section"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"The trait dataset is often called the \"main dataset\" and consists of an encrypted file containing individuals' trait information accessible via your project. The first option is thus to provide this dataset using the TRAITS_DATASET parameter. Since the data is encrypted, the pipeline will also need the encoding file that you can provide with UKB_ENCODING_FILE. If a \"typical\" main dataset has already been decrypted outside the TarGene workflow, one may use the TRAITS_DATASET as an input to the pipeline and leave the UKB_ENCODING_FILE argument unspecified. However, please make sure that all the fields in the UKB_CONFIG (see below) file described above are contained in this dataset.","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"!!! Tip","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"Since this decrypted dataset is a plain CSV file, one may build and add extra columns to it (for instance to define new phenotypes). Any column in the decrypted dataset which does not correspond to a UK-Biobank field will be considered as such.","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"A \"main dataset\" is typically very large and only a few traits will be of interest to a given study. To extract these traits from the dataset, a UKB_CONFIG YAML file must be provided. Since, writing by hand such a file for large scale study can quickly become tenuous, we provide a configuration file corresponding to the [GeneAtlas] study as a default and template to be modified.","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"The structure of the UKB_CONFIG YAML file consists of extraction rules that convert UK-Biobank fields to traits of interest. It contains two sections, a traits section and a subset section.","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"The traits section describes the phenotypes that are either confounders, covariates or outcomes in the causal model. See below for how this is defined.\nThe subset section provides a way to filter the data based on specific traits and hence estimate conditional effects. Each element in this section corresponds to the additional condition to be met by an individual to be added in the subset.","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"Each section is further divided in a list of fields items that match the UK-Biobank data fields and a list of phenotypes that can be extracted from these fields. This is because the content of a UK-Biobank field cannot typically be used immediately and need to be processed. For instance a UK-Biobank field may contain information about multiple phenotypes or a phenotype may be defined from multiple fields. Each phenotype in the phenotypes subsection of the UKB_CONFIG YAML file is identified by a name and an optional list of codings identifying it. Altogether, a phenotype element defines an extraction rule from a list of fields and codings.","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"Example of a minimal UKB_CONFIG YAML file:","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"traits:\n    - fields:\n        - \"41202\"\n        - \"41204\"\n      phenotypes:\n        - name: \"Chronic lower respiratory diseases\"\n          codings:\n            - \"J40\"\n            - \"J41\"\n            - \"J410\"\n            - \"J411\"\n            - \"J418\"\n            - \"J42\"\n            - \"J43\"\n        - name: \"Other extrapyramidal and movement disorders\"\n          codings:\n            - \"G254\"\n            - \"G255\"\n            - \"G256\"\n            - \"G258\"\n    - fields:\n        - \"40006\"\n      phenotypes:\n        - name: \"Malignant melanoma of skin\"\n          codings:\n            - \"C430\"\n            - \"C431\"\n            - \"C432\"\n            - \"C433\"\n            - \"C434\"\n            - \"C435\"\n            - \"C436\"\n            - \"C437\"\n            - \"C438\"\n            - \"C439\"\n    - fields:\n        - \"1558\"\n      phenotypes:\n        - name: \"Alcohol intake frequency.\"\n    - fields:\n        - \"20117\"\n      phenotypes:\n        - name: \"Alcohol drinker status\"\n\nsubset: \n    - fields: 22001\n      phenotypes:\n        - name: Female\n          codings: 0\n    - fields: 21000\n      phenotypes:\n        - name: White\n          codings: [1001]","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"This configuration file restricts the analysis to white females and extract 5 traits. The first trait, \"Chronic lower respiratory diseases\" is considered \"true\" for an individual if any of the codings: (\"J40\", \"J41\", \"J410\", \"J411\", \"J418\", \"J42\", \"J43\") appears in any of the fields (41202, 41204).","category":"page"},{"location":"targene/data_sources/#The-Extraction-Rules","page":"Setting a Data Source","title":"The Extraction Rules","text":"","category":"section"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"We now further describe the currently available extraction rules based on variable types. Not that a variable type is determined by the metadata provided by the UK-Biobank and not a programmatic rule.","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"Continuous/Count variables: Those consist in a single field and the extraction rule will take the first visit assessment value for that field. For instance, the \"Alcohol intake frequency.\" is simply the value from the \"1558-0.0\" column.\nOrdinal data: Some variables are encoded as categorical by the UK-Biobank but an ordinal interpretation seems more appropriate. A list of such variables has been identified and hard-coded. They will be treated like continuous variables.\nCategorical variables: For those variables again, the first visit assessment value for the specified field is used. An additional codings field can be provided. In that case the variable is turned into a indicator variable indicating whether an individual is included in the criterion given by the codings list. In the previous subset section, \"White\" is such a transformation into a binary variable. Note that we could add more elements to the list to subset on a wider population.\nCategorical Arrayed variables: Some fields in the UK-Biobank consist in list of traits for individuals. This is the case for at least the fields: 41202, 41204, 20002 and 40006. For those fields it is essential to define a codings section that describes a trait. For instance \"Malignant melanoma of skin\" corresponds to the declaration of any of the codings for an individual. Moreover, some fields share the same encoding, this is the case for  41202 and 41204. In that situation it may be useful to aggregate those sources of information. For that purpose multiple fields can be provided to the fields section. In that scenario the declaration of any of the coding in a codings section for any of the fields will define the trait.","category":"page"},{"location":"targene/data_sources/#Additional-Files","page":"Setting a Data Source","title":"Additional Files","text":"","category":"section"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"Additional UK-Biobank required files for preprocessing and filtering are:","category":"page"},{"location":"targene/data_sources/","page":"Setting a Data Source","title":"Setting a Data Source","text":"QC_FILE: A path to the UK-Biobank SNP quality control ukb_snp_qc.txt file.\nUKB_WITHDRAWAL_LIST: A path to the withdrawal sample list to exclude removed participants from the study.","category":"page"},{"location":"negative_control/randomized_tests/#The-Randomized-Variants-Workflow","page":"The Randomized Variants Workflow","title":"The Randomized Variants Workflow","text":"","category":"section"},{"location":"negative_control/randomized_tests/","page":"The Randomized Variants Workflow","title":"The Randomized Variants Workflow","text":"In some interaction studies the variants are chosen based on a priori knowledge, typically they are QTLs. Consider two variants V_1 and V_2, that have been pre-identified to be eQTLs for a given gene's transcript. Then it makes sense to run a phenome-wide association study between these two variants because they potentially interact via the gene's transcript. This can be done via the TarGene workflow for instance and discoveries can be made. Because these two variants have been carefully chosen, replacing any of them for a random variant whose function is unknown should not result in an interaction. This is the basis of the randomized variants workflow.","category":"page"},{"location":"negative_control/randomized_tests/","page":"The Randomized Variants Workflow","title":"The Randomized Variants Workflow","text":"note: Note\nBy chance, one could pick a random variant also interacting with the other QTL. However, on average we should not deviate from the null hypothesis of no association. This can be checked via examination of the QQ plot generated by TarGene. This is addressed by selecting a certain number of random variants (N_RANDOM_VARIANTS) instead of 1.","category":"page"},{"location":"negative_control/randomized_tests/#Example-Run-Command","page":"The Randomized Variants Workflow","title":"Example Run Command","text":"","category":"section"},{"location":"negative_control/randomized_tests/","page":"The Randomized Variants Workflow","title":"The Randomized Variants Workflow","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TAG -entry RANDOMIZATION_TEST -profile P -resume","category":"page"},{"location":"negative_control/randomized_tests/#Configuration","page":"The Randomized Variants Workflow","title":"Configuration","text":"","category":"section"},{"location":"negative_control/randomized_tests/#Main-Outputs","page":"The Randomized Variants Workflow","title":"Main Outputs","text":"","category":"section"},{"location":"negative_control/randomized_tests/","page":"The Randomized Variants Workflow","title":"The Randomized Variants Workflow","text":"The workflow will produce the following output in the output directory (OUTDIR, defaults to results): an estimand file named \"randomvariantsestimands.jls\". This can be further used by running a classic TarGene workflow with STUDY_DESIGN=CUSTOM.","category":"page"},{"location":"negative_control/randomized_tests/#Arguments","page":"The Randomized Variants Workflow","title":"Arguments","text":"","category":"section"},{"location":"negative_control/randomized_tests/","page":"The Randomized Variants Workflow","title":"The Randomized Variants Workflow","text":"RESULTS_FILE (required): Path to an HDF5 results file generated by a previous TarGene run.\nVARIANTS_TO_RANDOMIZE (required): A text file containing the list of variants (one rsid per line) for which random candidates must be proposed.\nBGEN_FILES (required): Path to imputed BGEN files from which the variants in VARIANTS_TO_RANDOMIZE will be found.","category":"page"},{"location":"negative_control/randomized_tests/#Main-Options","page":"The Randomized Variants Workflow","title":"Main Options","text":"","category":"section"},{"location":"negative_control/randomized_tests/","page":"The Randomized Variants Workflow","title":"The Randomized Variants Workflow","text":"N_RANDOM_VARIANTS (optional, default: 10): The number of random variants to propose for each variant in VARIANTS_TO_RANDOMIZE.\nMAF_MATCHING_RELTOL (optional, default: 0.05): Each proposed random variant should have the same minor allele frequency as the origin variant, up to a relative threshold.\nPOSITIVITY_CONSTRAINT (optional, default: 0.01): When the list of estimands is generated or validated. Treatment variables' rarest configuration should have at least that frequency. For example if the treatment variables are two variants with minor allele A and T respectively. The rarest configuration will be (AA, TT) and should have a frequency of at least POSITIVITY_CONSTRAINT.\nPVAL_THRESHOLD (optional, default: 0.05): Only results with a p-value below this threshold are considered for permutation testing.\nESTIMATOR_KEY (optional, default: TMLE): The p-value for PVAL_THRESHOLD is computed using the result from this estimator.","category":"page"},{"location":"negative_control/randomized_tests/#Secondary-Options","page":"The Randomized Variants Workflow","title":"Secondary Options","text":"","category":"section"},{"location":"negative_control/randomized_tests/","page":"The Randomized Variants Workflow","title":"The Randomized Variants Workflow","text":"RNG (optional, default: 123): General random seed used to induce permutation.\nVERBOSITY (optional, default: 0): Verbosity level of the the Workflow's processes.","category":"page"},{"location":"targene/miscellaneous/#Tweaking-additional-behaviour","page":"Tweaking additional behaviour","title":"Tweaking additional behaviour","text":"","category":"section"},{"location":"targene/miscellaneous/","page":"Tweaking additional behaviour","title":"Tweaking additional behaviour","text":"Further Nextflow parameter affecting the behaviour of the pipeline but that does not fit in any previously described category is listed here:","category":"page"},{"location":"targene/miscellaneous/","page":"Tweaking additional behaviour","title":"Tweaking additional behaviour","text":"CALL_THRESHOLD (optional, default: 0.9): For putative causal variants (listed in the parameter files described in the Study Designs section). If an individual's allele's probability is greater than the threshold, then it is called, otherwise it is considered missing.\nBATCH_SIZE (optional, default: 400): The set of estimands to be estimated is batched and the TMLE processes will run in parallel across batches on your platform.\nSVP (optional, default: false): To activate Sieve Variance Plateau correction. If set to true, KEEP_IC will automatically be set to true as well.\nKEEP_IC (optional, default: false): For all estimands with a p-value below PVAL_THRESHOLD, the influence curve is saved.\nNB_SVP_ESTIMATORS (optional, default: 100): The number of estimators in Sieve Variance Plateau.\nOUTDIR (optional, default: \"results\"): Output directory","category":"page"},{"location":"secondary_workflows/make_dataset/#The-Make-Dataset-Workflow","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"","category":"section"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"This workflow extracts an aggregated dataset containing traits (TRAITS_DATASET), genetic confounders (NB_PCS) and genetic variants (VARIANTS_LIST) in an Arrow tabular format.","category":"page"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"(Image: Make Dataset)","category":"page"},{"location":"secondary_workflows/make_dataset/#Example-Run-Command","page":"The Make Dataset Workflow","title":"Example Run Command","text":"","category":"section"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TAG -entry MAKE_DATASET -profile P -resume","category":"page"},{"location":"secondary_workflows/make_dataset/#List-Of-Workflow-Arguments","page":"The Make Dataset Workflow","title":"List Of Workflow Arguments","text":"","category":"section"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"VARIANTS_LIST (required): A text file (one rsid per line) specifying the variants of interest.\nBGEN_FILES (required): Path to imputed BGEN files from which the variants in VARIANTS_LIST will be extracted.\nNB_PCS (optional, default: 6): The number of PCA components to extract.\nBED_FILES (required): Path expression to PLINK BED files.\nCOHORT (optional: \"UKBB\"): Current default for this is UKBB. If set to a value other than UKBB, this will not run UKBB-specific trait extraction.\nTRAITS_DATASET (required): Path to a traits dataset. If you are running this for a non-UKBB cohort, your sample IDs must be specified in the first column of this CSV file, with the column name SAMPLE_ID.\nFLASHPCA_EXCLUSION_REGIONS (optional, default: assets/exclusionregionshg19.txt): A path to the flashpca special exclusion regions.\nMAF_THRESHOLD (optional, default: 0.01): Only variants with that minor allele frequency are considered\nLD_BLOCKS (optional): A path to pre-identified linkage disequlibrium blocks to be removed from the BED files. It is good practice to specify LD_BLOCKS, as it will remove SNPs correlated with your variants-of-interest before running PCA.","category":"page"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"If the COHORT argument is set to UKBB:","category":"page"},{"location":"secondary_workflows/make_dataset/","page":"The Make Dataset Workflow","title":"The Make Dataset Workflow","text":"UKB_CONFIG (required): YAML configuration file describing which traits should be extracted and how the population should be subsetted.\nUKB_ENCODING_FILE (optional): If the TRAITS_DATASET is encrypted, an encoding file must be provided.\nUKB_WITHDRAWAL_LIST (optional): List of participants withdrawn from the study.\nQC_FILE (optional): Genotyping quality control file from the UK-Biobank study.","category":"page"},{"location":"associated_softwares/#Associated-Softwares","page":"Associated Softwares","title":"Associated Softwares","text":"","category":"section"},{"location":"associated_softwares/","page":"Associated Softwares","title":"Associated Softwares","text":"You may either not be interested in population genetics at all, or not willing to run the full pipeline on your project. If you are still eager to leverage the Targeted Learning framework, you can either rely on:","category":"page"},{"location":"associated_softwares/","page":"Associated Softwares","title":"Associated Softwares","text":"TMLE.jl: A Julia package for Targeted Maximum Likelihood Estimation (TMLE).\nTargetedEstimation.jl: A command line interface to run TMLE on your data.","category":"page"},{"location":"negative_control/permutation_tests/#The-Permutation-Test-Workflow","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"","category":"section"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"An association is a deterministic link between a set of at least two variables, for example a genetic variant V and a trait Y. If we were to randomly shuffle the column V in the dataset, our expectation is that the association between V and Y will be altered. Obviously, there is nothing special about the variant V, we could also independently shuffle the column Y and expect the same result. These are termed order 1 permutations because only one column has been permuted. An equally valid order 2 permutation would shuffle both V and Y independently. The following picture displays the example:","category":"page"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"(Image: Permutation Tests)","category":"page"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"In general for an association between K variables, there are 1K permutation orders. The Permutation Test Workflow generalizes this idea to all the associations passing a specific p-value threshold (PVAL_THRESHOLD) and induces all permutation orders given by PERMUTATION_ORDERS. It then runs the estimation workflow for the permuted estimands.","category":"page"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"note: Note\nOf course, if we are unlucky, it could be that a given permutation will not completely alter an association. However, on average we should not deviate from the null hypothesis of no association. This can be checked via examination of the QQ plot generated by TarGene.","category":"page"},{"location":"negative_control/permutation_tests/#Example-Run-Command","page":"The Permutation Test Workflow","title":"Example Run Command","text":"","category":"section"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TAG -entry PERMUTATION_TEST -profile P -resume","category":"page"},{"location":"negative_control/permutation_tests/#Configuration","page":"The Permutation Test Workflow","title":"Configuration","text":"","category":"section"},{"location":"negative_control/permutation_tests/#Main-Outputs","page":"The Permutation Test Workflow","title":"Main Outputs","text":"","category":"section"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"The workflow will produce the following main outputs in the output directory (OUTDIR, defaults to results):","category":"page"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"An HDF5 file containing estimation results (HDF5_OUTPUT, default: results.hdf5)\nAn optional JSON file containing estimation results (JSON_OUTPUT)\nA Quantile-Quantile summary plot: QQ.png","category":"page"},{"location":"negative_control/permutation_tests/#Arguments","page":"The Permutation Test Workflow","title":"Arguments","text":"","category":"section"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"RESULTS_FILE (required): Path to an HDF5 results file generated by a previous TarGene run.\nAGGREGATED_DATASET (required): Path to an aggregated dataset generated by a previous TarGene run.\nESTIMATOR_FILE (required): Estimator name or Julia file containing the description of the Targeted Estimators to use. To be consistent it should match the argument provided to the previous TarGene run.","category":"page"},{"location":"negative_control/permutation_tests/#Main-Options","page":"The Permutation Test Workflow","title":"Main Options","text":"","category":"section"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"PERMUTATION_ORDERS (optional, default: \"1\"): A comma separating string defining the permutation test orders to be performed.\nPOSITIVITY_CONSTRAINT (optional, default: 0.01): When the list of estimands is generated or validated. Treatment variables' rarest configuration should have at least that frequency. For example if the treatment variables are two variants with minor allele A and T respectively. The rarest configuration will be (AA, TT) and should have a frequency of at least POSITIVITY_CONSTRAINT.\nPVAL_THRESHOLD (optional, default: 0.05): Only results with a p-value below this threshold are considered for permutation testing.\nESTIMATOR_KEY (optional, default: TMLE): The p-value for PVAL_THRESHOLD is computed using the result from this estimator.\nMAX_PERMUTATION_ATTEMPTS (optional, default: 1): When generating permutations across multiple columns, the permuted dataset could result in many positivity violations as defined by POSITIVITY_CONSTRAINT. Multiple attempts can be made to maximize the number of permutation estimands satisfying the positivity constraint.","category":"page"},{"location":"negative_control/permutation_tests/#Secondary-Options","page":"The Permutation Test Workflow","title":"Secondary Options","text":"","category":"section"},{"location":"negative_control/permutation_tests/","page":"The Permutation Test Workflow","title":"The Permutation Test Workflow","text":"MAX_PERMUTATION_TESTS (optional): If you have many discoveries, you may want to limit the number of performed permutation tests.\nRNG (optional, default: 123): General random seed used to induce permutation.\nVERBOSITY (optional, default: 0): Verbosity level of the the Workflow's processes.\nBATCH_SIZE (optional, default: 400): The set of estimands to be estimated is batched and the Targeted Learning processes will run in parallel across batches.\nTL_SAVE_EVERY (optional: default: 50): During the estimation process, results are appended to the file in chunks to free memory.\nKEEP_IC (optional): To save the Influence Curves for each estimate. Depending on the size of your dataset, this can result in massive disk usage.","category":"page"},{"location":"secondary_workflows/pca/#The-PCA-Workflow","page":"The PCA Workflow","title":"The PCA Workflow","text":"","category":"section"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"This workflow computes the number of specified principal components (NB_PCS) using flashpca from a set of genetic files in BED format (BED_FILES) and a trait dataset specifying the population of interest (TRAITS_DATASET).","category":"page"},{"location":"secondary_workflows/pca/#Example-Run-Command","page":"The PCA Workflow","title":"Example Run Command","text":"","category":"section"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TAG -entry PCA -profile P -resume","category":"page"},{"location":"secondary_workflows/pca/#List-Of-Workflow-Arguments","page":"The PCA Workflow","title":"List Of Workflow Arguments","text":"","category":"section"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"NB_PCS (optional, default: 6): The number of PCA components to extract.\nBED_FILES (required): Path expression to PLINK BED files.\nCOHORT (optional: \"UKBB\"): Current default for this is UKBB. If set to a value other than UKBB, this will not run UKBB-specific trait extraction.\nTRAITS_DATASET (required): Path to a traits dataset. If you are running this for a non-UKBB cohort, your sample IDs must be specified in the first column of this CSV file, with the column name SAMPLE_ID.\nFLASHPCA_EXCLUSION_REGIONS (optional, default: assets/exclusionregionshg19.txt): A path to the flashpca special exclusion regions.\nMAF_THRESHOLD (optional, default: 0.01): Only variants with that minor allele frequency are considered\nLD_BLOCKS (optional): A path to pre-identified linkage disequlibrium blocks to be removed from the BED files. It is good practice to specify LD_BLOCKS, as it will remove SNPs correlated with your variants-of-interest before running PCA.","category":"page"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"If the COHORT argument is set to UKBB:","category":"page"},{"location":"secondary_workflows/pca/","page":"The PCA Workflow","title":"The PCA Workflow","text":"UKB_CONFIG (required): YAML configuration file describing which traits should be extracted and how the population should be subsetted.\nUKB_ENCODING_FILE (optional): If the TRAITS_DATASET is encrypted, an encoding file must be provided.\nUKB_WITHDRAWAL_LIST (optional): List of participants withdrawn from the study.\nQC_FILE (optional): Genotyping quality control file from the UK-Biobank study.","category":"page"},{"location":"targene/confounding_adjustment/#Adjusting-for-confounders","page":"Adjusting for confounders","title":"Adjusting for confounders","text":"","category":"section"},{"location":"targene/confounding_adjustment/","page":"Adjusting for confounders","title":"Adjusting for confounders","text":"To account for potential confounding effect due to population stratification, we extract principal components from the genetic data using flashpca. We follow the recommended procedure for this tool which implies some preprocessing and filtering. The associated arguments are as follows:","category":"page"},{"location":"targene/confounding_adjustment/","page":"Adjusting for confounders","title":"Adjusting for confounders","text":"LD_BLOCKS (optional): A path to pre-identified linkage desequlibrium blocks around the variants that will be queried for causal effect estimation. Those LD blocks will be removed from the data.\nFLASHPCA_EXCLUSION_REGIONS (required, default: assets/exclusionregionshg19.txt): A path to the flashpca special exclusion regions which is provided in their repository.\nMAF_THRESHOLD (optional, default: 0.01): Only variants with that minor allele frequency are used to compute principal components.\nNB_PCS (optional, default: 6): The number of PCA components to extract.","category":"page"},{"location":"developer_guide/contribution_guide/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Contributions, whether bug fixes, new features or documentation improvements are very welcome!","category":"page"},{"location":"developer_guide/contribution_guide/#Raise-an-issue","page":"Contributing","title":"Raise an issue","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"In order to discuss and track the evolution of the project, please first raise an issue on the targene-pipeline repository. If a change is agreed upon, the discussion should identify the relevant repositories that are concerned by the change and open an issue on each of the repository. For instance, if one wishes to improve the extraction of traits from the UK-Biobank, the UKBMain.jl would surely be impacted and a new release for that package necessary.","category":"page"},{"location":"developer_guide/contribution_guide/#Workflow","page":"Contributing","title":"Workflow","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Following our previous UKBMain.jl example, there are two repositories that need to be updated, the current workflow is as follows:","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Develop\nUKBMain.jl\nCreate a new git branch for your change\nDevelop and test\nRelease an image for your branch by selecting it after clicking the Run workflow button. If the tests pass, a new docker image will be generated and hosted on Docker hub with your branch's name (see Note on Docker images).\ntargene-pipeline\nCreate a new git branch for your change\nFor each Nextflow process using the UKBMain.jl's docker image, update to the branch's image name.\nDevelop further required changes and run/add the tests (see Note on the pipeline's tests).\nReview: When everything is working, ask for a review\nRelease UKBMain.jl:\nMerge your branch into main\nCreate a new Github release following semantic versioning, this will create a new docker image with your release name.\nRelease TarGene\nFor each Nextflow process using the UKBMain.jl's docker image, update to the released image name (as before).\nCreate a new Github release following semantic versioning","category":"page"},{"location":"developer_guide/contribution_guide/#Note-on-Docker-images","page":"Contributing","title":"Note on Docker images","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Currently, all TarGene building blocks (executables) are provided as docker images. The following table provides a map linking each TarGene repository to the associated Docker image tags.","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"Repository Docker tag\nTargeneCore.jl tl-core\nUKBMain.jl ukbmain\nTargetedEstimation.jl targeted-estimation","category":"page"},{"location":"developer_guide/contribution_guide/#Note-on-the-pipeline's-tests","page":"Contributing","title":"Note on the pipeline's tests","text":"","category":"section"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"The pipeline is automatically tested for every push/pull-request made to the github repository. To run the tests locally, you will need Julia and the container engine of your choice to be installed (see below). Each test corresponds to a pipeline run and a file in the test directory is associated with it. Each test run can be launched locally as follows:","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"julia --project=test --startup-file=no test/TESTFILE -profile PROFILE -resume","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"where TESTFILE is the corresponding file and PROFILE is one of: 'local' (local with singularity engine), 'ci' (local with singularity engine), 'eddie' (SGE with singularity engine.).","category":"page"},{"location":"developer_guide/contribution_guide/","page":"Contributing","title":"Contributing","text":"The tests can also be run interactively via the Julia REPL.","category":"page"},{"location":"targene/overview/#The-TarGene-Workflow","page":"The TarGene Workflow","title":"The TarGene Workflow","text":"","category":"section"},{"location":"targene/overview/#Overview","page":"The TarGene Workflow","title":"Overview","text":"","category":"section"},{"location":"targene/overview/","page":"The TarGene Workflow","title":"The TarGene Workflow","text":"The main purpose of the TarGene workflow is to estimate a wide variety of genetic effect sizes using the Targeted Learning framework. Contrary to a GWAS setting, this workflow assumes that a set of genetic variants and/or environmental variables have been pre-identified and are provided as an input to the workflow. The workflow can be roughly decomposed into three main steps:","category":"page"},{"location":"targene/overview/","page":"The TarGene Workflow","title":"The TarGene Workflow","text":"In the first step, all data sources are brought together to build:\nAn aggregated dataset containing all variables in an Arrow tabular format.\nA set of estimands files that contain all the effect sizes to be estimated.\nIn the second step all effect sizes are estimated via Targeted Learning in a parallel manner across the generated estimands files. Results are then merged into a single file.\nFinally, an optional Sieve Variance Plateau correction of the variance estimates is performed.","category":"page"},{"location":"targene/overview/","page":"The TarGene Workflow","title":"The TarGene Workflow","text":"An overview of the workflow is presented in the following diagram.","category":"page"},{"location":"targene/overview/","page":"The TarGene Workflow","title":"The TarGene Workflow","text":"(Image: TarGene Workflow High Level)","category":"page"},{"location":"targene/overview/#Example-Run-Command","page":"The TarGene Workflow","title":"Example Run Command","text":"","category":"section"},{"location":"targene/overview/","page":"The TarGene Workflow","title":"The TarGene Workflow","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TAG -entry TARGENE -profile P -resume","category":"page"},{"location":"targene/overview/","page":"The TarGene Workflow","title":"The TarGene Workflow","text":"We now describe step by step how to setup a TarGene run configuration.","category":"page"},{"location":"targene/sieve_variance/#Correcting-for-population-relatedness","page":"Correcting for population relatedness","title":"Correcting for population relatedness","text":"","category":"section"},{"location":"targene/sieve_variance/","page":"Correcting for population relatedness","title":"Correcting for population relatedness","text":"If the i.i.d. (independent and identically distributed) hypothesis is not satisfied, most of the traditional statistical inference theory falls apart. This is typically possible in population genetics where a study may contain related individuals. Here we leverage a non-parametric method called Sieve Variance Plateau (SVP) estimation. The hypothesis is that the dependence between individuals is sufficiently small, so that our targeted estimator will still be asymptotically unbiased, but its variance will be under estimated. In brief, the SVP estimator computes a variance estimate for a range of thresholds 𝜏, by considering individuals to be genetically independent if their genetic distance exceeds 𝜏. The genetic distance between a pair of individuals (𝑖, 𝑗) equals 1 − GRM𝑖,𝑗 , i.e., one minus their genetic relatedness value. As the distance threshold 𝜏 increases, fewer individuals are assumed to be genetically independent. For instance, the estimate corresponding to a distance of 𝜏 = 0 corresponds to the i.i.d. hypothesis, while a distance of 𝜏 = 1 incorporates pairs of individuals who are not genetically correlated. TarGene varies the threshold 𝜏 from 0 to 1 and fits a curve to the corresponding variance estimates. The maximum of this curve is the most conservative estimate of the variance estimator and constitutes our corrected variance estimator.","category":"page"},{"location":"targene/sieve_variance/","page":"Correcting for population relatedness","title":"Correcting for population relatedness","text":"The following arguments can be changed to control the behaviour of the pipeline:","category":"page"},{"location":"targene/sieve_variance/","page":"Correcting for population relatedness","title":"Correcting for population relatedness","text":"SVP (default: false): Must be set to true to enable variance adjustement.\nESTIMATOR_KEY (default: TMLE): The estimator from the ESTIMATOR_FILE that will be used for sieve variance plateau adjustment.\nGRM_NSPLITS (default: 100): This is a purely computational argument. The GRM is typically very large and splitting enables a good memory/parallelization tradeoff.\nMAX_SVP_THRESHOLD (default: 0.8): Controls the maximum genetic distance considered.\nNB_SVP_ESTIMATORS (default: 0): Controls the number of points in the interval [0, MAX_SVP_THRESHOLD]. If 0, the Sieve Variance Plateau method will not be applied.\nPVAL_THRESHOLD (default: 0.05): Only estimates with a p-value lower than PVAL_THRESHOLD will be considered for SVP correction. This is because SVP will only increase the variance of the estimator.","category":"page"},{"location":"targene/configuration/#Configuration","page":"Configuration","title":"Configuration","text":"","category":"section"},{"location":"targene/configuration/#Main-Outputs","page":"Configuration","title":"Main Outputs","text":"","category":"section"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"The workflow will produce the following main outputs in the output directory (OUTDIR, defaults to results):","category":"page"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"An HDF5 file containing estimation results (HDF5_OUTPUT, default: results.hdf5)\nAn optional JSON file containing estimation results (JSON_OUTPUT)\nAn optional svp.hdf5 containing Sieve Variance Plateau corrected variance estimates.\nA Quantile-Quantile summary plot: QQ.png","category":"page"},{"location":"targene/configuration/#Arguments","page":"Configuration","title":"Arguments","text":"","category":"section"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"ESTIMATOR_FILE: Estimator name or Julia file containing the description of the Targeted Estimators to use. To be consistent it should match the argument provided to the previous TarGene run.\nBGEN_FILES: Path to imputed BGEN files from which the variants in VARIANTS_LIST will be extracted.\nBED_FILES: Path expression to PLINK BED files.\nTRAITS_DATASET: Path to a traits dataset. If you are running this for a non-UKBB cohort, your sample IDs must be specified in the first column of this CSV file, with the column name SAMPLE_ID.","category":"page"},{"location":"targene/configuration/#Main-Options","page":"Configuration","title":"Main Options","text":"","category":"section"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"STUDY_DESIGN (default: CUSTOM): How genetic variants and effect sizes are specified. See the study design section.\nCOHORT (default: \"UKBB\"): Current default for this is UKBB. If set to a value other than UKBB, this will not run UKBB-specific trait extraction.\nPOSITIVITY_CONSTRAINT (default: 0.01): When the list of estimands is generated or validated. Treatment variables' rarest configuration should have at least that frequency. For example if the treatment variables are two variants with minor allele A and T respectively. The rarest configuration will be (AA, TT) and should have a frequency of at least POSITIVITY_CONSTRAINT.\nNB_PCS (default: 6): The number of PCA components to extract.\nSVP (default: false): Whether Sieve Variance Plateau correction should be performed.\nFLASHPCA_EXCLUSION_REGIONS (default: assets/exclusionregionshg19.txt): A path to the flashpca special exclusion regions.\nMAF_THRESHOLD (default: 0.01): Only variants with that minor allele frequency are considered\nLD_BLOCKS: A path to pre-identified linkage disequilibrium blocks to be removed from the BED files. It is good practice to specify LD_BLOCKS, as it will remove SNPs correlated with your variants-of-interest before running PCA.","category":"page"},{"location":"targene/configuration/#If-COHORTUKBB-(default)","page":"Configuration","title":"If COHORT=UKBB (default)","text":"","category":"section"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"UKB_CONFIG (default: assets/ukbconfig.yaml): YAML configuration file describing which traits should be extracted and how the population should be subsetted.\nUKB_ENCODING_FILE: If the TRAITS_DATASET is encrypted, an encoding file must be provided.\nUKB_WITHDRAWAL_LIST: List of participants withdrawn from the study.\nQC_FILE: Genotyping quality control file from the UK-Biobank study.","category":"page"},{"location":"targene/configuration/#If-STUDY_DESIGNCUSTOM-(default)","page":"Configuration","title":"If STUDY_DESIGN=CUSTOM (default)","text":"","category":"section"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"ESTIMANDS_FILE: YAML configuration file describing the effect sizes of interest, see the custom study design section","category":"page"},{"location":"targene/configuration/#If-STUDY_DESIGNALLELE_INDEPENDENT","page":"Configuration","title":"If STUDY_DESIGN=ALLELE_INDEPENDENT","text":"","category":"section"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"ESTIMANDS_FILE: YAML configuration file describing the effect sizes of interest, see the allele independent section.","category":"page"},{"location":"targene/configuration/#If-STUDY_DESIGNFROM_ACTORS","page":"Configuration","title":"If STUDY_DESIGN=FROM_ACTORS","text":"","category":"section"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"BQTLS: A CSV file containing binding quantitative trait loci (bQTLs). If multiple transcription factors (TFs) are included in a single run, you must include a column called TF, which specifies the TF associated with each bQTL.\nTRANS_ACTORS: A prefix to CSV files containing quantitative trait loci potentially interacting with the previous bqtls. If multiple transcription factors (TFs) are included in a single run, you must include a column called TF, which specifies the TF associated with each transactor.\nEXTRA_CONFOUNDERS, default: nothing: Path to additional confounders file, one per line, no header.\nEXTRA_COVARIATES, default: nothing: Path to additional covariates file, one per line, no header.\nENVIRONMENTALS, default: nothing: Path to additional environmental treatments file, one per line, no header.\nORDERS, default: \"1,2\": Comma separated list describing the order of desired interaction estimands, 1 for the ATE (no interaction), 2 for pairwise interactions etc... e.g. \"1,2\"","category":"page"},{"location":"targene/configuration/#If-SVPtrue","page":"Configuration","title":"If SVP=true","text":"","category":"section"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"GRM_NSPLITS, default: 100: To fasten GRM computation, it is typically split in batches.\nNB_SVP_ESTIMATORS, default: 100: Number of sieve variance estimates per curve. Setting this value to 0 results in skipping sieve variance correction.\nMAX_SVP_THRESHOLD, default: 0.9: Variance estimates are computed for tau ranging from 0 to MAXSVPTHRESHOLD\nPVAL_THRESHOLD, default: 0.05: Only results with a p-value below this threshold are considered for Sieve Plateau Variance correction.\nESTIMATOR_KEY, default: TMLE: The p-value for PVAL_THRESHOLD is computed using the result from this estimator.","category":"page"},{"location":"targene/configuration/#Secondary-Options","page":"Configuration","title":"Secondary Options","text":"","category":"section"},{"location":"targene/configuration/","page":"Configuration","title":"Configuration","text":"RNG, default: 123: General random seed used to induce permutation.\nVERBOSITY, default: 0: Verbosity level of the the Workflow's processes.\nBATCH_SIZE, default: 400: The set of estimands to be estimated is batched and the Targeted Learning processes will run in parallel across batches.\nTL_SAVE_EVERY, default: 50: During the estimation process, results are appended to the file in chunks to free memory.\nKEEP_IC, default: SVP: To save the Influence Curves for each estimate. Depending on the size of your dataset, this can result in massive disk usage.","category":"page"},{"location":"overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"overview/#Running-the-Workflows","page":"Overview","title":"Running the Workflows","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"Since TarGene uses Nextflow, all workflows can be run in the same way from the command line:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"nextflow run https://github.com/TARGENE/targene-pipeline/ -r TAG -entry WORKFLOW_NAME -profile P -resume","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"where:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"TAG is the latest TarGene version, e.g. v0.9.0\nWORKFLOW_NAME is any of the TarGene workflows\nP is a Nextflow profile describing your run environment (see Environment Configuration).","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"Additional Nextflow command line arguments can be found in their documentation, for example:","category":"page"},{"location":"overview/","page":"Overview","title":"Overview","text":"-resume: Tells Nextflow to try to resume the pipeline if an error occurred during the execution (if you forgot to specify a parameter for instance)\n-with-trace and -with-report will generate additional report files.","category":"page"},{"location":"overview/#Workflows-Configurations","page":"Overview","title":"Workflows Configurations","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"There are mainly two parts to configuring a workflow run. The first part describes the execution and environment and the second part describes your actual project. When running the nextflow run command, Nextflow will look for a nextflow.config configuration file in your current directory. If you are new to Nextflow, we suggest you use this file to setup both the environment and project configurations. As your project grows you may want to benefit from splitting this file in more modular components.","category":"page"},{"location":"overview/#Environment-Configuration","page":"Overview","title":"Environment Configuration","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"It is likely that you will run TarGene on a HPC platform, in particular the Executors and Singularity configurations are required. Since Nextflow is so widespread, it is probable that such a configuration file is already available from your HPC administrators. Since this configuration only describes de computing platform and not your project, it is often described as a Profile. If your HPC uses the SGE executor, the -profile eddie may work with no, or minor adjustment (it can also serve as a template for other executors see file).","category":"page"},{"location":"overview/#Project-Configuration","page":"Overview","title":"Project Configuration","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"These are the configuration details associated with your project, this is usually done in a nextflow.config file living at the root of your project's directory. The configuration parameters are specific to each workflow and described in the following sections. There are currently two main workflows and two secondary workflows within TarGene.","category":"page"},{"location":"overview/#Main-Workflows","page":"Overview","title":"Main Workflows","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"The TarGene Workflow (WORKFLOW_NAME: TARGENE): It is the main workflow for the targeted estimation of genetic effects.\nThe Negative Control Workflows: These workflows enable the control of the false discovery rate by using the results obtained from a previous TarGene discovery run. There are currently two of them:\nThe Permutation Test Workflow (WORKFLOW_NAME: PERMUTATION_TEST): Performs permutation tests by independently shuffling the individuals in the columns of an aggregated dataset.\nThe Randomized Variants Workflow (WORKFLOW_NAME: RANDOMIZATION_TEST): When there are multiple genetic variants of interest, e.g. in an interaction study, one can replace one of the variant at random by another variant and the effect is expected to be 0 in average.","category":"page"},{"location":"overview/#Secondary-Workflows","page":"Overview","title":"Secondary Workflows","text":"","category":"section"},{"location":"overview/","page":"Overview","title":"Overview","text":"The PCA Workflow (WORKFLOW_NAME: PCA): This workflow computes principal components.\nThe Make Dataset Workflow (WORKFLOW_NAME: MAKE_DATASET): This workflow generates an aggregated dataset from traits and genetic data.","category":"page"},{"location":"#TarGene","page":"Home","title":"TarGene","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to TarGene, the software that brings Targeted Learning to population genetic studies! TarGene enables the estimation of various effect sizes including the Average Treatment Effect and the Interaction Average Treatment Effect (GxG and GxE) up to any order (GxGxGxEx...!). Because we follow the Targeted Learning framework, the final estimates provided by TarGene are covered by mathematical guarantees. The software is delivered as a Nexflow pipeline to bring scalability and reproducibility to your research.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Why using TarGene:","category":"page"},{"location":"","page":"Home","title":"Home","text":"To cover effect sizes with asymptotic mathematical guarantees.\nTo dispense with unrealistic and unnecessary parametric assumptions.\nTo embed a causal interpretation into reported estimates.\nTo perform any higher-order interaction (>=2) analysis, including GxG and GxE.","category":"page"},{"location":"#Overview-of-the-pipeline","page":"Home","title":"Overview of the pipeline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The pipeline can roughly be decoupled into three steps. The first, aims at pre-processing the data sources to convert them in a table data format that can be wielded by Targeted Maximum Likelihood Estimation (TMLE). The second is the TMLE itself. The third and final optional step is the Sieve Variance Plateau correction which revises the variance estimate to account for the fact that individuals in the population are not necessarily independent. The following diagram provides a high level interface of the organisation of the pipeline.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div style=\"text-align:center\">\n<img src=\"assets/targene_diagram.png\" alt=\"Targene Pipeline\" style=\"width:800px;\"/>\n</div>","category":"page"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The pipeline should work with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Nextflow >= 23.10.0\nSingularity >= 3.8.6","category":"page"},{"location":"#Quick-start-for-Eddie-users","page":"Home","title":"Quick start for Eddie users","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are a University of Edinburgh researcher and have access to the Eddie cluster you may want to use the Eddie-Template to quickly setup your project. ","category":"page"},{"location":"#Getting-in-touch","page":"Home","title":"Getting in touch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please feel free to raise an issue if you face a problem or would benefit from a new feature. Contributions are most welcome.","category":"page"},{"location":"targene/study_designs/#Study-Designs","page":"Study Designs","title":"Study Designs","text":"","category":"section"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"A study design describes what are the questions of interest for a given TarGene run. In particular, it answers the two following questions:","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"What variants are of interest?\nWhat are the quantities of interest: Average Treatment Effects, Epistatic Interactions, Gene by Environment Interactions, ...?","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"While specifying your estimands, it may be useful to keep the following causal model in mind.","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"(Image: Causal Graph)","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"Where V_1V_p are a set of genetic variants, the Y_1Y_K are a set of traits and C are a set of additional predictors for Y but not confounding the genetic effects.","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"The following sections describe the available study designs available in TarGene and assumes the following notations:","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"ATE: Average Treatment Effect\nIATE: Interaction Average Treatment Effect\nCM: Conditional Mean","category":"page"},{"location":"targene/study_designs/#CUSTOM","page":"Study Designs","title":"CUSTOM","text":"","category":"section"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"This is the most general setting and should match the needs of any project, however it requires some preliminary work. In this setting, one typically provides a file containing a list of the estimands of interest. If you are interested in only a few estimands, it may be acceptable to write them by hand. Otherwise it is best to generate them using a programming language (for instance using TMLE.jl). The path to those estimands is then provided with the ESTIMANDS_FILE Nextflow parameter. Estimands are specified via a YAML file as follows:","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"type: \"Configuration\"\nestimands:\n  - outcome_extra_covariates: []\n    type: \"ATE\"\n    treatment_values:\n      3:3502414:T:C:\n        case: \"TT\"\n        control: \"CT\"\n      1:238411180:T:C:\n        case: \"TC\"\n        control: \"TT\"\n    outcome: ALL\n    treatment_confounders: []\n  - outcome_extra_covariates: []\n    type: \"IATE\"\n    treatment_values:\n      3:3502414:T:C:\n        case: \"TT\"\n        control: \"CT\"\n      1:238411180:T:C:\n        case: \"TC\"\n        control: \"TT\"\n    outcome: ALL\n    treatment_confounders:\n      3:3502414:T:C: []\n      1:238411180:T:C: []\n  - outcome_extra_covariates: []\n    type: \"CM\"\n    treatment_values:\n      3:3502414:T:C: \"CT\"\n      1:238411180:T:C: \"TC\"\n    outcome: ALL\n    treatment_confounders:\n      3:3502414:T:C: []\n      1:238411180:T:C: []\n  - outcome_extra_covariates: []\n    type: \"ATE\"\n    treatment_values:\n      2:14983:G:A:\n        case: \"GG\"\n        control: \"AG\"\n    outcome: ALL\n    treatment_confounders: []\n  - outcome_extra_covariates: []\n    type: \"CM\"\n    treatment_values:\n      2:14983:G:A: \"AG\"\n    outcome: ALL\n    treatment_confounders: []","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"For each estimand:","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"type: refers to the type of effect size (ATE, IATE, CM)\noutcome: The trait of interest. If using the Uk-Biobank datasource it must match the phenotypes/name field in the associated UKB_CONFIG file. You can also use the wildcard \"ALL\" to specify that you want to estimate this parameter accross all traits in the dataset.\ntreatment_values: For each treatment variable (genetic-variant / environmental variables), the control/case contrasts.\ntreatment_confounders: If each treatment's set of confounding variables is assumed to be the same, it can be a list of these variables. Otherwise, for each treatment variable, a list of confounding variables. Note that principal components will be added to that list automatically and must not be provided here. You can provide an empty list.\noutcome_extra_covariates: This is optional and correspond to a list of additional covariates for the prediction of the trait.","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"Note that variants must be encoded via an explicit genotype string representation (e.g. \"AC\"), the order of the alleles in the genotype is not important.","category":"page"},{"location":"targene/study_designs/#ALLELE_INDEPENDENT","page":"Study Designs","title":"ALLELE_INDEPENDENT","text":"","category":"section"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"This mode aims to make it easy to generate estimands without explicitly having to write them down. We still rely on a YAML ESTIMANDS_FILE, that contains the following fields:","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"type: How the variants will be combined to generate estimands (see below).\nestimands: A list of the followings:\ntype: The type of generated estimands (CM, ATE, IATE)\norders: With respect to treatment variables if more than two are provided, the combination orders to be generated. For interactions, the order is always greater or equal than 2.\nvariants: The list of genetic variants of interest, see below for how this can be specified.\nextra_treatments: Environmental treatment variables that are added to the treatments combinations.\noutcome_extra_covariates: Additional covariates predictive of the outcomes\nextra_confounders: Confounding variables other than Principal Components.","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"In TarGene, the effect of a variant (or set of variants) on a trait is not defined in terms of a parametric model. As such, the effect size of a variant on a trait is in fact multidimensional, a vector for which each entry corresponds to a specific genotype change. As an example, if a variant RSID_17 has two alleles \"A\" and \"G\", some possible genotype changes are: AA → AG, AG → GG, AA → GG, GG → AA, ... However, some effects of these changes are redundant, e.g. the effect of AA → GG can be obtained from the effects of AA → AG and AG → GG via simple addition. TarGene will only compute transitive changes, in this case only: AA → AG and AG → GG need to be estimated since any other change can be obtained from them.","category":"page"},{"location":"targene/study_designs/#type-flat","page":"Study Designs","title":"type = flat","text":"","category":"section"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"In this mode, genetic variants are provided as a flat list, for each estimand type and order specified in the estimands section, the estimands are generated using combinations.","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"For example, with the following file:","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"type: flat\n\nestimands:\n  - type: IATE\n    orders: [2, 3]\n  - type: ATE\n\nvariants:\n  - RSID_17\n  - RSID_99\n  - RSID_102\n\nextra_treatments:\n  - TREAT_1\n\noutcome_extra_covariates:\n  - COV_1\n\nextra_confounders:\n  - 21003\n  - 22001","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"Average Treatment Effects of order 1 (default) for all (RSID17, RSID99, RSID102, TREAT1) are generated\nInteraction Effects of order 2 and 3 for all combinations of (RSID17, RSID99, RSID102, TREAT1) are generated. Some of these combinations are:\n(RSID17, RSID99)\n(RSID102, TREAT1)\n(RSID102, RSID99, TREAT_1)\n...","category":"page"},{"location":"targene/study_designs/#type-groups","page":"Study Designs","title":"type = groups","text":"","category":"section"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"When the number of variants becomes large, estimating all combinations becomes both computationally expensive and reduces power due to the associated multiple testing burden. If the genetic variants are not chosen at random but based on some biological mechanism (e.g. a transcription factor), it is more efficient to group them. Within each group further subgroups can be defined for the roles played by each variant, only combinations across groups are considered.","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"For example, the following file describes two groups each consisting of two subgroups:","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"type: groups\nestimands:\n  - type: IATE\n    orders: [2, 3]\nvariants:\n  TF1:\n    bQTLs:\n      - RSID_17\n      - RSID_99\n    eQTLs:\n      - RSID_102\n  TF2:\n    bQTLs:\n      - RSID_17\n      - RSID_198\n    eQTLs:\n      - RSID_2\nextra_treatments:\n  - TREAT_1\noutcome_extra_covariates:\n  - COV_1\nextra_confounders:\n  - 21003\n  - 22001","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"For group TF1, only (RSID17, RSID102) and (RSID99, RSID102) are considered and similarly for group TF2.","category":"page"},{"location":"targene/study_designs/#FROM_ACTORS","page":"Study Designs","title":"FROM_ACTORS","text":"","category":"section"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"In this setting the goal is to infer the interaction effect between multiple variants and potential external factors, interacting together via a specific biological mechanism. Typically, multiple sets of variants are of interest and each set is identified with a specific molecule, contributing to the mechanism. In particular, it is assumed that a set of variants, usually binding quantitative trait loci (bQTLs) play a pivotal role. All interactions of interest are thus defined with respect to that set of genetic variations. Let's Consider the following scenario: we know that a transcription factor binds to molecules x and y and then differentially binds to specific regions in the genome (bQTLs) to regulate downstream genes. We suspect that an alteration of this mechanism is responsible for some diseases. A set of xQTLs, associated with the expression of x and a set of yQTLs associated with the expression of y have been identified. Together xQTLs and yQTLs variants are termed \"trans actors\". We further suspect that some environmental factors may influence this process. From that scenario, there are many questions that can be asked, for instance : \"What is the interaction effects of a bQTL with an environmental factor?\". This is a simple pairwise interaction setting and more complex scenarios can be envisaged as described in the following graph.","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"<div style=\"text-align:center\">\n<img src=\"../assets/from_actors.png\" alt=\"FromActors\" style=\"width:600px;\"/>\n</div>","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"Let us now turn to the pipeline specification for this parameter plan:","category":"page"},{"location":"targene/study_designs/","page":"Study Designs","title":"Study Designs","text":"BQTLS: A path to a .csv file containing at least an ID column for each rsID and an optional CHR column for the chromosome on which the SNP is located.\nTRANS_ACTORS: A path prefix to a set of .csv files identifying different trans-acting variants. Each file has the same format as for the bQTLs.\nENVIRONMENTALS: A path to a .txt file containing a list of environmental exposures with no header and one exposure per line. Each exposure should be available from the trait dataset.\nEXTRA_COVARIATES: A path to a .txt file containing a list of extra covariate variables with no header and one variable per line. Each variable should be available from the trait dataset.\nEXTRA_CONFOUNDERS: A path to a .txt file containing a list of extra confounding variables with no header and one variable per line. Each variable should be available from the trait dataset.\nORDERS: A comma separated string that specifies the various interaction orders of interest. All combinations satisfying the positivity constraint will be generated. The order 1 corresponds to the Average Treatment Effect (ATE) for bQTLs, any higher order corresponds to the Interaction Average Treatment Effect (IATE) for the various actors. For example, in the previous scenario, assume we provided ORDERS=1,2. This would generate parameter files for the estimation of all:\nATEs estimands for all bQTLs\nIATEs estimands for all (bQTLs, xQTLs), (bQTLs, yQTLs), (bQTLs, Envs) pairs.","category":"page"}]
}
